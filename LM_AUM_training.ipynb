{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba2824bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nou-z\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e65c5af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"EleutherAI/wikitext_document_level\", \"wikitext-2-raw-v1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecbf6daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50257\n",
      "End of text token: 50256\n",
      "Example tokenization: [15496, 995, 0]\n",
      "Input Shape torch.Size([2, 4])\n",
      "Output Shape torch.Size([2, 4])\n",
      "Input Example:\n",
      "tensor([[   27,  7700,    29,   220],\n",
      "        [  569, 18354,  7496, 17740]])\n",
      "Output Example:\n",
      "tensor([[ 7700,    29,   220,   796],\n",
      "        [18354,  7496, 17740,  6711]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\") # Get the same tokenizer used for GPT-2\n",
    "\n",
    "\n",
    "print(\"Vocabulary size:\", tokenizer.n_vocab) # Vocabilary size is how many unique tokens the tokenizer can encode\n",
    "print(\"End of text token:\", tokenizer.eot_token) # End of text token is used to indicate the end of a text sequence\n",
    "print(\"Example tokenization:\", tokenizer.encode(\"Hello world!\"))\n",
    "\n",
    "# Convert entire dataset into a single string\n",
    "# This dataset is small enough to fit into memory\n",
    "# For larger datasets, you may need to use more \n",
    "# sophisticated methods to process the data.\n",
    "\n",
    "all_text = \"\"\n",
    "all_data = dataset[\"page\"]\n",
    "for example in all_data:\n",
    "    all_text += \"<page> \"+ example + \" </page>\"\n",
    "\n",
    "# Tokenize the entire text at once\n",
    "tokenized_text = tokenizer.encode(all_text)\n",
    "\n",
    "\n",
    "# We will create a function that generates a dataset of examples\n",
    "# for the language model. The function will take in the number of\n",
    "# examples to generate, the block size, and the test split.\n",
    "# It will return the training and test datasets.\n",
    "def get_dataset(num_examples, context_window_length, test_split=0.1):\n",
    "    input_blocks = [] # List to store input sequences\n",
    "    target_blocks = [] # List to store target sequences\n",
    "\n",
    "    # Use a sliding window to create input/target sequences\n",
    "    for i in range(0, len(tokenized_text), context_window_length + 1):\n",
    "        block = tokenized_text[i:i+context_window_length+ 1]\n",
    "        \n",
    "        # Skip blocks that are too short\n",
    "        if len(block) < context_window_length + 1:\n",
    "            continue\n",
    "\n",
    "        input_seq = block[:-1]  \n",
    "        target_seq = block[1:]  \n",
    "\n",
    "        input_blocks.append(input_seq)\n",
    "        target_blocks.append(target_seq)\n",
    "        \n",
    "        # Stop if we have enough examples\n",
    "        if len(input_blocks) >= num_examples:\n",
    "            break\n",
    "\n",
    "    # Convert to tensors for pytorch and move to gpu\n",
    "    inputs = torch.tensor(input_blocks, dtype=torch.long).to(device)\n",
    "    targets = torch.tensor(target_blocks, dtype=torch.long).to(device)\n",
    "\n",
    "    # Calculate train/test split point\n",
    "    split_idx = int(num_examples * (1 - test_split))\n",
    "\n",
    "    # Split into train/test\n",
    "    train_inputs = inputs[:split_idx]\n",
    "    train_targets = targets[:split_idx]\n",
    "    test_inputs = inputs[split_idx:]\n",
    "    test_targets = targets[split_idx:]\n",
    "    return train_inputs, train_targets, test_inputs, test_targets\n",
    "\n",
    "# Get a small dataset\n",
    "i, o, _, _ = get_dataset(2, 4, 0)\n",
    "print(\"Input Shape\", i.shape)\n",
    "print(\"Output Shape\", o.shape)\n",
    "print(\"Input Example:\")\n",
    "print(i)\n",
    "print(\"Output Example:\")\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39fec469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# A simple configuration container\n",
    "class GPTConfig:\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size,  # size of the vocabulary, from tokenizer, for gpt2 tokenizer it is 50257\n",
    "        n_layer,   # number of transformer blocks\n",
    "        n_head,    # number of attention heads for each transformer block\n",
    "        n_embd,  # embedding dimension for each token\n",
    "        seq_len,  # sequence length for the model - e.g. the \"context window\" \n",
    "    \n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.seq_len = seq_len\n",
    "     \n",
    "test_config = GPTConfig(\n",
    "    vocab_size=tokenizer.n_vocab,\n",
    "    n_layer=2,  \n",
    "    n_head=3,\n",
    "    n_embd=6,\n",
    "    seq_len=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0c10ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position encoding shape: torch.Size([1, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "def get_position_encoding(seq_len, d, n=10000):\n",
    "    \"\"\"\n",
    "    Computes the positional encoding matrix of shape (seq_len, d).\n",
    "    \n",
    "    Args:\n",
    "        seq_len (int): Length of the sequence.\n",
    "        d (int): Dimension of the embedding.\n",
    "        n (float): The base for the exponential term (default 10000 in many Transformer implementations).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (seq_len, d) containing the positional encodings.\n",
    "    \"\"\"\n",
    "    \n",
    "    P = torch.zeros(seq_len, d).to(device)\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, d // 2):\n",
    "            P[pos, 2 * i] = math.sin(pos / (n ** ((2 * i) / d)))\n",
    "            if i + 1 < d:\n",
    "                P[pos, 2* i + 1] = math.cos(pos / (n ** ((2 * i) / d)))\n",
    "\n",
    "    return P.unsqueeze(0)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "position_encoding = get_position_encoding(seq_len=test_config.seq_len, d=test_config.n_embd)\n",
    "print(\"Position encoding shape:\", position_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aa070fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.Wq = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Query weights - will transform input embeddings into queries\n",
    "        self.Wk = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Key weights - will transform input embeddings into keys\n",
    "        self.Wv = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Value weights - will transform input embeddings into values\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Attention input shape:\", x.shape)\n",
    "        print(\"\")\n",
    "        print(\"Query weights shape:\", self.Wq.shape)\n",
    "        print(\"Key weights shape:\", self.Wk.shape)\n",
    "        print(\"Value weights shape:\", self.Wv.shape)\n",
    "        queries = x @ self.Wq # Matrix multiplication to transform input embeddings into queries\n",
    "        keys = x @ self.Wk # Matrix multiplication to transform input embeddings into keys\n",
    "        values = x @ self.Wv # Matrix multiplication to transform input embeddings into values\n",
    "        print(\"\")\n",
    "        print(\"Queries shape:\", queries.shape)\n",
    "        print(\"Keys shape:\", keys.shape)\n",
    "        print(\"Values shape:\", values.shape)\n",
    "\n",
    "        qkt = queries @ keys.transpose(-2, -1) # Calculate QK^T\n",
    "        qkt_scaled = qkt / math.sqrt(queries.size(-1)) # Scale QK^T by the dimension of the keys\n",
    "        qkt_softmax = F.softmax(qkt_scaled, dim=-1) # Apply softmax row-wise to get attention weights\n",
    "        print(\"\")\n",
    "        print(\"QK^T shape:\", qkt.shape)\n",
    "\n",
    "        attn_output = qkt_softmax @ values # Multiply softmax(QK^T) by values\n",
    "        print(\"\")\n",
    "        print(\"Attention output shape:\", attn_output.shape)\n",
    "        return attn_output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b19357b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.Wq = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Query weights - will transform input embeddings into queries\n",
    "        self.Wk = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Key weights - will transform input embeddings into keys\n",
    "        self.Wv = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Value weights - will transform input embeddings into values\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1] # Get sequence length (number of tokens / context window length)\n",
    "        queries = x @ self.Wq # Matrix multiplication to transform input embeddings into queries\n",
    "        keys = x @ self.Wk    # Matrix multiplication to transform input embeddings into keys\n",
    "        values = x @ self.Wv  # Matrix multiplication to transform input embeddings into values\n",
    "        qkt = queries @ keys.transpose(-2, -1)  # Calculate QK^T\n",
    "        qkt_scaled = qkt / math.sqrt(queries.size(-1))  # Scale QK^T by the dimension of the keys\n",
    "\n",
    "        # MASKING\n",
    "        # THIS IS THE ONLY DIFFERENCE, USE -inf FOR UPPER TRIANGLE MASK SO THAT SOFTMAX WILL BE 0\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1)\n",
    "        causal_mask = causal_mask.masked_fill(causal_mask == 1, float('-inf'))  # Upper triangle masked with -inf \n",
    "        qkt_scaled = qkt_scaled + causal_mask # Add the mask to the scaled QK^T\n",
    "        # END MASKING\n",
    "\n",
    "        qkt_softmax = F.softmax(qkt_scaled, dim=-1) # Apply softmax row-wise to get attention weights, the -inf values will become 0 here\n",
    "        attn_output = qkt_softmax @ values # Multiply softmax(QK^T) by values\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12972c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn_heads = nn.ModuleList([\n",
    "            CausalSelfAttention(config) for _ in range(config.n_head)\n",
    "        ])  # Create n_head attention heads\n",
    "        self.projection = nn.Linear(config.n_embd * config.n_head, config.n_embd).to(device) # Linear layer to project multi-head attention outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        head_outputs = [head(x) for head in self.attn_heads] # Get the output of each attention head\n",
    "        multihead_output = torch.cat(head_outputs, dim=-1) # Concatenate the outputs\n",
    "        return self.projection(multihead_output) # Project the concatenated outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e759e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(config)\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd).to(device)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "        ).to(device)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c11fbe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.n_embd).to(device)\n",
    "        self.position_encoding = get_position_encoding(config.seq_len, config.n_embd)\n",
    "        self.blocks = nn.Sequential(*[GPTBlock(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd).to(device)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.token_embedding(x) + self.position_encoding\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36ea8969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_curve_micro(pred_tensor, label_tensor):\n",
    "    device=pred_tensor.device\n",
    "    n_class=pred_tensor.size(1)\n",
    "    one_hot_labels = F.one_hot(label_tensor, num_classes=n_class).to(device)\n",
    "    is_positive = one_hot_labels\n",
    "    is_negative =1-one_hot_labels\n",
    "    fn_diff = -is_positive.flatten()\n",
    "    fp_diff = is_negative.flatten()\n",
    "    thresh_tensor = -pred_tensor.flatten()\n",
    "    fn_denom = is_positive.sum()\n",
    "    fp_denom = is_negative.sum()\n",
    "    sorted_indices = torch.argsort(thresh_tensor)\n",
    "    sorted_fp_cum = fp_diff[sorted_indices].cumsum(0) / fp_denom\n",
    "    sorted_fn_cum = -fn_diff[sorted_indices].flip(0).cumsum(0).flip(0) / fn_denom\n",
    "\n",
    "    sorted_thresh = thresh_tensor[sorted_indices]\n",
    "    sorted_is_diff = sorted_thresh.diff() != 0\n",
    "    sorted_fp_end = torch.cat([sorted_is_diff, torch.tensor([True],device=device)])\n",
    "    sorted_fn_end = torch.cat([torch.tensor([True],device=device), sorted_is_diff])\n",
    "\n",
    "    uniq_thresh = sorted_thresh[sorted_fp_end]\n",
    "    uniq_fp_after = sorted_fp_cum[sorted_fp_end]\n",
    "    uniq_fn_before = sorted_fn_cum[sorted_fn_end]\n",
    "\n",
    "    FPR = torch.cat([torch.tensor([0.0],device=device), uniq_fp_after])\n",
    "    FNR = torch.cat([uniq_fn_before, torch.tensor([0.0],device=device)])\n",
    "\n",
    "    return {\n",
    "        \"FPR\": FPR,\n",
    "        \"FNR\": FNR,\n",
    "        \"TPR\": 1 - FNR,\n",
    "        \"min(FPR,FNR)\": torch.minimum(FPR, FNR),\n",
    "        \"min_constant\": torch.cat([torch.tensor([-1],device=device), uniq_thresh]),\n",
    "        \"max_constant\": torch.cat([uniq_thresh, torch.tensor([0],device=device)])\n",
    "    }\n",
    "def ROC_AUC_micro(pred_tensor, label_tensor):\n",
    "    roc = ROC_curve_micro(pred_tensor, label_tensor)\n",
    "    FPR_diff = roc[\"FPR\"][1:]-roc[\"FPR\"][:-1]   \n",
    "    TPR_sum = roc[\"TPR\"][1:]+roc[\"TPR\"][:-1]\n",
    "    return torch.sum(FPR_diff*TPR_sum/2.0)\n",
    "#AUM \n",
    "def Proposed_AUM_micro(pred_tensor, label_tensor):\n",
    "\n",
    "    roc = ROC_curve_micro(pred_tensor, label_tensor)\n",
    "    min_FPR_FNR = roc[\"min(FPR,FNR)\"][1:-1]\n",
    "    constant_diff = roc[\"min_constant\"][1:].diff()\n",
    "    return torch.sum(min_FPR_FNR * constant_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab91b166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_curve_macro(pred_tensor, label_tensor):\n",
    "    n_class=pred_tensor.size(1)\n",
    "    one_hot_labels = F.one_hot(label_tensor, num_classes=n_class)\n",
    "    is_positive = one_hot_labels\n",
    "    is_negative =1-one_hot_labels\n",
    "    fn_diff = -is_positive\n",
    "    fp_diff = is_negative\n",
    "    thresh_tensor = -pred_tensor\n",
    "    fn_denom = is_positive.sum(dim=0).clamp(min=1)\n",
    "    fp_denom = is_negative.sum(dim=0).clamp(min=1)\n",
    "    sorted_indices = torch.argsort(thresh_tensor,dim=0)\n",
    "    sorted_fp_cum = torch.div(torch.gather(fp_diff, dim=0, index=sorted_indices).cumsum(0), fp_denom)\n",
    "    sorted_fn_cum = -torch.div(torch.gather(fn_diff, dim=0, index=sorted_indices).flip(0).cumsum(0).flip(0) , fn_denom)\n",
    "    sorted_thresh = torch.gather(thresh_tensor, dim=0, index=sorted_indices)\n",
    "    #Problem starts here \n",
    "    zeros_vec=torch.zeros(1,n_class)\n",
    "    FPR = torch.cat([zeros_vec, sorted_fp_cum])\n",
    "    FNR = torch.cat([sorted_fn_cum, zeros_vec])\n",
    "    return {\n",
    "        \"FPR_all_classes\": FPR,\n",
    "        \"FNR_all_classes\": FNR,\n",
    "        \"TPR_all_classes\": 1 - FNR,\n",
    "        \"min(FPR,FNR)\": torch.minimum(FPR, FNR),\n",
    "        \"min_constant\": torch.cat([-torch.ones(1,n_class), sorted_thresh]),\n",
    "        \"max_constant\": torch.cat([sorted_thresh, zeros_vec])\n",
    "    }\n",
    "\n",
    "def ROC_AUC_macro(pred_tensor, label_tensor):\n",
    "    roc = ROC_curve_macro(pred_tensor, label_tensor)\n",
    "    FPR_diff = roc[\"FPR_all_classes\"][1:,:]-roc[\"FPR_all_classes\"][:-1,]\n",
    "    TPR_sum = roc[\"TPR_all_classes\"][1:,:]+roc[\"TPR_all_classes\"][:-1,:]\n",
    "    sum_FPR_TPR= torch.sum(FPR_diff*TPR_sum/2.0,dim=0)\n",
    "    count_non_defined=(sum_FPR_TPR == 0).sum()\n",
    "    if count_non_defined==pred_tensor.size(1):\n",
    "        return 0\n",
    "    return  sum_FPR_TPR.sum()/(pred_tensor.size(1)-count_non_defined)\n",
    "def Proposed_AUM_macro(pred_tensor, label_tensor):\n",
    "\n",
    "    roc = ROC_curve_macro(pred_tensor, label_tensor)\n",
    "    min_FPR_FNR = roc[\"min(FPR,FNR)\"][1:-1,:]\n",
    "    constant_diff = roc[\"min_constant\"][1:,:].diff(dim=0)\n",
    "    sum_min= torch.sum(min_FPR_FNR * constant_diff,dim=0)\n",
    "    count_non_defined=(sum_min== 0).sum()\n",
    "    if count_non_defined==pred_tensor.size(1):\n",
    "        return 0\n",
    "    return  sum_min.sum()/(pred_tensor.size(1)-count_non_defined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a3c4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   runs parallel to the first game and follows the \" Nameless \" , a penal military unit serv\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "sequence_len = 128\n",
    "num_steps = 300\n",
    "train_inputs, train_targets, _, _ = get_dataset(10, sequence_len, 0)\n",
    "config = GPTConfig(\n",
    "    vocab_size=tokenizer.n_vocab,\n",
    "    n_layer=4,   # fewer layers for a quick demo\n",
    "    n_head=4,\n",
    "    n_embd=128,\n",
    "    seq_len=sequence_len,\n",
    ")\n",
    "\n",
    "    \n",
    "print(\"Original: \", tokenizer.decode(train_inputs[1].tolist())[:90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18e219db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 13,707,857\n",
      "Trainable parameters: 13,707,857\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "total_params, trainable_params = count_parameters(loaded_CE)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca7878ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2/1000, AUM_micro Loss: 0.45409220457077026, LR: 0.001\n",
      "Step 3/1000, AUM_micro Loss: 0.40841758251190186, LR: 0.001\n",
      "Step 4/1000, AUM_micro Loss: 0.36697402596473694, LR: 0.001\n",
      "Step 5/1000, AUM_micro Loss: 0.3294964134693146, LR: 0.001\n",
      "Step 6/1000, AUM_micro Loss: 0.2882547676563263, LR: 0.001\n",
      "Step 7/1000, AUM_micro Loss: 0.24000276625156403, LR: 0.001\n",
      "Step 8/1000, AUM_micro Loss: 0.20470930635929108, LR: 0.001\n",
      "Step 9/1000, AUM_micro Loss: 0.16551318764686584, LR: 0.001\n",
      "Step 10/1000, AUM_micro Loss: 0.13269056379795074, LR: 0.001\n",
      "Step 11/1000, AUM_micro Loss: 0.10919132828712463, LR: 0.001\n",
      "Step 12/1000, AUM_micro Loss: 0.08454707264900208, LR: 0.001\n",
      "Step 13/1000, AUM_micro Loss: 0.06617487967014313, LR: 0.001\n",
      "Step 14/1000, AUM_micro Loss: 0.04881587624549866, LR: 0.001\n",
      "Step 15/1000, AUM_micro Loss: 0.03705509379506111, LR: 0.001\n",
      "Step 16/1000, AUM_micro Loss: 0.028202300891280174, LR: 0.001\n",
      "Step 17/1000, AUM_micro Loss: 0.020419776439666748, LR: 0.001\n",
      "Step 18/1000, AUM_micro Loss: 0.015437465161085129, LR: 0.001\n",
      "Step 19/1000, AUM_micro Loss: 0.011419765651226044, LR: 0.001\n",
      "Step 20/1000, AUM_micro Loss: 0.009510451927781105, LR: 0.001\n",
      "Step 21/1000, AUM_micro Loss: 0.008352273143827915, LR: 0.001\n",
      "Step 22/1000, AUM_micro Loss: 0.007129867561161518, LR: 0.001\n",
      "Step 23/1000, AUM_micro Loss: 0.0061750998720526695, LR: 0.001\n",
      "Step 24/1000, AUM_micro Loss: 0.005461083725094795, LR: 0.001\n",
      "Step 25/1000, AUM_micro Loss: 0.00491169560700655, LR: 0.001\n",
      "Step 26/1000, AUM_micro Loss: 0.005196661222726107, LR: 0.001\n",
      "Step 27/1000, AUM_micro Loss: 0.00554743641987443, LR: 0.001\n",
      "Step 28/1000, AUM_micro Loss: 0.005045278463512659, LR: 0.001\n",
      "Step 29/1000, AUM_micro Loss: 0.005451088305562735, LR: 0.001\n",
      "Step 30/1000, AUM_micro Loss: 0.005363342352211475, LR: 0.001\n",
      "Step 31/1000, AUM_micro Loss: 0.005697514861822128, LR: 0.001\n",
      "Step 32/1000, AUM_micro Loss: 0.006128411740064621, LR: 0.001\n",
      "Step 33/1000, AUM_micro Loss: 0.006542163901031017, LR: 0.001\n",
      "Step 34/1000, AUM_micro Loss: 0.006543990224599838, LR: 0.001\n",
      "Step 35/1000, AUM_micro Loss: 0.006822843104600906, LR: 0.001\n",
      "Step 36/1000, AUM_micro Loss: 0.007053106091916561, LR: 0.001\n",
      "Step 37/1000, AUM_micro Loss: 0.006765064783394337, LR: 0.001\n",
      "Step 38/1000, AUM_micro Loss: 0.006960107013583183, LR: 0.001\n",
      "Step 39/1000, AUM_micro Loss: 0.006692447699606419, LR: 0.001\n",
      "Step 40/1000, AUM_micro Loss: 0.006566083990037441, LR: 0.001\n",
      "Step 41/1000, AUM_micro Loss: 0.006680912338197231, LR: 0.001\n",
      "Step 42/1000, AUM_micro Loss: 0.00674823671579361, LR: 0.001\n",
      "Step 43/1000, AUM_micro Loss: 0.006326279137283564, LR: 0.001\n",
      "Step 44/1000, AUM_micro Loss: 0.006601395551115274, LR: 0.001\n",
      "Step 45/1000, AUM_micro Loss: 0.006585926748812199, LR: 0.001\n",
      "Step 46/1000, AUM_micro Loss: 0.0063811070285737514, LR: 0.0002\n",
      "Step 47/1000, AUM_micro Loss: 0.0063378517515957355, LR: 0.0002\n",
      "Step 48/1000, AUM_micro Loss: 0.005929718725383282, LR: 0.0002\n",
      "Step 49/1000, AUM_micro Loss: 0.005877197254449129, LR: 0.0002\n",
      "Step 50/1000, AUM_micro Loss: 0.005542464554309845, LR: 0.0002\n",
      "Step 51/1000, AUM_micro Loss: 0.00531751150265336, LR: 0.0002\n",
      "Step 52/1000, AUM_micro Loss: 0.00512370839715004, LR: 0.0002\n",
      "Step 53/1000, AUM_micro Loss: 0.0048264856450259686, LR: 0.0002\n",
      "Step 54/1000, AUM_micro Loss: 0.0047473968006670475, LR: 0.0002\n",
      "Step 55/1000, AUM_micro Loss: 0.004611697047948837, LR: 0.0002\n",
      "Step 56/1000, AUM_micro Loss: 0.00460427813231945, LR: 0.0002\n",
      "Step 57/1000, AUM_micro Loss: 0.004446014296263456, LR: 0.0002\n",
      "Step 58/1000, AUM_micro Loss: 0.004301062785089016, LR: 0.0002\n",
      "Step 59/1000, AUM_micro Loss: 0.0042519764974713326, LR: 0.0002\n",
      "Step 60/1000, AUM_micro Loss: 0.004231561906635761, LR: 0.0002\n",
      "Step 61/1000, AUM_micro Loss: 0.004330158233642578, LR: 0.0002\n",
      "Step 62/1000, AUM_micro Loss: 0.00412136223167181, LR: 0.0002\n",
      "Step 63/1000, AUM_micro Loss: 0.0038551073521375656, LR: 0.0002\n",
      "Step 64/1000, AUM_micro Loss: 0.003670834004878998, LR: 0.0002\n",
      "Step 65/1000, AUM_micro Loss: 0.003576230723410845, LR: 0.0002\n",
      "Step 66/1000, AUM_micro Loss: 0.0034440127201378345, LR: 0.0002\n",
      "Step 67/1000, AUM_micro Loss: 0.003357979469001293, LR: 0.0002\n",
      "Step 68/1000, AUM_micro Loss: 0.00332904071547091, LR: 0.0002\n",
      "Step 69/1000, AUM_micro Loss: 0.003183895256370306, LR: 0.0002\n",
      "Step 70/1000, AUM_micro Loss: 0.003225094173103571, LR: 0.0002\n",
      "Step 71/1000, AUM_micro Loss: 0.0030705237295478582, LR: 0.0002\n",
      "Step 72/1000, AUM_micro Loss: 0.002973270369693637, LR: 0.0002\n",
      "Step 73/1000, AUM_micro Loss: 0.002973350929096341, LR: 0.0002\n",
      "Step 74/1000, AUM_micro Loss: 0.0028988963458687067, LR: 0.0002\n",
      "Step 75/1000, AUM_micro Loss: 0.002767696976661682, LR: 0.0002\n",
      "Step 76/1000, AUM_micro Loss: 0.002835595980286598, LR: 0.0002\n",
      "Step 77/1000, AUM_micro Loss: 0.0026777656748890877, LR: 0.0002\n",
      "Step 78/1000, AUM_micro Loss: 0.0026151128113269806, LR: 0.0002\n",
      "Step 79/1000, AUM_micro Loss: 0.002515422645956278, LR: 0.0002\n",
      "Step 80/1000, AUM_micro Loss: 0.0024654152803122997, LR: 0.0002\n",
      "Step 81/1000, AUM_micro Loss: 0.0024885726161301136, LR: 0.0002\n",
      "Step 82/1000, AUM_micro Loss: 0.0023987586610019207, LR: 0.0002\n",
      "Step 83/1000, AUM_micro Loss: 0.0024271649308502674, LR: 0.0002\n",
      "Step 84/1000, AUM_micro Loss: 0.002382969483733177, LR: 0.0002\n",
      "Step 85/1000, AUM_micro Loss: 0.0023135181982070208, LR: 0.0002\n",
      "Step 86/1000, AUM_micro Loss: 0.0022743178997188807, LR: 0.0002\n",
      "Step 87/1000, AUM_micro Loss: 0.002153775654733181, LR: 0.0002\n",
      "Step 88/1000, AUM_micro Loss: 0.002287876093760133, LR: 0.0002\n",
      "Step 89/1000, AUM_micro Loss: 0.002185909543186426, LR: 0.0002\n",
      "Step 90/1000, AUM_micro Loss: 0.00219415663741529, LR: 0.0002\n",
      "Step 91/1000, AUM_micro Loss: 0.0021917882841080427, LR: 0.0002\n",
      "Step 92/1000, AUM_micro Loss: 0.0021669352427124977, LR: 0.0002\n",
      "Step 93/1000, AUM_micro Loss: 0.0020591251086443663, LR: 0.0002\n",
      "Step 94/1000, AUM_micro Loss: 0.002212061081081629, LR: 0.0002\n",
      "Step 95/1000, AUM_micro Loss: 0.0021516303531825542, LR: 0.0002\n",
      "Step 96/1000, AUM_micro Loss: 0.0021029850468039513, LR: 0.0002\n",
      "Step 97/1000, AUM_micro Loss: 0.0020589365158230066, LR: 0.0002\n",
      "Step 98/1000, AUM_micro Loss: 0.0019100377103313804, LR: 0.0002\n",
      "Step 99/1000, AUM_micro Loss: 0.0018295866902917624, LR: 0.0002\n",
      "Step 100/1000, AUM_micro Loss: 0.0018613330321386456, LR: 0.0002\n",
      "Step 101/1000, AUM_micro Loss: 0.0018462989246472716, LR: 0.0002\n",
      "Step 102/1000, AUM_micro Loss: 0.0018775113858282566, LR: 0.0002\n",
      "Step 103/1000, AUM_micro Loss: 0.0017069352325052023, LR: 0.0002\n",
      "Step 104/1000, AUM_micro Loss: 0.0017420705407857895, LR: 0.0002\n",
      "Step 105/1000, AUM_micro Loss: 0.0016615272033959627, LR: 0.0002\n",
      "Step 106/1000, AUM_micro Loss: 0.001694136648438871, LR: 0.0002\n",
      "Step 107/1000, AUM_micro Loss: 0.0016510756686329842, LR: 0.0002\n",
      "Step 108/1000, AUM_micro Loss: 0.0015956287970766425, LR: 0.0002\n",
      "Step 109/1000, AUM_micro Loss: 0.001574683585204184, LR: 0.0002\n",
      "Step 110/1000, AUM_micro Loss: 0.0015441534342244267, LR: 0.0002\n",
      "Step 111/1000, AUM_micro Loss: 0.001490089576691389, LR: 0.0002\n",
      "Step 112/1000, AUM_micro Loss: 0.0015534092672169209, LR: 0.0002\n",
      "Step 113/1000, AUM_micro Loss: 0.0014452413888648152, LR: 0.0002\n",
      "Step 114/1000, AUM_micro Loss: 0.0015433859080076218, LR: 0.0002\n",
      "Step 115/1000, AUM_micro Loss: 0.0014243301702663302, LR: 0.0002\n",
      "Step 116/1000, AUM_micro Loss: 0.0013884701766073704, LR: 0.0002\n",
      "Step 117/1000, AUM_micro Loss: 0.0013996484922245145, LR: 0.0002\n",
      "Step 118/1000, AUM_micro Loss: 0.0014830081490799785, LR: 0.0002\n",
      "Step 119/1000, AUM_micro Loss: 0.0013477894244715571, LR: 0.0002\n",
      "Step 120/1000, AUM_micro Loss: 0.0014024915872141719, LR: 0.0002\n",
      "Step 121/1000, AUM_micro Loss: 0.0013704441953450441, LR: 0.0002\n",
      "Step 122/1000, AUM_micro Loss: 0.0013481939677149057, LR: 0.0002\n",
      "Step 123/1000, AUM_micro Loss: 0.0013340822188183665, LR: 0.0002\n",
      "Step 124/1000, AUM_micro Loss: 0.0012635634047910571, LR: 0.0002\n",
      "Step 125/1000, AUM_micro Loss: 0.0013869802933186293, LR: 0.0002\n",
      "Step 126/1000, AUM_micro Loss: 0.0014093827921897173, LR: 0.0002\n",
      "Step 127/1000, AUM_micro Loss: 0.0012590814149007201, LR: 0.0002\n",
      "Step 128/1000, AUM_micro Loss: 0.0013625284191220999, LR: 0.0002\n",
      "Step 129/1000, AUM_micro Loss: 0.001332488958723843, LR: 0.0002\n",
      "Step 130/1000, AUM_micro Loss: 0.0012370436452329159, LR: 0.0002\n",
      "Step 131/1000, AUM_micro Loss: 0.0012387093156576157, LR: 0.0002\n",
      "Step 132/1000, AUM_micro Loss: 0.0011889694724231958, LR: 0.0002\n",
      "Step 133/1000, AUM_micro Loss: 0.0013253205688670278, LR: 0.0002\n",
      "Step 134/1000, AUM_micro Loss: 0.0013679887633770704, LR: 0.0002\n",
      "Step 135/1000, AUM_micro Loss: 0.001287905965000391, LR: 0.0002\n",
      "Step 136/1000, AUM_micro Loss: 0.0011816981714218855, LR: 0.0002\n",
      "Step 137/1000, AUM_micro Loss: 0.001198591198772192, LR: 0.0002\n",
      "Step 138/1000, AUM_micro Loss: 0.0011840460356324911, LR: 0.0002\n",
      "Step 139/1000, AUM_micro Loss: 0.001173600321635604, LR: 0.0002\n",
      "Step 140/1000, AUM_micro Loss: 0.001191651914268732, LR: 0.0002\n",
      "Step 141/1000, AUM_micro Loss: 0.0011222430039197206, LR: 0.0002\n",
      "Step 142/1000, AUM_micro Loss: 0.001145515707321465, LR: 0.0002\n",
      "Step 143/1000, AUM_micro Loss: 0.0011252143885940313, LR: 0.0002\n",
      "Step 144/1000, AUM_micro Loss: 0.0011187094496563077, LR: 0.0002\n",
      "Step 145/1000, AUM_micro Loss: 0.0010678383987396955, LR: 0.0002\n",
      "Step 146/1000, AUM_micro Loss: 0.0010432301787659526, LR: 0.0002\n",
      "Step 147/1000, AUM_micro Loss: 0.0010144388070330024, LR: 0.0002\n",
      "Step 148/1000, AUM_micro Loss: 0.0009927725186571479, LR: 0.0002\n",
      "Step 149/1000, AUM_micro Loss: 0.001153641496784985, LR: 0.0002\n",
      "Step 150/1000, AUM_micro Loss: 0.0010210433974862099, LR: 0.0002\n",
      "Step 151/1000, AUM_micro Loss: 0.0010500975186005235, LR: 0.0002\n",
      "Step 152/1000, AUM_micro Loss: 0.001009291736409068, LR: 0.0002\n",
      "Step 153/1000, AUM_micro Loss: 0.0009669458377175033, LR: 0.0002\n",
      "Step 154/1000, AUM_micro Loss: 0.0010385469067841768, LR: 0.0002\n",
      "Step 155/1000, AUM_micro Loss: 0.0009487274219281971, LR: 0.0002\n",
      "Step 156/1000, AUM_micro Loss: 0.0009510944364592433, LR: 0.0002\n",
      "Step 157/1000, AUM_micro Loss: 0.0009892533998936415, LR: 0.0002\n",
      "Step 158/1000, AUM_micro Loss: 0.0008983329171314836, LR: 0.0002\n",
      "Step 159/1000, AUM_micro Loss: 0.0008894589263945818, LR: 0.0002\n",
      "Step 160/1000, AUM_micro Loss: 0.0008558995323255658, LR: 0.0002\n",
      "Step 161/1000, AUM_micro Loss: 0.0009292273898608983, LR: 0.0002\n",
      "Step 162/1000, AUM_micro Loss: 0.0008488959283567965, LR: 0.0002\n",
      "Step 163/1000, AUM_micro Loss: 0.0008620225125923753, LR: 0.0002\n",
      "Step 164/1000, AUM_micro Loss: 0.0008492573979310691, LR: 0.0002\n",
      "Step 165/1000, AUM_micro Loss: 0.0009384581935591996, LR: 0.0002\n",
      "Step 166/1000, AUM_micro Loss: 0.0008822799427434802, LR: 0.0002\n",
      "Step 167/1000, AUM_micro Loss: 0.0009126298828050494, LR: 0.0002\n",
      "Step 168/1000, AUM_micro Loss: 0.0008443632395938039, LR: 0.0002\n",
      "Step 169/1000, AUM_micro Loss: 0.0008713510469533503, LR: 0.0002\n",
      "Step 170/1000, AUM_micro Loss: 0.0008416796335950494, LR: 0.0002\n",
      "Step 171/1000, AUM_micro Loss: 0.0008428852306678891, LR: 0.0002\n",
      "Step 172/1000, AUM_micro Loss: 0.0008662859909236431, LR: 0.0002\n",
      "Step 173/1000, AUM_micro Loss: 0.0008668536902405322, LR: 0.0002\n",
      "Step 174/1000, AUM_micro Loss: 0.0008301279740408063, LR: 0.0002\n",
      "Step 175/1000, AUM_micro Loss: 0.000858606246765703, LR: 0.0002\n",
      "Step 176/1000, AUM_micro Loss: 0.0009017076226882637, LR: 0.0002\n",
      "Step 177/1000, AUM_micro Loss: 0.000804736337158829, LR: 0.0002\n",
      "Step 178/1000, AUM_micro Loss: 0.0009308852022513747, LR: 0.0002\n",
      "Step 179/1000, AUM_micro Loss: 0.0008720679907128215, LR: 0.0002\n",
      "Step 180/1000, AUM_micro Loss: 0.0008515655645169318, LR: 0.0002\n",
      "Step 181/1000, AUM_micro Loss: 0.000785052077844739, LR: 0.0002\n",
      "Step 182/1000, AUM_micro Loss: 0.0010304859606549144, LR: 0.0002\n",
      "Step 183/1000, AUM_micro Loss: 0.0008070511976256967, LR: 0.0002\n",
      "Step 184/1000, AUM_micro Loss: 0.0008823373937048018, LR: 0.0002\n",
      "Step 185/1000, AUM_micro Loss: 0.0009470867225900292, LR: 0.0002\n",
      "Step 186/1000, AUM_micro Loss: 0.0009358460083603859, LR: 0.0002\n",
      "Step 187/1000, AUM_micro Loss: 0.0008089220500551164, LR: 0.0002\n",
      "Step 188/1000, AUM_micro Loss: 0.0008622315945103765, LR: 0.0002\n",
      "Step 189/1000, AUM_micro Loss: 0.0008762768120504916, LR: 0.0002\n",
      "Step 190/1000, AUM_micro Loss: 0.0008285187650471926, LR: 0.0002\n",
      "Step 191/1000, AUM_micro Loss: 0.0007700232090428472, LR: 0.0002\n",
      "Step 192/1000, AUM_micro Loss: 0.0008293436840176582, LR: 0.0002\n",
      "Step 193/1000, AUM_micro Loss: 0.0007855337462387979, LR: 0.0002\n",
      "Step 194/1000, AUM_micro Loss: 0.0008016449864953756, LR: 0.0002\n",
      "Step 195/1000, AUM_micro Loss: 0.0007516314508393407, LR: 0.0002\n",
      "Step 196/1000, AUM_micro Loss: 0.0007160838576965034, LR: 0.0002\n",
      "Step 197/1000, AUM_micro Loss: 0.0007427114760503173, LR: 0.0002\n",
      "Step 198/1000, AUM_micro Loss: 0.000683178601320833, LR: 0.0002\n",
      "Step 199/1000, AUM_micro Loss: 0.0007721787551417947, LR: 0.0002\n",
      "Step 200/1000, AUM_micro Loss: 0.0008062312845140696, LR: 0.0002\n",
      "Step 201/1000, AUM_micro Loss: 0.0008359047351405025, LR: 0.0002\n",
      "Step 202/1000, AUM_micro Loss: 0.0007398262387141585, LR: 0.0002\n",
      "Step 203/1000, AUM_micro Loss: 0.0007565274718217552, LR: 0.0002\n",
      "Step 204/1000, AUM_micro Loss: 0.0007044376106932759, LR: 0.0002\n",
      "Step 205/1000, AUM_micro Loss: 0.0006753550842404366, LR: 0.0002\n",
      "Step 206/1000, AUM_micro Loss: 0.0007220119005069137, LR: 0.0002\n",
      "Step 207/1000, AUM_micro Loss: 0.0006991850095801055, LR: 0.0002\n",
      "Step 208/1000, AUM_micro Loss: 0.0006947463261894882, LR: 0.0002\n",
      "Step 209/1000, AUM_micro Loss: 0.0006850116769783199, LR: 0.0002\n",
      "Step 210/1000, AUM_micro Loss: 0.0006738025695085526, LR: 0.0002\n",
      "Step 211/1000, AUM_micro Loss: 0.0008145272731781006, LR: 0.0002\n",
      "Step 212/1000, AUM_micro Loss: 0.000797862303443253, LR: 0.0002\n",
      "Step 213/1000, AUM_micro Loss: 0.0008380283834412694, LR: 0.0002\n",
      "Step 214/1000, AUM_micro Loss: 0.0007840123143978417, LR: 0.0002\n",
      "Step 215/1000, AUM_micro Loss: 0.0007496628095395863, LR: 0.0002\n",
      "Step 216/1000, AUM_micro Loss: 0.0008177637355402112, LR: 0.0002\n",
      "Step 217/1000, AUM_micro Loss: 0.0006976093864068389, LR: 0.0002\n",
      "Step 218/1000, AUM_micro Loss: 0.0007192672928795218, LR: 0.0002\n",
      "Step 219/1000, AUM_micro Loss: 0.0006873958627693355, LR: 0.0002\n",
      "Step 220/1000, AUM_micro Loss: 0.0006852614460512996, LR: 0.0002\n",
      "Step 221/1000, AUM_micro Loss: 0.0006368848262354732, LR: 0.0002\n",
      "Step 222/1000, AUM_micro Loss: 0.0006696587079204619, LR: 0.0002\n",
      "Step 223/1000, AUM_micro Loss: 0.0007064155070111156, LR: 0.0002\n",
      "Step 224/1000, AUM_micro Loss: 0.0006137880263850093, LR: 0.0002\n",
      "Step 225/1000, AUM_micro Loss: 0.0006816867389716208, LR: 0.0002\n",
      "Step 226/1000, AUM_micro Loss: 0.00069080526009202, LR: 0.0002\n",
      "Step 227/1000, AUM_micro Loss: 0.0006190065760165453, LR: 0.0002\n",
      "Step 228/1000, AUM_micro Loss: 0.0006392973009496927, LR: 0.0002\n",
      "Step 229/1000, AUM_micro Loss: 0.0006293986807577312, LR: 0.0002\n",
      "Step 230/1000, AUM_micro Loss: 0.0006242083618417382, LR: 0.0002\n",
      "Step 231/1000, AUM_micro Loss: 0.000590663286857307, LR: 0.0002\n",
      "Step 232/1000, AUM_micro Loss: 0.0006541941547766328, LR: 0.0002\n",
      "Step 233/1000, AUM_micro Loss: 0.0007894656155258417, LR: 0.0002\n",
      "Step 234/1000, AUM_micro Loss: 0.0006105188513174653, LR: 0.0002\n",
      "Step 235/1000, AUM_micro Loss: 0.0006532033439725637, LR: 0.0002\n",
      "Step 236/1000, AUM_micro Loss: 0.0006185919628478587, LR: 0.0002\n",
      "Step 237/1000, AUM_micro Loss: 0.0007573225884698331, LR: 0.0002\n",
      "Step 238/1000, AUM_micro Loss: 0.0005654964479617774, LR: 0.0002\n",
      "Step 239/1000, AUM_micro Loss: 0.0006290249293670058, LR: 0.0002\n",
      "Step 240/1000, AUM_micro Loss: 0.0006174871232360601, LR: 0.0002\n",
      "Step 241/1000, AUM_micro Loss: 0.0006485728081315756, LR: 0.0002\n",
      "Step 242/1000, AUM_micro Loss: 0.000750910781789571, LR: 0.0002\n",
      "Step 243/1000, AUM_micro Loss: 0.0008073991048149765, LR: 0.0002\n",
      "Step 244/1000, AUM_micro Loss: 0.0007201538537628949, LR: 0.0002\n",
      "Step 245/1000, AUM_micro Loss: 0.0006455061957240105, LR: 0.0002\n",
      "Step 246/1000, AUM_micro Loss: 0.0006337158265523612, LR: 0.0002\n",
      "Step 247/1000, AUM_micro Loss: 0.0006682794773951173, LR: 0.0002\n",
      "Step 248/1000, AUM_micro Loss: 0.0006136760930530727, LR: 0.0002\n",
      "Step 249/1000, AUM_micro Loss: 0.0006132959388196468, LR: 0.0002\n",
      "Step 250/1000, AUM_micro Loss: 0.0006753935012966394, LR: 0.0002\n",
      "Step 251/1000, AUM_micro Loss: 0.0006055211997590959, LR: 0.0002\n",
      "Step 252/1000, AUM_micro Loss: 0.0007566654239781201, LR: 0.0002\n",
      "Step 253/1000, AUM_micro Loss: 0.0007101399241946638, LR: 0.0002\n",
      "Step 254/1000, AUM_micro Loss: 0.0006988794775679708, LR: 0.0002\n",
      "Step 255/1000, AUM_micro Loss: 0.000666234118398279, LR: 0.0002\n",
      "Step 256/1000, AUM_micro Loss: 0.000608119647949934, LR: 0.0002\n",
      "Step 257/1000, AUM_micro Loss: 0.0006562009220942855, LR: 0.0002\n",
      "Step 258/1000, AUM_micro Loss: 0.0006132114212960005, LR: 0.0002\n",
      "Step 259/1000, AUM_micro Loss: 0.000741723517421633, LR: 4e-05\n",
      "Step 260/1000, AUM_micro Loss: 0.0006468056817539036, LR: 4e-05\n",
      "Step 261/1000, AUM_micro Loss: 0.0006179424235597253, LR: 4e-05\n",
      "Step 262/1000, AUM_micro Loss: 0.0006361622945405543, LR: 4e-05\n",
      "Step 263/1000, AUM_micro Loss: 0.0006107895169407129, LR: 4e-05\n",
      "Step 264/1000, AUM_micro Loss: 0.0005858530639670789, LR: 4e-05\n",
      "Step 265/1000, AUM_micro Loss: 0.0005562564474530518, LR: 4e-05\n",
      "Step 266/1000, AUM_micro Loss: 0.0005644026095978916, LR: 4e-05\n",
      "Step 267/1000, AUM_micro Loss: 0.000534150458406657, LR: 4e-05\n",
      "Step 268/1000, AUM_micro Loss: 0.0005386155098676682, LR: 4e-05\n",
      "Step 269/1000, AUM_micro Loss: 0.0005165565526112914, LR: 4e-05\n",
      "Step 270/1000, AUM_micro Loss: 0.0005062433192506433, LR: 4e-05\n",
      "Step 271/1000, AUM_micro Loss: 0.0005007932777516544, LR: 4e-05\n",
      "Step 272/1000, AUM_micro Loss: 0.00048500957200303674, LR: 4e-05\n",
      "Step 273/1000, AUM_micro Loss: 0.000476628280011937, LR: 4e-05\n",
      "Step 274/1000, AUM_micro Loss: 0.0004678185214288533, LR: 4e-05\n",
      "Step 275/1000, AUM_micro Loss: 0.0004914563032798469, LR: 4e-05\n",
      "Step 276/1000, AUM_micro Loss: 0.0004597124643623829, LR: 4e-05\n",
      "Step 277/1000, AUM_micro Loss: 0.0004528772260528058, LR: 4e-05\n",
      "Step 278/1000, AUM_micro Loss: 0.0004479707858990878, LR: 4e-05\n",
      "Step 279/1000, AUM_micro Loss: 0.0004420517070684582, LR: 4e-05\n",
      "Step 280/1000, AUM_micro Loss: 0.000442681455751881, LR: 4e-05\n",
      "Step 281/1000, AUM_micro Loss: 0.00043187360279262066, LR: 4e-05\n",
      "Step 282/1000, AUM_micro Loss: 0.0004344831395428628, LR: 4e-05\n",
      "Step 283/1000, AUM_micro Loss: 0.00042474872316233814, LR: 4e-05\n",
      "Step 284/1000, AUM_micro Loss: 0.00044383175554685295, LR: 4e-05\n",
      "Step 285/1000, AUM_micro Loss: 0.00042441097320988774, LR: 4e-05\n",
      "Step 286/1000, AUM_micro Loss: 0.0004581580578815192, LR: 4e-05\n",
      "Step 287/1000, AUM_micro Loss: 0.00041475100442767143, LR: 4e-05\n",
      "Step 288/1000, AUM_micro Loss: 0.0004506502009462565, LR: 4e-05\n",
      "Step 289/1000, AUM_micro Loss: 0.0004000677145086229, LR: 4e-05\n",
      "Step 290/1000, AUM_micro Loss: 0.00043510907562449574, LR: 4e-05\n",
      "Step 291/1000, AUM_micro Loss: 0.0004391768015921116, LR: 4e-05\n",
      "Step 292/1000, AUM_micro Loss: 0.00039451848715543747, LR: 4e-05\n",
      "Step 293/1000, AUM_micro Loss: 0.00039144227048382163, LR: 4e-05\n",
      "Step 294/1000, AUM_micro Loss: 0.0003887825587298721, LR: 4e-05\n",
      "Step 295/1000, AUM_micro Loss: 0.00048326270189136267, LR: 4e-05\n",
      "Step 296/1000, AUM_micro Loss: 0.0004888283438049257, LR: 4e-05\n",
      "Step 297/1000, AUM_micro Loss: 0.0004527592100203037, LR: 4e-05\n",
      "Step 298/1000, AUM_micro Loss: 0.0005070526385679841, LR: 4e-05\n",
      "Step 299/1000, AUM_micro Loss: 0.00047375683789141476, LR: 4e-05\n",
      "Step 300/1000, AUM_micro Loss: 0.0004754302790388465, LR: 4e-05\n",
      "Step 301/1000, AUM_micro Loss: 0.00045490512275137007, LR: 4e-05\n",
      "Step 302/1000, AUM_micro Loss: 0.00042110501090064645, LR: 4e-05\n",
      "Step 303/1000, AUM_micro Loss: 0.000399023643694818, LR: 4e-05\n",
      "Step 304/1000, AUM_micro Loss: 0.00037772548967041075, LR: 4e-05\n",
      "Step 305/1000, AUM_micro Loss: 0.0003748680173885077, LR: 4e-05\n",
      "Step 306/1000, AUM_micro Loss: 0.00040758834802545607, LR: 4e-05\n",
      "Step 307/1000, AUM_micro Loss: 0.00037131443968974054, LR: 4e-05\n",
      "Step 308/1000, AUM_micro Loss: 0.00040285405702888966, LR: 4e-05\n",
      "Step 309/1000, AUM_micro Loss: 0.00035934531479142606, LR: 4e-05\n",
      "Step 310/1000, AUM_micro Loss: 0.0003588000836316496, LR: 4e-05\n",
      "Step 311/1000, AUM_micro Loss: 0.0003561576013453305, LR: 4e-05\n",
      "Step 312/1000, AUM_micro Loss: 0.00035275713889859617, LR: 4e-05\n",
      "Step 313/1000, AUM_micro Loss: 0.0003602045471780002, LR: 4e-05\n",
      "Step 314/1000, AUM_micro Loss: 0.0003471144300419837, LR: 4e-05\n",
      "Step 315/1000, AUM_micro Loss: 0.00034582155058160424, LR: 4e-05\n",
      "Step 316/1000, AUM_micro Loss: 0.00034343916922807693, LR: 4e-05\n",
      "Step 317/1000, AUM_micro Loss: 0.00034487180528230965, LR: 4e-05\n",
      "Step 318/1000, AUM_micro Loss: 0.0003432536032050848, LR: 4e-05\n",
      "Step 319/1000, AUM_micro Loss: 0.0003414025704842061, LR: 4e-05\n",
      "Step 320/1000, AUM_micro Loss: 0.0003405993338674307, LR: 4e-05\n",
      "Step 321/1000, AUM_micro Loss: 0.0003359926340635866, LR: 4e-05\n",
      "Step 322/1000, AUM_micro Loss: 0.00033302235533483326, LR: 4e-05\n",
      "Step 323/1000, AUM_micro Loss: 0.00033043805160559714, LR: 4e-05\n",
      "Step 324/1000, AUM_micro Loss: 0.0003393821243662387, LR: 4e-05\n",
      "Step 325/1000, AUM_micro Loss: 0.0003418277483433485, LR: 4e-05\n",
      "Step 326/1000, AUM_micro Loss: 0.0003243439714424312, LR: 4e-05\n",
      "Step 327/1000, AUM_micro Loss: 0.0003224264073651284, LR: 4e-05\n",
      "Step 328/1000, AUM_micro Loss: 0.00032536551589146256, LR: 4e-05\n",
      "Step 329/1000, AUM_micro Loss: 0.0003212281735613942, LR: 4e-05\n",
      "Step 330/1000, AUM_micro Loss: 0.00033244641963392496, LR: 4e-05\n",
      "Step 331/1000, AUM_micro Loss: 0.0003490719245746732, LR: 4e-05\n",
      "Step 332/1000, AUM_micro Loss: 0.0003374167426954955, LR: 4e-05\n",
      "Step 333/1000, AUM_micro Loss: 0.00032610323978587985, LR: 4e-05\n",
      "Step 334/1000, AUM_micro Loss: 0.0003783759893849492, LR: 4e-05\n",
      "Step 335/1000, AUM_micro Loss: 0.00032060075318440795, LR: 4e-05\n",
      "Step 336/1000, AUM_micro Loss: 0.00031831147498451173, LR: 4e-05\n",
      "Step 337/1000, AUM_micro Loss: 0.00031807529740035534, LR: 4e-05\n",
      "Step 338/1000, AUM_micro Loss: 0.00034528333344496787, LR: 4e-05\n",
      "Step 339/1000, AUM_micro Loss: 0.0004097720666322857, LR: 4e-05\n",
      "Step 340/1000, AUM_micro Loss: 0.00040952852577902377, LR: 4e-05\n",
      "Step 341/1000, AUM_micro Loss: 0.00039320276118814945, LR: 4e-05\n",
      "Step 342/1000, AUM_micro Loss: 0.00031960057094693184, LR: 4e-05\n",
      "Step 343/1000, AUM_micro Loss: 0.00031563855009153485, LR: 4e-05\n",
      "Step 344/1000, AUM_micro Loss: 0.00031220560777001083, LR: 4e-05\n",
      "Step 345/1000, AUM_micro Loss: 0.0003156586899422109, LR: 4e-05\n",
      "Step 346/1000, AUM_micro Loss: 0.00031414389377459884, LR: 4e-05\n",
      "Step 347/1000, AUM_micro Loss: 0.0003097229346167296, LR: 4e-05\n",
      "Step 348/1000, AUM_micro Loss: 0.00031100783962756395, LR: 4e-05\n",
      "Step 349/1000, AUM_micro Loss: 0.0003076358698308468, LR: 4e-05\n",
      "Step 350/1000, AUM_micro Loss: 0.00030553812393918633, LR: 4e-05\n",
      "Step 351/1000, AUM_micro Loss: 0.00030293394229374826, LR: 4e-05\n",
      "Step 352/1000, AUM_micro Loss: 0.00030167767545208335, LR: 4e-05\n",
      "Step 353/1000, AUM_micro Loss: 0.0003000088327098638, LR: 4e-05\n",
      "Step 354/1000, AUM_micro Loss: 0.0003092450206167996, LR: 4e-05\n",
      "Step 355/1000, AUM_micro Loss: 0.0002987277111969888, LR: 4e-05\n",
      "Step 356/1000, AUM_micro Loss: 0.00029483428806997836, LR: 4e-05\n",
      "Step 357/1000, AUM_micro Loss: 0.00030032871291041374, LR: 4e-05\n",
      "Step 358/1000, AUM_micro Loss: 0.0002997852861881256, LR: 4e-05\n",
      "Step 359/1000, AUM_micro Loss: 0.00029559587710537016, LR: 4e-05\n",
      "Step 360/1000, AUM_micro Loss: 0.00029623639420606196, LR: 4e-05\n",
      "Step 361/1000, AUM_micro Loss: 0.00029725025524385273, LR: 4e-05\n",
      "Step 362/1000, AUM_micro Loss: 0.0002991918008774519, LR: 4e-05\n",
      "Step 363/1000, AUM_micro Loss: 0.0002925989974755794, LR: 4e-05\n",
      "Step 364/1000, AUM_micro Loss: 0.0002924588625319302, LR: 4e-05\n",
      "Step 365/1000, AUM_micro Loss: 0.0002939448459073901, LR: 4e-05\n",
      "Step 366/1000, AUM_micro Loss: 0.00029081007232889533, LR: 4e-05\n",
      "Step 367/1000, AUM_micro Loss: 0.00029103856650181115, LR: 4e-05\n",
      "Step 368/1000, AUM_micro Loss: 0.0002894765930250287, LR: 4e-05\n",
      "Step 369/1000, AUM_micro Loss: 0.0002889327588491142, LR: 4e-05\n",
      "Step 370/1000, AUM_micro Loss: 0.0002859435335267335, LR: 4e-05\n",
      "Step 371/1000, AUM_micro Loss: 0.00028509640833362937, LR: 4e-05\n",
      "Step 372/1000, AUM_micro Loss: 0.000303073029499501, LR: 4e-05\n",
      "Step 373/1000, AUM_micro Loss: 0.00029219940188340843, LR: 4e-05\n",
      "Step 374/1000, AUM_micro Loss: 0.0002861618413589895, LR: 4e-05\n",
      "Step 375/1000, AUM_micro Loss: 0.0003740924585144967, LR: 4e-05\n",
      "Step 376/1000, AUM_micro Loss: 0.0002812516177073121, LR: 4e-05\n",
      "Step 377/1000, AUM_micro Loss: 0.00029771827394142747, LR: 4e-05\n",
      "Step 378/1000, AUM_micro Loss: 0.00029448576970025897, LR: 4e-05\n",
      "Step 379/1000, AUM_micro Loss: 0.00033164650085382164, LR: 4e-05\n",
      "Step 380/1000, AUM_micro Loss: 0.0003158000763505697, LR: 4e-05\n",
      "Step 381/1000, AUM_micro Loss: 0.0003126189112663269, LR: 4e-05\n",
      "Step 382/1000, AUM_micro Loss: 0.00030278414487838745, LR: 4e-05\n",
      "Step 383/1000, AUM_micro Loss: 0.0002860736567527056, LR: 4e-05\n",
      "Step 384/1000, AUM_micro Loss: 0.0003118421009276062, LR: 4e-05\n",
      "Step 385/1000, AUM_micro Loss: 0.00028470897814258933, LR: 4e-05\n",
      "Step 386/1000, AUM_micro Loss: 0.0003066146746277809, LR: 4e-05\n",
      "Step 387/1000, AUM_micro Loss: 0.0003125504299532622, LR: 4e-05\n",
      "Step 388/1000, AUM_micro Loss: 0.00028718667454086244, LR: 4e-05\n",
      "Step 389/1000, AUM_micro Loss: 0.00028498252504505217, LR: 4e-05\n",
      "Step 390/1000, AUM_micro Loss: 0.0002906470326706767, LR: 4e-05\n",
      "Step 391/1000, AUM_micro Loss: 0.0002860848908312619, LR: 4e-05\n",
      "Step 392/1000, AUM_micro Loss: 0.0002811196609400213, LR: 4e-05\n",
      "Step 393/1000, AUM_micro Loss: 0.00030133919790387154, LR: 4e-05\n",
      "Step 394/1000, AUM_micro Loss: 0.0002774400054477155, LR: 4e-05\n",
      "Step 395/1000, AUM_micro Loss: 0.0002749532868620008, LR: 4e-05\n",
      "Step 396/1000, AUM_micro Loss: 0.0002731910499278456, LR: 4e-05\n",
      "Step 397/1000, AUM_micro Loss: 0.0002711719716899097, LR: 4e-05\n",
      "Step 398/1000, AUM_micro Loss: 0.0002700394834391773, LR: 4e-05\n",
      "Step 399/1000, AUM_micro Loss: 0.0002675707801245153, LR: 4e-05\n",
      "Step 400/1000, AUM_micro Loss: 0.0002668939996510744, LR: 4e-05\n",
      "Step 401/1000, AUM_micro Loss: 0.0002795184846036136, LR: 4e-05\n",
      "Step 402/1000, AUM_micro Loss: 0.00027016131207346916, LR: 4e-05\n",
      "Step 403/1000, AUM_micro Loss: 0.0003259376098867506, LR: 4e-05\n",
      "Step 404/1000, AUM_micro Loss: 0.0002685882500372827, LR: 4e-05\n",
      "Step 405/1000, AUM_micro Loss: 0.0002690753899514675, LR: 4e-05\n",
      "Step 406/1000, AUM_micro Loss: 0.00026542734121903777, LR: 4e-05\n",
      "Step 407/1000, AUM_micro Loss: 0.00026664824690669775, LR: 4e-05\n",
      "Step 408/1000, AUM_micro Loss: 0.00026327790692448616, LR: 4e-05\n",
      "Step 409/1000, AUM_micro Loss: 0.00026176549727097154, LR: 4e-05\n",
      "Step 410/1000, AUM_micro Loss: 0.00025963870575651526, LR: 4e-05\n",
      "Step 411/1000, AUM_micro Loss: 0.0002595083205960691, LR: 4e-05\n",
      "Step 412/1000, AUM_micro Loss: 0.0002584420726634562, LR: 4e-05\n",
      "Step 413/1000, AUM_micro Loss: 0.00025739497505128384, LR: 4e-05\n",
      "Step 414/1000, AUM_micro Loss: 0.0002596329140942544, LR: 4e-05\n",
      "Step 415/1000, AUM_micro Loss: 0.00025750193162821233, LR: 4e-05\n",
      "Step 416/1000, AUM_micro Loss: 0.0002571704681031406, LR: 4e-05\n",
      "Step 417/1000, AUM_micro Loss: 0.0002543895097915083, LR: 4e-05\n",
      "Step 418/1000, AUM_micro Loss: 0.00025461360928602517, LR: 4e-05\n",
      "Step 419/1000, AUM_micro Loss: 0.000253184320172295, LR: 4e-05\n",
      "Step 420/1000, AUM_micro Loss: 0.00025342294247820973, LR: 4e-05\n",
      "Step 421/1000, AUM_micro Loss: 0.00025328711490146816, LR: 4e-05\n",
      "Step 422/1000, AUM_micro Loss: 0.0002523856528569013, LR: 4e-05\n",
      "Step 423/1000, AUM_micro Loss: 0.00025085400557145476, LR: 4e-05\n",
      "Step 424/1000, AUM_micro Loss: 0.0002508499310351908, LR: 4e-05\n",
      "Step 425/1000, AUM_micro Loss: 0.00025119821657426655, LR: 4e-05\n",
      "Step 426/1000, AUM_micro Loss: 0.00024934925022535026, LR: 4e-05\n",
      "Step 427/1000, AUM_micro Loss: 0.00025511611602269113, LR: 4e-05\n",
      "Step 428/1000, AUM_micro Loss: 0.00024889601627364755, LR: 4e-05\n",
      "Step 429/1000, AUM_micro Loss: 0.00024732225574553013, LR: 4e-05\n",
      "Step 430/1000, AUM_micro Loss: 0.0002468195161782205, LR: 4e-05\n",
      "Step 431/1000, AUM_micro Loss: 0.00024685176322236657, LR: 4e-05\n",
      "Step 432/1000, AUM_micro Loss: 0.0002456822548992932, LR: 4e-05\n",
      "Step 433/1000, AUM_micro Loss: 0.00024557943106628954, LR: 4e-05\n",
      "Step 434/1000, AUM_micro Loss: 0.00024555367417633533, LR: 4e-05\n",
      "Step 435/1000, AUM_micro Loss: 0.00024479333660565317, LR: 4e-05\n",
      "Step 436/1000, AUM_micro Loss: 0.00024452817160636187, LR: 4e-05\n",
      "Step 437/1000, AUM_micro Loss: 0.0002447807346470654, LR: 4e-05\n",
      "Step 438/1000, AUM_micro Loss: 0.00024398215464316308, LR: 4e-05\n",
      "Step 439/1000, AUM_micro Loss: 0.00024374638451263309, LR: 4e-05\n",
      "Step 440/1000, AUM_micro Loss: 0.00024659110931679606, LR: 4e-05\n",
      "Step 441/1000, AUM_micro Loss: 0.00024313907488249242, LR: 4e-05\n",
      "Step 442/1000, AUM_micro Loss: 0.000241786619881168, LR: 4e-05\n",
      "Step 443/1000, AUM_micro Loss: 0.00024287895939778537, LR: 4e-05\n",
      "Step 444/1000, AUM_micro Loss: 0.0002401485398877412, LR: 4e-05\n",
      "Step 445/1000, AUM_micro Loss: 0.00024283613311126828, LR: 4e-05\n",
      "Step 446/1000, AUM_micro Loss: 0.000246523879468441, LR: 4e-05\n",
      "Step 447/1000, AUM_micro Loss: 0.0002485373697709292, LR: 4e-05\n",
      "Step 448/1000, AUM_micro Loss: 0.0002593531389720738, LR: 4e-05\n",
      "Step 449/1000, AUM_micro Loss: 0.00024612381821498275, LR: 4e-05\n",
      "Step 450/1000, AUM_micro Loss: 0.00025773289962671697, LR: 4e-05\n",
      "Step 451/1000, AUM_micro Loss: 0.00027002798742614686, LR: 4e-05\n",
      "Step 452/1000, AUM_micro Loss: 0.0002437171060591936, LR: 4e-05\n",
      "Step 453/1000, AUM_micro Loss: 0.00024600117467343807, LR: 4e-05\n",
      "Step 454/1000, AUM_micro Loss: 0.00024418614339083433, LR: 4e-05\n",
      "Step 455/1000, AUM_micro Loss: 0.00024294955073855817, LR: 4e-05\n",
      "Step 456/1000, AUM_micro Loss: 0.0002543461159802973, LR: 4e-05\n",
      "Step 457/1000, AUM_micro Loss: 0.0002482382988091558, LR: 4e-05\n",
      "Step 458/1000, AUM_micro Loss: 0.0002435210917610675, LR: 4e-05\n",
      "Step 459/1000, AUM_micro Loss: 0.00023998775577638298, LR: 4e-05\n",
      "Step 460/1000, AUM_micro Loss: 0.0002399924851488322, LR: 4e-05\n",
      "Step 461/1000, AUM_micro Loss: 0.00025972662842832506, LR: 4e-05\n",
      "Step 462/1000, AUM_micro Loss: 0.0002393017493886873, LR: 4e-05\n",
      "Step 463/1000, AUM_micro Loss: 0.00024059849965851754, LR: 4e-05\n",
      "Step 464/1000, AUM_micro Loss: 0.00023743914789520204, LR: 4e-05\n",
      "Step 465/1000, AUM_micro Loss: 0.0002382127131568268, LR: 4e-05\n",
      "Step 466/1000, AUM_micro Loss: 0.0002359578211326152, LR: 4e-05\n",
      "Step 467/1000, AUM_micro Loss: 0.00023526781296823174, LR: 4e-05\n",
      "Step 468/1000, AUM_micro Loss: 0.00023463233083020896, LR: 4e-05\n",
      "Step 469/1000, AUM_micro Loss: 0.00023347379465121776, LR: 4e-05\n",
      "Step 470/1000, AUM_micro Loss: 0.00023371967836283147, LR: 4e-05\n",
      "Step 471/1000, AUM_micro Loss: 0.0002327571710338816, LR: 4e-05\n",
      "Step 472/1000, AUM_micro Loss: 0.0002333359734620899, LR: 4e-05\n",
      "Step 473/1000, AUM_micro Loss: 0.00023257554857991636, LR: 4e-05\n",
      "Step 474/1000, AUM_micro Loss: 0.0002320605854038149, LR: 4e-05\n",
      "Step 475/1000, AUM_micro Loss: 0.00023110350593924522, LR: 4e-05\n",
      "Step 476/1000, AUM_micro Loss: 0.00023103597050067037, LR: 4e-05\n",
      "Step 477/1000, AUM_micro Loss: 0.00023035661433823407, LR: 4e-05\n",
      "Step 478/1000, AUM_micro Loss: 0.00023020421213004738, LR: 4e-05\n",
      "Step 479/1000, AUM_micro Loss: 0.00022908480605110526, LR: 4e-05\n",
      "Step 480/1000, AUM_micro Loss: 0.000233865634072572, LR: 4e-05\n",
      "Step 481/1000, AUM_micro Loss: 0.00022972682199906558, LR: 4e-05\n",
      "Step 482/1000, AUM_micro Loss: 0.00022939112386666238, LR: 4e-05\n",
      "Step 483/1000, AUM_micro Loss: 0.0002284753427375108, LR: 4e-05\n",
      "Step 484/1000, AUM_micro Loss: 0.00022781909501645714, LR: 4e-05\n",
      "Step 485/1000, AUM_micro Loss: 0.00022719136904925108, LR: 4e-05\n",
      "Step 486/1000, AUM_micro Loss: 0.000226218908210285, LR: 4e-05\n",
      "Step 487/1000, AUM_micro Loss: 0.0002257688611280173, LR: 4e-05\n",
      "Step 488/1000, AUM_micro Loss: 0.0002258873573737219, LR: 4e-05\n",
      "Step 489/1000, AUM_micro Loss: 0.00022568966960534453, LR: 4e-05\n",
      "Step 490/1000, AUM_micro Loss: 0.0002258461172459647, LR: 4e-05\n",
      "Step 491/1000, AUM_micro Loss: 0.00023168751795310527, LR: 4e-05\n",
      "Step 492/1000, AUM_micro Loss: 0.00022777062258683145, LR: 4e-05\n",
      "Step 493/1000, AUM_micro Loss: 0.00024240057973656803, LR: 4e-05\n",
      "Step 494/1000, AUM_micro Loss: 0.00023560921545140445, LR: 4e-05\n",
      "Step 495/1000, AUM_micro Loss: 0.00023431918816640973, LR: 4e-05\n",
      "Step 496/1000, AUM_micro Loss: 0.00023755483562126756, LR: 4e-05\n",
      "Step 497/1000, AUM_micro Loss: 0.00023569571203552186, LR: 4e-05\n",
      "Step 498/1000, AUM_micro Loss: 0.00023461956880055368, LR: 4e-05\n",
      "Step 499/1000, AUM_micro Loss: 0.00024489263887517154, LR: 4e-05\n",
      "Step 500/1000, AUM_micro Loss: 0.0002360778016736731, LR: 4e-05\n",
      "Step 501/1000, AUM_micro Loss: 0.00022966531105339527, LR: 4e-05\n",
      "Step 502/1000, AUM_micro Loss: 0.00022921311028767377, LR: 4e-05\n",
      "Step 503/1000, AUM_micro Loss: 0.00022740010172128677, LR: 4e-05\n",
      "Step 504/1000, AUM_micro Loss: 0.00023795213201083243, LR: 4e-05\n",
      "Step 505/1000, AUM_micro Loss: 0.0002283030335092917, LR: 4e-05\n",
      "Step 506/1000, AUM_micro Loss: 0.000227649012231268, LR: 4e-05\n",
      "Step 507/1000, AUM_micro Loss: 0.0002348512498429045, LR: 4e-05\n",
      "Step 508/1000, AUM_micro Loss: 0.00022895721485838294, LR: 4e-05\n",
      "Step 509/1000, AUM_micro Loss: 0.0002295530866831541, LR: 4e-05\n",
      "Step 510/1000, AUM_micro Loss: 0.00023022065579425544, LR: 8.000000000000001e-06\n",
      "Step 511/1000, AUM_micro Loss: 0.00023207614140119404, LR: 8.000000000000001e-06\n",
      "Step 512/1000, AUM_micro Loss: 0.00023027608403936028, LR: 8.000000000000001e-06\n",
      "Step 513/1000, AUM_micro Loss: 0.0002275991573696956, LR: 8.000000000000001e-06\n",
      "Step 514/1000, AUM_micro Loss: 0.0002267271192977205, LR: 8.000000000000001e-06\n",
      "Step 515/1000, AUM_micro Loss: 0.00022555995383299887, LR: 8.000000000000001e-06\n",
      "Step 516/1000, AUM_micro Loss: 0.0002250301477033645, LR: 8.000000000000001e-06\n",
      "Step 517/1000, AUM_micro Loss: 0.0002238028682768345, LR: 8.000000000000001e-06\n",
      "Step 518/1000, AUM_micro Loss: 0.00022233267372939736, LR: 8.000000000000001e-06\n",
      "Step 519/1000, AUM_micro Loss: 0.00022110299323685467, LR: 8.000000000000001e-06\n",
      "Step 520/1000, AUM_micro Loss: 0.00022002338664606214, LR: 8.000000000000001e-06\n",
      "Step 521/1000, AUM_micro Loss: 0.00021835375810042024, LR: 8.000000000000001e-06\n",
      "Step 522/1000, AUM_micro Loss: 0.0002175350091420114, LR: 8.000000000000001e-06\n",
      "Step 523/1000, AUM_micro Loss: 0.00021680064674001187, LR: 8.000000000000001e-06\n",
      "Step 524/1000, AUM_micro Loss: 0.00021617367747239769, LR: 8.000000000000001e-06\n",
      "Step 525/1000, AUM_micro Loss: 0.0002156912232749164, LR: 8.000000000000001e-06\n",
      "Step 526/1000, AUM_micro Loss: 0.00021515158005058765, LR: 8.000000000000001e-06\n",
      "Step 527/1000, AUM_micro Loss: 0.00021513283718377352, LR: 8.000000000000001e-06\n",
      "Step 528/1000, AUM_micro Loss: 0.00021367328008636832, LR: 8.000000000000001e-06\n",
      "Step 529/1000, AUM_micro Loss: 0.0002127657935488969, LR: 8.000000000000001e-06\n",
      "Step 530/1000, AUM_micro Loss: 0.00021242568618617952, LR: 8.000000000000001e-06\n",
      "Step 531/1000, AUM_micro Loss: 0.00021186454978305846, LR: 8.000000000000001e-06\n",
      "Step 532/1000, AUM_micro Loss: 0.00021110748639330268, LR: 8.000000000000001e-06\n",
      "Step 533/1000, AUM_micro Loss: 0.00021544542687479407, LR: 8.000000000000001e-06\n",
      "Step 534/1000, AUM_micro Loss: 0.0002132755471393466, LR: 8.000000000000001e-06\n",
      "Step 535/1000, AUM_micro Loss: 0.0002112093789037317, LR: 8.000000000000001e-06\n",
      "Step 536/1000, AUM_micro Loss: 0.00021005436428822577, LR: 8.000000000000001e-06\n",
      "Step 537/1000, AUM_micro Loss: 0.00020945831784047186, LR: 8.000000000000001e-06\n",
      "Step 538/1000, AUM_micro Loss: 0.0002087337925331667, LR: 8.000000000000001e-06\n",
      "Step 539/1000, AUM_micro Loss: 0.0002086129825329408, LR: 8.000000000000001e-06\n",
      "Step 540/1000, AUM_micro Loss: 0.00020828063134104013, LR: 8.000000000000001e-06\n",
      "Step 541/1000, AUM_micro Loss: 0.00020809064153581858, LR: 8.000000000000001e-06\n",
      "Step 542/1000, AUM_micro Loss: 0.0002073654904961586, LR: 8.000000000000001e-06\n",
      "Step 543/1000, AUM_micro Loss: 0.00020693526312243193, LR: 8.000000000000001e-06\n",
      "Step 544/1000, AUM_micro Loss: 0.00020666570344474167, LR: 8.000000000000001e-06\n",
      "Step 545/1000, AUM_micro Loss: 0.00020615258836187422, LR: 8.000000000000001e-06\n",
      "Step 546/1000, AUM_micro Loss: 0.00020589737687259912, LR: 8.000000000000001e-06\n",
      "Step 547/1000, AUM_micro Loss: 0.00020584443700499833, LR: 8.000000000000001e-06\n",
      "Step 548/1000, AUM_micro Loss: 0.0002049646427622065, LR: 8.000000000000001e-06\n",
      "Step 549/1000, AUM_micro Loss: 0.00020366300304885954, LR: 8.000000000000001e-06\n",
      "Step 550/1000, AUM_micro Loss: 0.0002027330920100212, LR: 8.000000000000001e-06\n",
      "Step 551/1000, AUM_micro Loss: 0.00020232214592397213, LR: 8.000000000000001e-06\n",
      "Step 552/1000, AUM_micro Loss: 0.00020198780111968517, LR: 8.000000000000001e-06\n",
      "Step 553/1000, AUM_micro Loss: 0.00020139150728937238, LR: 8.000000000000001e-06\n",
      "Step 554/1000, AUM_micro Loss: 0.00020137790124863386, LR: 8.000000000000001e-06\n",
      "Step 555/1000, AUM_micro Loss: 0.00020100024994462729, LR: 8.000000000000001e-06\n",
      "Step 556/1000, AUM_micro Loss: 0.00020052003674209118, LR: 8.000000000000001e-06\n",
      "Step 557/1000, AUM_micro Loss: 0.00020008531282655895, LR: 8.000000000000001e-06\n",
      "Step 558/1000, AUM_micro Loss: 0.00019988625717815012, LR: 8.000000000000001e-06\n",
      "Step 559/1000, AUM_micro Loss: 0.00019933594739995897, LR: 8.000000000000001e-06\n",
      "Step 560/1000, AUM_micro Loss: 0.0001988742733374238, LR: 8.000000000000001e-06\n",
      "Step 561/1000, AUM_micro Loss: 0.00019897017045877874, LR: 8.000000000000001e-06\n",
      "Step 562/1000, AUM_micro Loss: 0.00019873669953085482, LR: 8.000000000000001e-06\n",
      "Step 563/1000, AUM_micro Loss: 0.0001984321279451251, LR: 8.000000000000001e-06\n",
      "Step 564/1000, AUM_micro Loss: 0.00019933420117013156, LR: 8.000000000000001e-06\n",
      "Step 565/1000, AUM_micro Loss: 0.0002520587295293808, LR: 8.000000000000001e-06\n",
      "Step 566/1000, AUM_micro Loss: 0.0002542385773267597, LR: 8.000000000000001e-06\n",
      "Step 567/1000, AUM_micro Loss: 0.0002516726090107113, LR: 8.000000000000001e-06\n",
      "Step 568/1000, AUM_micro Loss: 0.0002463523705955595, LR: 8.000000000000001e-06\n",
      "Step 569/1000, AUM_micro Loss: 0.00024231986026279628, LR: 8.000000000000001e-06\n",
      "Step 570/1000, AUM_micro Loss: 0.00023804869852028787, LR: 8.000000000000001e-06\n",
      "Step 571/1000, AUM_micro Loss: 0.00023331916600000113, LR: 8.000000000000001e-06\n",
      "Step 572/1000, AUM_micro Loss: 0.0002281936613144353, LR: 8.000000000000001e-06\n",
      "Step 573/1000, AUM_micro Loss: 0.00022286952298600227, LR: 8.000000000000001e-06\n",
      "Step 574/1000, AUM_micro Loss: 0.00021724586258642375, LR: 8.000000000000001e-06\n",
      "Step 575/1000, AUM_micro Loss: 0.00021151277178432792, LR: 8.000000000000001e-06\n",
      "Step 576/1000, AUM_micro Loss: 0.00020536655210889876, LR: 8.000000000000001e-06\n",
      "Step 577/1000, AUM_micro Loss: 0.00019907974638044834, LR: 8.000000000000001e-06\n",
      "Step 578/1000, AUM_micro Loss: 0.0001979209773708135, LR: 8.000000000000001e-06\n",
      "Step 579/1000, AUM_micro Loss: 0.00019775956752710044, LR: 8.000000000000001e-06\n",
      "Step 580/1000, AUM_micro Loss: 0.0001975612685782835, LR: 8.000000000000001e-06\n",
      "Step 581/1000, AUM_micro Loss: 0.00019743345910683274, LR: 8.000000000000001e-06\n",
      "Step 582/1000, AUM_micro Loss: 0.00019678739772643894, LR: 8.000000000000001e-06\n",
      "Step 583/1000, AUM_micro Loss: 0.00019652901391964406, LR: 8.000000000000001e-06\n",
      "Step 584/1000, AUM_micro Loss: 0.00019635193166323006, LR: 8.000000000000001e-06\n",
      "Step 585/1000, AUM_micro Loss: 0.00019617596990428865, LR: 8.000000000000001e-06\n",
      "Step 586/1000, AUM_micro Loss: 0.0001960895024240017, LR: 8.000000000000001e-06\n",
      "Step 587/1000, AUM_micro Loss: 0.00019613461336120963, LR: 8.000000000000001e-06\n",
      "Step 588/1000, AUM_micro Loss: 0.00019548347336240113, LR: 8.000000000000001e-06\n",
      "Step 589/1000, AUM_micro Loss: 0.00019549464923329651, LR: 8.000000000000001e-06\n",
      "Step 590/1000, AUM_micro Loss: 0.00019521759531926364, LR: 8.000000000000001e-06\n",
      "Step 591/1000, AUM_micro Loss: 0.00019522853835951537, LR: 8.000000000000001e-06\n",
      "Step 592/1000, AUM_micro Loss: 0.00019515579333528876, LR: 8.000000000000001e-06\n",
      "Step 593/1000, AUM_micro Loss: 0.00019492134742904454, LR: 8.000000000000001e-06\n",
      "Step 594/1000, AUM_micro Loss: 0.00019488045654725283, LR: 8.000000000000001e-06\n",
      "Step 595/1000, AUM_micro Loss: 0.00019466808589641005, LR: 8.000000000000001e-06\n",
      "Step 596/1000, AUM_micro Loss: 0.00019450612307991832, LR: 8.000000000000001e-06\n",
      "Step 597/1000, AUM_micro Loss: 0.00019414590497035533, LR: 8.000000000000001e-06\n",
      "Step 598/1000, AUM_micro Loss: 0.0001937673077918589, LR: 8.000000000000001e-06\n",
      "Step 599/1000, AUM_micro Loss: 0.00019348447676748037, LR: 8.000000000000001e-06\n",
      "Step 600/1000, AUM_micro Loss: 0.0001932830346049741, LR: 8.000000000000001e-06\n",
      "Step 601/1000, AUM_micro Loss: 0.0001927295234054327, LR: 8.000000000000001e-06\n",
      "Step 602/1000, AUM_micro Loss: 0.00019282172434031963, LR: 8.000000000000001e-06\n",
      "Step 603/1000, AUM_micro Loss: 0.00019263304420746863, LR: 8.000000000000001e-06\n",
      "Step 604/1000, AUM_micro Loss: 0.00019247263844590634, LR: 8.000000000000001e-06\n",
      "Step 605/1000, AUM_micro Loss: 0.00019220044487155974, LR: 8.000000000000001e-06\n",
      "Step 606/1000, AUM_micro Loss: 0.00019172558677382767, LR: 8.000000000000001e-06\n",
      "Step 607/1000, AUM_micro Loss: 0.00019166515266988426, LR: 8.000000000000001e-06\n",
      "Step 608/1000, AUM_micro Loss: 0.0001915264001581818, LR: 8.000000000000001e-06\n",
      "Step 609/1000, AUM_micro Loss: 0.00019148571300320327, LR: 8.000000000000001e-06\n",
      "Step 610/1000, AUM_micro Loss: 0.0001912675506900996, LR: 8.000000000000001e-06\n",
      "Step 611/1000, AUM_micro Loss: 0.0001913252635858953, LR: 8.000000000000001e-06\n",
      "Step 612/1000, AUM_micro Loss: 0.00019066451932303607, LR: 8.000000000000001e-06\n",
      "Step 613/1000, AUM_micro Loss: 0.00019050833361689, LR: 8.000000000000001e-06\n",
      "Step 614/1000, AUM_micro Loss: 0.0001903467928059399, LR: 8.000000000000001e-06\n",
      "Step 615/1000, AUM_micro Loss: 0.00019014217832591385, LR: 8.000000000000001e-06\n",
      "Step 616/1000, AUM_micro Loss: 0.00019021821208298206, LR: 8.000000000000001e-06\n",
      "Step 617/1000, AUM_micro Loss: 0.00018987841031048447, LR: 8.000000000000001e-06\n",
      "Step 618/1000, AUM_micro Loss: 0.00018967577489092946, LR: 8.000000000000001e-06\n",
      "Step 619/1000, AUM_micro Loss: 0.00018931696831714362, LR: 8.000000000000001e-06\n",
      "Step 620/1000, AUM_micro Loss: 0.0001893460430437699, LR: 8.000000000000001e-06\n",
      "Step 621/1000, AUM_micro Loss: 0.0001892949512694031, LR: 8.000000000000001e-06\n",
      "Step 622/1000, AUM_micro Loss: 0.00018917571287602186, LR: 8.000000000000001e-06\n",
      "Step 623/1000, AUM_micro Loss: 0.00018896999245043844, LR: 8.000000000000001e-06\n",
      "Step 624/1000, AUM_micro Loss: 0.00018867020844481885, LR: 8.000000000000001e-06\n",
      "Step 625/1000, AUM_micro Loss: 0.00018855647067539394, LR: 8.000000000000001e-06\n",
      "Step 626/1000, AUM_micro Loss: 0.00018831742636393756, LR: 8.000000000000001e-06\n",
      "Step 627/1000, AUM_micro Loss: 0.0001880838390206918, LR: 8.000000000000001e-06\n",
      "Step 628/1000, AUM_micro Loss: 0.00018812388589140028, LR: 8.000000000000001e-06\n",
      "Step 629/1000, AUM_micro Loss: 0.00018793021445162594, LR: 8.000000000000001e-06\n",
      "Step 630/1000, AUM_micro Loss: 0.00018781535618472844, LR: 8.000000000000001e-06\n",
      "Step 631/1000, AUM_micro Loss: 0.00018722403910942376, LR: 8.000000000000001e-06\n",
      "Step 632/1000, AUM_micro Loss: 0.0001872481225291267, LR: 8.000000000000001e-06\n",
      "Step 633/1000, AUM_micro Loss: 0.0001869970583356917, LR: 8.000000000000001e-06\n",
      "Step 634/1000, AUM_micro Loss: 0.00018671572615858167, LR: 8.000000000000001e-06\n",
      "Step 635/1000, AUM_micro Loss: 0.00018648724653758109, LR: 8.000000000000001e-06\n",
      "Step 636/1000, AUM_micro Loss: 0.00018636461754795164, LR: 8.000000000000001e-06\n",
      "Step 637/1000, AUM_micro Loss: 0.00018625069060362875, LR: 8.000000000000001e-06\n",
      "Step 638/1000, AUM_micro Loss: 0.00018619897309690714, LR: 8.000000000000001e-06\n",
      "Step 639/1000, AUM_micro Loss: 0.00018576653383206576, LR: 8.000000000000001e-06\n",
      "Step 640/1000, AUM_micro Loss: 0.0001857809111243114, LR: 8.000000000000001e-06\n",
      "Step 641/1000, AUM_micro Loss: 0.00018589702085591853, LR: 8.000000000000001e-06\n",
      "Step 642/1000, AUM_micro Loss: 0.00018573817214928567, LR: 8.000000000000001e-06\n",
      "Step 643/1000, AUM_micro Loss: 0.00018574758723843843, LR: 8.000000000000001e-06\n",
      "Step 644/1000, AUM_micro Loss: 0.00018559483578428626, LR: 8.000000000000001e-06\n",
      "Step 645/1000, AUM_micro Loss: 0.00018551433458924294, LR: 8.000000000000001e-06\n",
      "Step 646/1000, AUM_micro Loss: 0.00018551408720668405, LR: 8.000000000000001e-06\n",
      "Step 647/1000, AUM_micro Loss: 0.00018536477000452578, LR: 8.000000000000001e-06\n",
      "Step 648/1000, AUM_micro Loss: 0.00018510984955355525, LR: 8.000000000000001e-06\n",
      "Step 649/1000, AUM_micro Loss: 0.00018498688586987555, LR: 8.000000000000001e-06\n",
      "Step 650/1000, AUM_micro Loss: 0.00018495549738872796, LR: 8.000000000000001e-06\n",
      "Step 651/1000, AUM_micro Loss: 0.00022304321464616805, LR: 8.000000000000001e-06\n",
      "Step 652/1000, AUM_micro Loss: 0.0001980441011255607, LR: 8.000000000000001e-06\n",
      "Step 653/1000, AUM_micro Loss: 0.00019488271209411323, LR: 8.000000000000001e-06\n",
      "Step 654/1000, AUM_micro Loss: 0.00018899497808888555, LR: 8.000000000000001e-06\n",
      "Step 655/1000, AUM_micro Loss: 0.0001843036152422428, LR: 8.000000000000001e-06\n",
      "Step 656/1000, AUM_micro Loss: 0.0001843787613324821, LR: 8.000000000000001e-06\n",
      "Step 657/1000, AUM_micro Loss: 0.00018415892554912716, LR: 8.000000000000001e-06\n",
      "Step 658/1000, AUM_micro Loss: 0.00018386877491138875, LR: 8.000000000000001e-06\n",
      "Step 659/1000, AUM_micro Loss: 0.00018334372725803405, LR: 8.000000000000001e-06\n",
      "Step 660/1000, AUM_micro Loss: 0.00018322767573408782, LR: 8.000000000000001e-06\n",
      "Step 661/1000, AUM_micro Loss: 0.00018310549785383046, LR: 8.000000000000001e-06\n",
      "Step 662/1000, AUM_micro Loss: 0.0001830199471442029, LR: 8.000000000000001e-06\n",
      "Step 663/1000, AUM_micro Loss: 0.00018263548554386944, LR: 8.000000000000001e-06\n",
      "Step 664/1000, AUM_micro Loss: 0.0001827003143262118, LR: 8.000000000000001e-06\n",
      "Step 665/1000, AUM_micro Loss: 0.00018259749049320817, LR: 8.000000000000001e-06\n",
      "Step 666/1000, AUM_micro Loss: 0.00018248021660838276, LR: 8.000000000000001e-06\n",
      "Step 667/1000, AUM_micro Loss: 0.0001823481870815158, LR: 8.000000000000001e-06\n",
      "Step 668/1000, AUM_micro Loss: 0.00018212343275081366, LR: 8.000000000000001e-06\n",
      "Step 669/1000, AUM_micro Loss: 0.00018209817062597722, LR: 8.000000000000001e-06\n",
      "Step 670/1000, AUM_micro Loss: 0.0001820280886022374, LR: 8.000000000000001e-06\n",
      "Step 671/1000, AUM_micro Loss: 0.0002081304119201377, LR: 8.000000000000001e-06\n",
      "Step 672/1000, AUM_micro Loss: 0.00018199440091848373, LR: 8.000000000000001e-06\n",
      "Step 673/1000, AUM_micro Loss: 0.00018168901442550123, LR: 8.000000000000001e-06\n",
      "Step 674/1000, AUM_micro Loss: 0.00018141057807952166, LR: 8.000000000000001e-06\n",
      "Step 675/1000, AUM_micro Loss: 0.00018140637257602066, LR: 8.000000000000001e-06\n",
      "Step 676/1000, AUM_micro Loss: 0.00018137917504645884, LR: 8.000000000000001e-06\n",
      "Step 677/1000, AUM_micro Loss: 0.0001812806585803628, LR: 8.000000000000001e-06\n",
      "Step 678/1000, AUM_micro Loss: 0.00018107010691892356, LR: 8.000000000000001e-06\n",
      "Step 679/1000, AUM_micro Loss: 0.0001810343674151227, LR: 8.000000000000001e-06\n",
      "Step 680/1000, AUM_micro Loss: 0.00018106985953636467, LR: 8.000000000000001e-06\n",
      "Step 681/1000, AUM_micro Loss: 0.00018099720182362944, LR: 8.000000000000001e-06\n",
      "Step 682/1000, AUM_micro Loss: 0.0001809330569813028, LR: 8.000000000000001e-06\n",
      "Step 683/1000, AUM_micro Loss: 0.0001807522785384208, LR: 8.000000000000001e-06\n",
      "Step 684/1000, AUM_micro Loss: 0.00018050849030259997, LR: 8.000000000000001e-06\n",
      "Step 685/1000, AUM_micro Loss: 0.00018131946853827685, LR: 8.000000000000001e-06\n",
      "Step 686/1000, AUM_micro Loss: 0.0001805062056519091, LR: 8.000000000000001e-06\n",
      "Step 687/1000, AUM_micro Loss: 0.00018090590310748667, LR: 8.000000000000001e-06\n",
      "Step 688/1000, AUM_micro Loss: 0.00019508374680299312, LR: 8.000000000000001e-06\n",
      "Step 689/1000, AUM_micro Loss: 0.00018041409202851355, LR: 8.000000000000001e-06\n",
      "Step 690/1000, AUM_micro Loss: 0.00018051326333079487, LR: 8.000000000000001e-06\n",
      "Step 691/1000, AUM_micro Loss: 0.00018032461230177432, LR: 8.000000000000001e-06\n",
      "Step 692/1000, AUM_micro Loss: 0.00018029945204034448, LR: 8.000000000000001e-06\n",
      "Step 693/1000, AUM_micro Loss: 0.00018058167188428342, LR: 8.000000000000001e-06\n",
      "Step 694/1000, AUM_micro Loss: 0.0001800043974071741, LR: 8.000000000000001e-06\n",
      "Step 695/1000, AUM_micro Loss: 0.00017994684458244592, LR: 8.000000000000001e-06\n",
      "Step 696/1000, AUM_micro Loss: 0.00017976896197069436, LR: 8.000000000000001e-06\n",
      "Step 697/1000, AUM_micro Loss: 0.00017964579456020147, LR: 8.000000000000001e-06\n",
      "Step 698/1000, AUM_micro Loss: 0.00017970569024328142, LR: 8.000000000000001e-06\n",
      "Step 699/1000, AUM_micro Loss: 0.00017972708155866712, LR: 8.000000000000001e-06\n",
      "Step 700/1000, AUM_micro Loss: 0.00017974406364373863, LR: 8.000000000000001e-06\n",
      "Step 701/1000, AUM_micro Loss: 0.0001794768904801458, LR: 8.000000000000001e-06\n",
      "Step 702/1000, AUM_micro Loss: 0.00017956941155716777, LR: 8.000000000000001e-06\n",
      "Step 703/1000, AUM_micro Loss: 0.00017955189105123281, LR: 8.000000000000001e-06\n",
      "Step 704/1000, AUM_micro Loss: 0.00017942592967301607, LR: 8.000000000000001e-06\n",
      "Step 705/1000, AUM_micro Loss: 0.00017951631161849946, LR: 8.000000000000001e-06\n",
      "Step 706/1000, AUM_micro Loss: 0.00017939426470547915, LR: 8.000000000000001e-06\n",
      "Step 707/1000, AUM_micro Loss: 0.00017940961697604507, LR: 8.000000000000001e-06\n",
      "Step 708/1000, AUM_micro Loss: 0.00017938294331543148, LR: 8.000000000000001e-06\n",
      "Step 709/1000, AUM_micro Loss: 0.0001793711562640965, LR: 8.000000000000001e-06\n",
      "Step 710/1000, AUM_micro Loss: 0.0001792319817468524, LR: 8.000000000000001e-06\n",
      "Step 711/1000, AUM_micro Loss: 0.00017895863857120275, LR: 8.000000000000001e-06\n",
      "Step 712/1000, AUM_micro Loss: 0.000178921822225675, LR: 8.000000000000001e-06\n",
      "Step 713/1000, AUM_micro Loss: 0.0001787792716640979, LR: 8.000000000000001e-06\n",
      "Step 714/1000, AUM_micro Loss: 0.00017854438920039684, LR: 8.000000000000001e-06\n",
      "Step 715/1000, AUM_micro Loss: 0.00017854697944130749, LR: 8.000000000000001e-06\n",
      "Step 716/1000, AUM_micro Loss: 0.00017833661695476621, LR: 8.000000000000001e-06\n",
      "Step 717/1000, AUM_micro Loss: 0.00017785310046747327, LR: 8.000000000000001e-06\n",
      "Step 718/1000, AUM_micro Loss: 0.00017757999012246728, LR: 8.000000000000001e-06\n",
      "Step 719/1000, AUM_micro Loss: 0.00017763851792551577, LR: 8.000000000000001e-06\n",
      "Step 720/1000, AUM_micro Loss: 0.00017762085190042853, LR: 8.000000000000001e-06\n",
      "Step 721/1000, AUM_micro Loss: 0.0001776076969690621, LR: 8.000000000000001e-06\n",
      "Step 722/1000, AUM_micro Loss: 0.00017739729082677513, LR: 8.000000000000001e-06\n",
      "Step 723/1000, AUM_micro Loss: 0.00017740642942953855, LR: 8.000000000000001e-06\n",
      "Step 724/1000, AUM_micro Loss: 0.00017721197218634188, LR: 8.000000000000001e-06\n",
      "Step 725/1000, AUM_micro Loss: 0.00017710166866891086, LR: 8.000000000000001e-06\n",
      "Step 726/1000, AUM_micro Loss: 0.00017685978673398495, LR: 8.000000000000001e-06\n",
      "Step 727/1000, AUM_micro Loss: 0.00017685783677734435, LR: 8.000000000000001e-06\n",
      "Step 728/1000, AUM_micro Loss: 0.0001768038491718471, LR: 8.000000000000001e-06\n",
      "Step 729/1000, AUM_micro Loss: 0.00017678621225059032, LR: 8.000000000000001e-06\n",
      "Step 730/1000, AUM_micro Loss: 0.0001765323686413467, LR: 8.000000000000001e-06\n",
      "Step 731/1000, AUM_micro Loss: 0.0001764256739988923, LR: 8.000000000000001e-06\n",
      "Step 732/1000, AUM_micro Loss: 0.00017622513405513018, LR: 8.000000000000001e-06\n",
      "Step 733/1000, AUM_micro Loss: 0.00017625794862397015, LR: 8.000000000000001e-06\n",
      "Step 734/1000, AUM_micro Loss: 0.00018367687880527228, LR: 8.000000000000001e-06\n",
      "Step 735/1000, AUM_micro Loss: 0.0001812951231840998, LR: 8.000000000000001e-06\n",
      "Step 736/1000, AUM_micro Loss: 0.00018007971812039614, LR: 8.000000000000001e-06\n",
      "Step 737/1000, AUM_micro Loss: 0.00017801145440898836, LR: 8.000000000000001e-06\n",
      "Step 738/1000, AUM_micro Loss: 0.00017702624609228224, LR: 8.000000000000001e-06\n",
      "Step 739/1000, AUM_micro Loss: 0.0001762690517352894, LR: 8.000000000000001e-06\n",
      "Step 740/1000, AUM_micro Loss: 0.00017622818995732814, LR: 8.000000000000001e-06\n",
      "Step 741/1000, AUM_micro Loss: 0.00017607536574359983, LR: 8.000000000000001e-06\n",
      "Step 742/1000, AUM_micro Loss: 0.00018004524463322014, LR: 8.000000000000001e-06\n",
      "Step 743/1000, AUM_micro Loss: 0.00017571936768945307, LR: 8.000000000000001e-06\n",
      "Step 744/1000, AUM_micro Loss: 0.0001757191348588094, LR: 8.000000000000001e-06\n",
      "Step 745/1000, AUM_micro Loss: 0.00017538540123496205, LR: 8.000000000000001e-06\n",
      "Step 746/1000, AUM_micro Loss: 0.0001752165553625673, LR: 8.000000000000001e-06\n",
      "Step 747/1000, AUM_micro Loss: 0.00017507105076219887, LR: 8.000000000000001e-06\n",
      "Step 748/1000, AUM_micro Loss: 0.0002108852204401046, LR: 8.000000000000001e-06\n",
      "Step 749/1000, AUM_micro Loss: 0.00018299181829206645, LR: 8.000000000000001e-06\n",
      "Step 750/1000, AUM_micro Loss: 0.0001769203518051654, LR: 8.000000000000001e-06\n",
      "Step 751/1000, AUM_micro Loss: 0.00017570826457813382, LR: 8.000000000000001e-06\n",
      "Step 752/1000, AUM_micro Loss: 0.00017558848776388913, LR: 8.000000000000001e-06\n",
      "Step 753/1000, AUM_micro Loss: 0.00017548364121466875, LR: 8.000000000000001e-06\n",
      "Step 754/1000, AUM_micro Loss: 0.00017564973677508533, LR: 8.000000000000001e-06\n",
      "Step 755/1000, AUM_micro Loss: 0.00017531680350657552, LR: 8.000000000000001e-06\n",
      "Step 756/1000, AUM_micro Loss: 0.00017498906527180225, LR: 8.000000000000001e-06\n",
      "Step 757/1000, AUM_micro Loss: 0.00017479703819844872, LR: 8.000000000000001e-06\n",
      "Step 758/1000, AUM_micro Loss: 0.0001746874477248639, LR: 8.000000000000001e-06\n",
      "Step 759/1000, AUM_micro Loss: 0.00017522572306916118, LR: 8.000000000000001e-06\n",
      "Step 760/1000, AUM_micro Loss: 0.00017463270341977477, LR: 8.000000000000001e-06\n",
      "Step 761/1000, AUM_micro Loss: 0.0002073687792290002, LR: 8.000000000000001e-06\n",
      "Step 762/1000, AUM_micro Loss: 0.00017420656513422728, LR: 8.000000000000001e-06\n",
      "Step 763/1000, AUM_micro Loss: 0.0001741124433465302, LR: 8.000000000000001e-06\n",
      "Step 764/1000, AUM_micro Loss: 0.0001745906047290191, LR: 8.000000000000001e-06\n",
      "Step 765/1000, AUM_micro Loss: 0.00018453704251442105, LR: 8.000000000000001e-06\n",
      "Step 766/1000, AUM_micro Loss: 0.00017435774498153478, LR: 8.000000000000001e-06\n",
      "Step 767/1000, AUM_micro Loss: 0.00017395430768374354, LR: 8.000000000000001e-06\n",
      "Step 768/1000, AUM_micro Loss: 0.00017392594600096345, LR: 8.000000000000001e-06\n",
      "Step 769/1000, AUM_micro Loss: 0.0001738383580232039, LR: 8.000000000000001e-06\n",
      "Step 770/1000, AUM_micro Loss: 0.0001739263243507594, LR: 8.000000000000001e-06\n",
      "Step 771/1000, AUM_micro Loss: 0.00017380966164637357, LR: 8.000000000000001e-06\n",
      "Step 772/1000, AUM_micro Loss: 0.00017382213263772428, LR: 8.000000000000001e-06\n",
      "Step 773/1000, AUM_micro Loss: 0.0001736839476507157, LR: 8.000000000000001e-06\n",
      "Step 774/1000, AUM_micro Loss: 0.00017450223094783723, LR: 8.000000000000001e-06\n",
      "Step 775/1000, AUM_micro Loss: 0.0001737299025990069, LR: 8.000000000000001e-06\n",
      "Step 776/1000, AUM_micro Loss: 0.00017369638953823596, LR: 8.000000000000001e-06\n",
      "Step 777/1000, AUM_micro Loss: 0.00017361897334922105, LR: 8.000000000000001e-06\n",
      "Step 778/1000, AUM_micro Loss: 0.00017360223864670843, LR: 8.000000000000001e-06\n",
      "Step 779/1000, AUM_micro Loss: 0.0001733535755192861, LR: 8.000000000000001e-06\n",
      "Step 780/1000, AUM_micro Loss: 0.0001752582029439509, LR: 8.000000000000001e-06\n",
      "Step 781/1000, AUM_micro Loss: 0.00017369817942380905, LR: 8.000000000000001e-06\n",
      "Step 782/1000, AUM_micro Loss: 0.00017360337369609624, LR: 8.000000000000001e-06\n",
      "Step 783/1000, AUM_micro Loss: 0.00017361569916829467, LR: 8.000000000000001e-06\n",
      "Step 784/1000, AUM_micro Loss: 0.0001732632372295484, LR: 8.000000000000001e-06\n",
      "Step 785/1000, AUM_micro Loss: 0.0001732227101456374, LR: 8.000000000000001e-06\n",
      "Step 786/1000, AUM_micro Loss: 0.00017333283904008567, LR: 8.000000000000001e-06\n",
      "Step 787/1000, AUM_micro Loss: 0.00017316236335318536, LR: 8.000000000000001e-06\n",
      "Step 788/1000, AUM_micro Loss: 0.00017274759011343122, LR: 8.000000000000001e-06\n",
      "Step 789/1000, AUM_micro Loss: 0.00017272564582526684, LR: 8.000000000000001e-06\n",
      "Step 790/1000, AUM_micro Loss: 0.00017367843247484416, LR: 8.000000000000001e-06\n",
      "Step 791/1000, AUM_micro Loss: 0.0001726972113829106, LR: 8.000000000000001e-06\n",
      "Step 792/1000, AUM_micro Loss: 0.00017254234990105033, LR: 8.000000000000001e-06\n",
      "Step 793/1000, AUM_micro Loss: 0.00017249866505153477, LR: 8.000000000000001e-06\n",
      "Step 794/1000, AUM_micro Loss: 0.00017249587108381093, LR: 8.000000000000001e-06\n",
      "Step 795/1000, AUM_micro Loss: 0.000172617903444916, LR: 8.000000000000001e-06\n",
      "Step 796/1000, AUM_micro Loss: 0.00017204045434482396, LR: 8.000000000000001e-06\n",
      "Step 797/1000, AUM_micro Loss: 0.0001719744032016024, LR: 8.000000000000001e-06\n",
      "Step 798/1000, AUM_micro Loss: 0.00017153903900180012, LR: 8.000000000000001e-06\n",
      "Step 799/1000, AUM_micro Loss: 0.00017139280680567026, LR: 8.000000000000001e-06\n",
      "Step 800/1000, AUM_micro Loss: 0.0001713596866466105, LR: 8.000000000000001e-06\n",
      "Step 801/1000, AUM_micro Loss: 0.00017130884225480258, LR: 8.000000000000001e-06\n",
      "Step 802/1000, AUM_micro Loss: 0.00017135756206698716, LR: 8.000000000000001e-06\n",
      "Step 803/1000, AUM_micro Loss: 0.0001712665252853185, LR: 8.000000000000001e-06\n",
      "Step 804/1000, AUM_micro Loss: 0.0001709961798042059, LR: 8.000000000000001e-06\n",
      "Step 805/1000, AUM_micro Loss: 0.0001710915530566126, LR: 8.000000000000001e-06\n",
      "Step 806/1000, AUM_micro Loss: 0.00017084348655771464, LR: 8.000000000000001e-06\n",
      "Step 807/1000, AUM_micro Loss: 0.00017069070599973202, LR: 8.000000000000001e-06\n",
      "Step 808/1000, AUM_micro Loss: 0.00017067349108401686, LR: 8.000000000000001e-06\n",
      "Step 809/1000, AUM_micro Loss: 0.00017071104957722127, LR: 8.000000000000001e-06\n",
      "Step 810/1000, AUM_micro Loss: 0.0001705939939711243, LR: 8.000000000000001e-06\n",
      "Step 811/1000, AUM_micro Loss: 0.0001705401809886098, LR: 8.000000000000001e-06\n",
      "Step 812/1000, AUM_micro Loss: 0.00017053032934200019, LR: 8.000000000000001e-06\n",
      "Step 813/1000, AUM_micro Loss: 0.0001704336900729686, LR: 8.000000000000001e-06\n",
      "Step 814/1000, AUM_micro Loss: 0.00017022184329107404, LR: 8.000000000000001e-06\n",
      "Step 815/1000, AUM_micro Loss: 0.00017013504111673683, LR: 8.000000000000001e-06\n",
      "Step 816/1000, AUM_micro Loss: 0.0001698787382338196, LR: 8.000000000000001e-06\n",
      "Step 817/1000, AUM_micro Loss: 0.000169697857927531, LR: 8.000000000000001e-06\n",
      "Step 818/1000, AUM_micro Loss: 0.00016961978690233082, LR: 8.000000000000001e-06\n",
      "Step 819/1000, AUM_micro Loss: 0.00016968905401881784, LR: 8.000000000000001e-06\n",
      "Step 820/1000, AUM_micro Loss: 0.00016940297791734338, LR: 8.000000000000001e-06\n",
      "Step 821/1000, AUM_micro Loss: 0.00016919776680879295, LR: 8.000000000000001e-06\n",
      "Step 822/1000, AUM_micro Loss: 0.000168897895491682, LR: 8.000000000000001e-06\n",
      "Step 823/1000, AUM_micro Loss: 0.0001689781784079969, LR: 8.000000000000001e-06\n",
      "Step 824/1000, AUM_micro Loss: 0.00016903472715057433, LR: 8.000000000000001e-06\n",
      "Step 825/1000, AUM_micro Loss: 0.00016890853294171393, LR: 8.000000000000001e-06\n",
      "Step 826/1000, AUM_micro Loss: 0.0001689354539848864, LR: 8.000000000000001e-06\n",
      "Step 827/1000, AUM_micro Loss: 0.00016882542695384473, LR: 8.000000000000001e-06\n",
      "Step 828/1000, AUM_micro Loss: 0.00016869539103936404, LR: 8.000000000000001e-06\n",
      "Step 829/1000, AUM_micro Loss: 0.00016870145918801427, LR: 8.000000000000001e-06\n",
      "Step 830/1000, AUM_micro Loss: 0.0001685522438492626, LR: 8.000000000000001e-06\n",
      "Step 831/1000, AUM_micro Loss: 0.0001684763265075162, LR: 8.000000000000001e-06\n",
      "Step 832/1000, AUM_micro Loss: 0.00016817569849081337, LR: 8.000000000000001e-06\n",
      "Step 833/1000, AUM_micro Loss: 0.00016785389743745327, LR: 8.000000000000001e-06\n",
      "Step 834/1000, AUM_micro Loss: 0.0001678365806583315, LR: 8.000000000000001e-06\n",
      "Step 835/1000, AUM_micro Loss: 0.00016773394600022584, LR: 8.000000000000001e-06\n",
      "Step 836/1000, AUM_micro Loss: 0.0001677697873674333, LR: 8.000000000000001e-06\n",
      "Step 837/1000, AUM_micro Loss: 0.00016778339340817183, LR: 8.000000000000001e-06\n",
      "Step 838/1000, AUM_micro Loss: 0.0001678896223893389, LR: 8.000000000000001e-06\n",
      "Step 839/1000, AUM_micro Loss: 0.00016791775124147534, LR: 8.000000000000001e-06\n",
      "Step 840/1000, AUM_micro Loss: 0.00016777923156041652, LR: 8.000000000000001e-06\n",
      "Step 841/1000, AUM_micro Loss: 0.00016744821914471686, LR: 8.000000000000001e-06\n",
      "Step 842/1000, AUM_micro Loss: 0.0001673900696914643, LR: 8.000000000000001e-06\n",
      "Step 843/1000, AUM_micro Loss: 0.0001674293598625809, LR: 8.000000000000001e-06\n",
      "Step 844/1000, AUM_micro Loss: 0.0001674816885497421, LR: 8.000000000000001e-06\n",
      "Step 845/1000, AUM_micro Loss: 0.00016720953863114119, LR: 8.000000000000001e-06\n",
      "Step 846/1000, AUM_micro Loss: 0.00016708912153262645, LR: 8.000000000000001e-06\n",
      "Step 847/1000, AUM_micro Loss: 0.00016707000031601638, LR: 8.000000000000001e-06\n",
      "Step 848/1000, AUM_micro Loss: 0.00016706401947885752, LR: 8.000000000000001e-06\n",
      "Step 849/1000, AUM_micro Loss: 0.00016714102821424603, LR: 8.000000000000001e-06\n",
      "Step 850/1000, AUM_micro Loss: 0.00016720789426472038, LR: 8.000000000000001e-06\n",
      "Step 851/1000, AUM_micro Loss: 0.00016676260565873235, LR: 8.000000000000001e-06\n",
      "Step 852/1000, AUM_micro Loss: 0.00016666333249304444, LR: 8.000000000000001e-06\n",
      "Step 853/1000, AUM_micro Loss: 0.00016648898599669337, LR: 8.000000000000001e-06\n",
      "Step 854/1000, AUM_micro Loss: 0.00016632367623969913, LR: 8.000000000000001e-06\n",
      "Step 855/1000, AUM_micro Loss: 0.00016607104043941945, LR: 8.000000000000001e-06\n",
      "Step 856/1000, AUM_micro Loss: 0.0001659298868617043, LR: 8.000000000000001e-06\n",
      "Step 857/1000, AUM_micro Loss: 0.00016592913016211241, LR: 8.000000000000001e-06\n",
      "Step 858/1000, AUM_micro Loss: 0.0001658627879805863, LR: 8.000000000000001e-06\n",
      "Step 859/1000, AUM_micro Loss: 0.00016583767137490213, LR: 8.000000000000001e-06\n",
      "Step 860/1000, AUM_micro Loss: 0.00016592990141361952, LR: 8.000000000000001e-06\n",
      "Step 861/1000, AUM_micro Loss: 0.00016567010607104748, LR: 8.000000000000001e-06\n",
      "Step 862/1000, AUM_micro Loss: 0.0001655978267081082, LR: 8.000000000000001e-06\n",
      "Step 863/1000, AUM_micro Loss: 0.00016563854296691716, LR: 8.000000000000001e-06\n",
      "Step 864/1000, AUM_micro Loss: 0.00016548906569369137, LR: 8.000000000000001e-06\n",
      "Step 865/1000, AUM_micro Loss: 0.00016552302986383438, LR: 8.000000000000001e-06\n",
      "Step 866/1000, AUM_micro Loss: 0.00016552380111534148, LR: 8.000000000000001e-06\n",
      "Step 867/1000, AUM_micro Loss: 0.0001654757943470031, LR: 8.000000000000001e-06\n",
      "Step 868/1000, AUM_micro Loss: 0.0001654565567150712, LR: 8.000000000000001e-06\n",
      "Step 869/1000, AUM_micro Loss: 0.00016539005446247756, LR: 8.000000000000001e-06\n",
      "Step 870/1000, AUM_micro Loss: 0.00016539712669327855, LR: 8.000000000000001e-06\n",
      "Step 871/1000, AUM_micro Loss: 0.00016538231284357607, LR: 8.000000000000001e-06\n",
      "Step 872/1000, AUM_micro Loss: 0.00016533804591745138, LR: 8.000000000000001e-06\n",
      "Step 873/1000, AUM_micro Loss: 0.00016535432951059192, LR: 8.000000000000001e-06\n",
      "Step 874/1000, AUM_micro Loss: 0.00016537388728465885, LR: 8.000000000000001e-06\n",
      "Step 875/1000, AUM_micro Loss: 0.00016515828610863537, LR: 8.000000000000001e-06\n",
      "Step 876/1000, AUM_micro Loss: 0.00016515151946805418, LR: 8.000000000000001e-06\n",
      "Step 877/1000, AUM_micro Loss: 0.00016519639757461846, LR: 8.000000000000001e-06\n",
      "Step 878/1000, AUM_micro Loss: 0.00016511138528585434, LR: 8.000000000000001e-06\n",
      "Step 879/1000, AUM_micro Loss: 0.0001651955972192809, LR: 8.000000000000001e-06\n",
      "Step 880/1000, AUM_micro Loss: 0.00016515569586772472, LR: 8.000000000000001e-06\n",
      "Step 881/1000, AUM_micro Loss: 0.0001651954953558743, LR: 8.000000000000001e-06\n",
      "Step 882/1000, AUM_micro Loss: 0.00016505774692632258, LR: 8.000000000000001e-06\n",
      "Step 883/1000, AUM_micro Loss: 0.0001648481556912884, LR: 8.000000000000001e-06\n",
      "Step 884/1000, AUM_micro Loss: 0.00016474325093440711, LR: 8.000000000000001e-06\n",
      "Step 885/1000, AUM_micro Loss: 0.0001647213357500732, LR: 8.000000000000001e-06\n",
      "Step 886/1000, AUM_micro Loss: 0.00016480404883623123, LR: 8.000000000000001e-06\n",
      "Step 887/1000, AUM_micro Loss: 0.00016469422553200275, LR: 8.000000000000001e-06\n",
      "Step 888/1000, AUM_micro Loss: 0.0001646131422603503, LR: 8.000000000000001e-06\n",
      "Step 889/1000, AUM_micro Loss: 0.00016437469457741827, LR: 8.000000000000001e-06\n",
      "Step 890/1000, AUM_micro Loss: 0.00016437597514595836, LR: 8.000000000000001e-06\n",
      "Step 891/1000, AUM_micro Loss: 0.00016424413479398936, LR: 8.000000000000001e-06\n",
      "Step 892/1000, AUM_micro Loss: 0.00016410017269663513, LR: 8.000000000000001e-06\n",
      "Step 893/1000, AUM_micro Loss: 0.00016385769413318485, LR: 8.000000000000001e-06\n",
      "Step 894/1000, AUM_micro Loss: 0.0001638755784370005, LR: 8.000000000000001e-06\n",
      "Step 895/1000, AUM_micro Loss: 0.00016393218538723886, LR: 8.000000000000001e-06\n",
      "Step 896/1000, AUM_micro Loss: 0.00016386009519919753, LR: 8.000000000000001e-06\n",
      "Step 897/1000, AUM_micro Loss: 0.00016374305414501578, LR: 8.000000000000001e-06\n",
      "Step 898/1000, AUM_micro Loss: 0.00016382895410060883, LR: 8.000000000000001e-06\n",
      "Step 899/1000, AUM_micro Loss: 0.00016380818851757795, LR: 8.000000000000001e-06\n",
      "Step 900/1000, AUM_micro Loss: 0.00016390622477047145, LR: 8.000000000000001e-06\n",
      "Step 901/1000, AUM_micro Loss: 0.00016385001072194427, LR: 8.000000000000001e-06\n",
      "Step 902/1000, AUM_micro Loss: 0.0001634325599297881, LR: 8.000000000000001e-06\n",
      "Step 903/1000, AUM_micro Loss: 0.00016341876471415162, LR: 8.000000000000001e-06\n",
      "Step 904/1000, AUM_micro Loss: 0.00016334466636180878, LR: 8.000000000000001e-06\n",
      "Step 905/1000, AUM_micro Loss: 0.0001631146005820483, LR: 8.000000000000001e-06\n",
      "Step 906/1000, AUM_micro Loss: 0.00016317216795869172, LR: 8.000000000000001e-06\n",
      "Step 907/1000, AUM_micro Loss: 0.00016316706023644656, LR: 8.000000000000001e-06\n",
      "Step 908/1000, AUM_micro Loss: 0.0001630957267479971, LR: 8.000000000000001e-06\n",
      "Step 909/1000, AUM_micro Loss: 0.0001632632192922756, LR: 8.000000000000001e-06\n",
      "Step 910/1000, AUM_micro Loss: 0.00016308070917148143, LR: 8.000000000000001e-06\n",
      "Step 911/1000, AUM_micro Loss: 0.00016277789836749434, LR: 8.000000000000001e-06\n",
      "Step 912/1000, AUM_micro Loss: 0.00016333886014763266, LR: 8.000000000000001e-06\n",
      "Step 913/1000, AUM_micro Loss: 0.00016286619938910007, LR: 8.000000000000001e-06\n",
      "Step 914/1000, AUM_micro Loss: 0.00016273734217975289, LR: 8.000000000000001e-06\n",
      "Step 915/1000, AUM_micro Loss: 0.00016277765098493546, LR: 8.000000000000001e-06\n",
      "Step 916/1000, AUM_micro Loss: 0.00016249064356088638, LR: 8.000000000000001e-06\n",
      "Step 917/1000, AUM_micro Loss: 0.0001624380674911663, LR: 8.000000000000001e-06\n",
      "Step 918/1000, AUM_micro Loss: 0.00016216708172578365, LR: 8.000000000000001e-06\n",
      "Step 919/1000, AUM_micro Loss: 0.00016204053827095777, LR: 8.000000000000001e-06\n",
      "Step 920/1000, AUM_micro Loss: 0.0001619922259123996, LR: 8.000000000000001e-06\n",
      "Step 921/1000, AUM_micro Loss: 0.0001620479888515547, LR: 8.000000000000001e-06\n",
      "Step 922/1000, AUM_micro Loss: 0.00016206530563067645, LR: 8.000000000000001e-06\n",
      "Step 923/1000, AUM_micro Loss: 0.0001620239781914279, LR: 8.000000000000001e-06\n",
      "Step 924/1000, AUM_micro Loss: 0.00016204478743020445, LR: 8.000000000000001e-06\n",
      "Step 925/1000, AUM_micro Loss: 0.00016173420590348542, LR: 8.000000000000001e-06\n",
      "Step 926/1000, AUM_micro Loss: 0.00016173432231880724, LR: 8.000000000000001e-06\n",
      "Step 927/1000, AUM_micro Loss: 0.00016179658996406943, LR: 8.000000000000001e-06\n",
      "Step 928/1000, AUM_micro Loss: 0.00016177663928829134, LR: 8.000000000000001e-06\n",
      "Step 929/1000, AUM_micro Loss: 0.00016155262710526586, LR: 8.000000000000001e-06\n",
      "Step 930/1000, AUM_micro Loss: 0.00016151425370480865, LR: 8.000000000000001e-06\n",
      "Step 931/1000, AUM_micro Loss: 0.000161519565153867, LR: 8.000000000000001e-06\n",
      "Step 932/1000, AUM_micro Loss: 0.00016157641948666424, LR: 8.000000000000001e-06\n",
      "Step 933/1000, AUM_micro Loss: 0.00016146567941177636, LR: 8.000000000000001e-06\n",
      "Step 934/1000, AUM_micro Loss: 0.00016152509488165379, LR: 8.000000000000001e-06\n",
      "Step 935/1000, AUM_micro Loss: 0.00016131550364661962, LR: 8.000000000000001e-06\n",
      "Step 936/1000, AUM_micro Loss: 0.00016123625391628593, LR: 8.000000000000001e-06\n",
      "Step 937/1000, AUM_micro Loss: 0.00016125837282743305, LR: 8.000000000000001e-06\n",
      "Step 938/1000, AUM_micro Loss: 0.00016122142551466823, LR: 8.000000000000001e-06\n",
      "Step 939/1000, AUM_micro Loss: 0.00016097453772090375, LR: 8.000000000000001e-06\n",
      "Step 940/1000, AUM_micro Loss: 0.00016095646424219012, LR: 8.000000000000001e-06\n",
      "Step 941/1000, AUM_micro Loss: 0.0001605959259904921, LR: 8.000000000000001e-06\n",
      "Step 942/1000, AUM_micro Loss: 0.00016044265066739172, LR: 8.000000000000001e-06\n",
      "Step 943/1000, AUM_micro Loss: 0.00016029759717639536, LR: 8.000000000000001e-06\n",
      "Step 944/1000, AUM_micro Loss: 0.00016020388284232467, LR: 8.000000000000001e-06\n",
      "Step 945/1000, AUM_micro Loss: 0.0001601718395249918, LR: 8.000000000000001e-06\n",
      "Step 946/1000, AUM_micro Loss: 0.00016008099191822112, LR: 8.000000000000001e-06\n",
      "Step 947/1000, AUM_micro Loss: 0.00015991396503522992, LR: 8.000000000000001e-06\n",
      "Step 948/1000, AUM_micro Loss: 0.0001598863600520417, LR: 8.000000000000001e-06\n",
      "Step 949/1000, AUM_micro Loss: 0.0001598288945388049, LR: 8.000000000000001e-06\n",
      "Step 950/1000, AUM_micro Loss: 0.00015979234012775123, LR: 8.000000000000001e-06\n",
      "Step 951/1000, AUM_micro Loss: 0.00015975280257407576, LR: 8.000000000000001e-06\n",
      "Step 952/1000, AUM_micro Loss: 0.0001596454094396904, LR: 8.000000000000001e-06\n",
      "Step 953/1000, AUM_micro Loss: 0.0001597521622898057, LR: 8.000000000000001e-06\n",
      "Step 954/1000, AUM_micro Loss: 0.0001598263334017247, LR: 8.000000000000001e-06\n",
      "Step 955/1000, AUM_micro Loss: 0.00015974124835338444, LR: 8.000000000000001e-06\n",
      "Step 956/1000, AUM_micro Loss: 0.000160694879014045, LR: 8.000000000000001e-06\n",
      "Step 957/1000, AUM_micro Loss: 0.0001599079987499863, LR: 8.000000000000001e-06\n",
      "Step 958/1000, AUM_micro Loss: 0.000160034658620134, LR: 8.000000000000001e-06\n",
      "Step 959/1000, AUM_micro Loss: 0.00019206487922929227, LR: 8.000000000000001e-06\n",
      "Step 960/1000, AUM_micro Loss: 0.0001930102880578488, LR: 8.000000000000001e-06\n",
      "Step 961/1000, AUM_micro Loss: 0.00018626349628902972, LR: 8.000000000000001e-06\n",
      "Step 962/1000, AUM_micro Loss: 0.00015977125440258533, LR: 8.000000000000001e-06\n",
      "Step 963/1000, AUM_micro Loss: 0.00015972545952536166, LR: 8.000000000000001e-06\n",
      "Step 964/1000, AUM_micro Loss: 0.00015956678544171154, LR: 8.000000000000001e-06\n",
      "Step 965/1000, AUM_micro Loss: 0.0001594234345247969, LR: 8.000000000000001e-06\n",
      "Step 966/1000, AUM_micro Loss: 0.00015930824156384915, LR: 8.000000000000001e-06\n",
      "Step 967/1000, AUM_micro Loss: 0.000159185248776339, LR: 8.000000000000001e-06\n",
      "Step 968/1000, AUM_micro Loss: 0.0001587006845511496, LR: 8.000000000000001e-06\n",
      "Step 969/1000, AUM_micro Loss: 0.00016180425882339478, LR: 8.000000000000001e-06\n",
      "Step 970/1000, AUM_micro Loss: 0.00015936270938254893, LR: 8.000000000000001e-06\n",
      "Step 971/1000, AUM_micro Loss: 0.00015890112263150513, LR: 8.000000000000001e-06\n",
      "Step 972/1000, AUM_micro Loss: 0.0001589532330399379, LR: 8.000000000000001e-06\n",
      "Step 973/1000, AUM_micro Loss: 0.00015912257367745042, LR: 8.000000000000001e-06\n",
      "Step 974/1000, AUM_micro Loss: 0.0001591877080500126, LR: 8.000000000000001e-06\n",
      "Step 975/1000, AUM_micro Loss: 0.00015916768461465836, LR: 8.000000000000001e-06\n",
      "Step 976/1000, AUM_micro Loss: 0.00015870496281422675, LR: 8.000000000000001e-06\n",
      "Step 977/1000, AUM_micro Loss: 0.00015866670582909137, LR: 8.000000000000001e-06\n",
      "Step 978/1000, AUM_micro Loss: 0.000158621885930188, LR: 8.000000000000001e-06\n",
      "Step 979/1000, AUM_micro Loss: 0.0001584320270922035, LR: 8.000000000000001e-06\n",
      "Step 980/1000, AUM_micro Loss: 0.00015837341197766364, LR: 8.000000000000001e-06\n",
      "Step 981/1000, AUM_micro Loss: 0.00015852684737183154, LR: 8.000000000000001e-06\n",
      "Step 982/1000, AUM_micro Loss: 0.00015836386592127383, LR: 8.000000000000001e-06\n",
      "Step 983/1000, AUM_micro Loss: 0.00015828173491172493, LR: 8.000000000000001e-06\n",
      "Step 984/1000, AUM_micro Loss: 0.00015810324111953378, LR: 8.000000000000001e-06\n",
      "Step 985/1000, AUM_micro Loss: 0.00015810702461749315, LR: 8.000000000000001e-06\n",
      "Step 986/1000, AUM_micro Loss: 0.0001581053074914962, LR: 8.000000000000001e-06\n",
      "Step 987/1000, AUM_micro Loss: 0.00015812476340215653, LR: 8.000000000000001e-06\n",
      "Step 988/1000, AUM_micro Loss: 0.00015794276259839535, LR: 8.000000000000001e-06\n",
      "Step 989/1000, AUM_micro Loss: 0.00015781627735123038, LR: 8.000000000000001e-06\n",
      "Step 990/1000, AUM_micro Loss: 0.00015780980174895376, LR: 8.000000000000001e-06\n",
      "Step 991/1000, AUM_micro Loss: 0.00015798985259607434, LR: 8.000000000000001e-06\n",
      "Step 992/1000, AUM_micro Loss: 0.00015792649355717003, LR: 8.000000000000001e-06\n",
      "Step 993/1000, AUM_micro Loss: 0.0001578603551024571, LR: 8.000000000000001e-06\n",
      "Step 994/1000, AUM_micro Loss: 0.00015797634841874242, LR: 8.000000000000001e-06\n",
      "Step 995/1000, AUM_micro Loss: 0.00015842739958316088, LR: 8.000000000000001e-06\n",
      "Step 996/1000, AUM_micro Loss: 0.0001578045339556411, LR: 8.000000000000001e-06\n",
      "Step 997/1000, AUM_micro Loss: 0.00015767590957693756, LR: 8.000000000000001e-06\n",
      "Step 998/1000, AUM_micro Loss: 0.00015764209092594683, LR: 8.000000000000001e-06\n",
      "Step 999/1000, AUM_micro Loss: 0.00015748789883218706, LR: 8.000000000000001e-06\n",
      "Step 1000/1000, AUM_micro Loss: 0.0001576232025399804, LR: 8.000000000000001e-06\n",
      "Step 2/1000, AUM_macro Loss: 0.3213498294353485, LR: 0.001\n",
      "Step 3/1000, AUM_macro Loss: 0.26720044016838074, LR: 0.001\n",
      "Step 4/1000, AUM_macro Loss: 0.2255040407180786, LR: 0.001\n",
      "Step 5/1000, AUM_macro Loss: 0.193778395652771, LR: 0.001\n",
      "Step 6/1000, AUM_macro Loss: 0.17004868388175964, LR: 0.001\n",
      "Step 7/1000, AUM_macro Loss: 0.14051476120948792, LR: 0.001\n",
      "Step 8/1000, AUM_macro Loss: 0.1201835572719574, LR: 0.001\n",
      "Step 9/1000, AUM_macro Loss: 0.09587371349334717, LR: 0.001\n",
      "Step 10/1000, AUM_macro Loss: 0.08729471266269684, LR: 0.001\n",
      "Step 11/1000, AUM_macro Loss: 0.07025060802698135, LR: 0.001\n",
      "Step 12/1000, AUM_macro Loss: 0.05700554698705673, LR: 0.001\n",
      "Step 13/1000, AUM_macro Loss: 0.04708263278007507, LR: 0.001\n",
      "Step 14/1000, AUM_macro Loss: 0.042420703917741776, LR: 0.001\n",
      "Step 15/1000, AUM_macro Loss: 0.03493056818842888, LR: 0.001\n",
      "Step 16/1000, AUM_macro Loss: 0.029253611341118813, LR: 0.001\n",
      "Step 17/1000, AUM_macro Loss: 0.025505801662802696, LR: 0.001\n",
      "Step 18/1000, AUM_macro Loss: 0.01920432224869728, LR: 0.001\n",
      "Step 19/1000, AUM_macro Loss: 0.01589365117251873, LR: 0.001\n",
      "Step 20/1000, AUM_macro Loss: 0.013727125711739063, LR: 0.001\n",
      "Step 21/1000, AUM_macro Loss: 0.01112357061356306, LR: 0.001\n",
      "Step 22/1000, AUM_macro Loss: 0.009323765523731709, LR: 0.001\n",
      "Step 23/1000, AUM_macro Loss: 0.007910980843007565, LR: 0.001\n",
      "Step 24/1000, AUM_macro Loss: 0.006639264989644289, LR: 0.001\n",
      "Step 25/1000, AUM_macro Loss: 0.00551857054233551, LR: 0.001\n",
      "Step 26/1000, AUM_macro Loss: 0.00493104662746191, LR: 0.001\n",
      "Step 27/1000, AUM_macro Loss: 0.004715129733085632, LR: 0.001\n",
      "Step 28/1000, AUM_macro Loss: 0.004027658607810736, LR: 0.001\n",
      "Step 29/1000, AUM_macro Loss: 0.0032186114694923162, LR: 0.001\n",
      "Step 30/1000, AUM_macro Loss: 0.0028226834256201982, LR: 0.001\n",
      "Step 31/1000, AUM_macro Loss: 0.002436843467876315, LR: 0.001\n",
      "Step 32/1000, AUM_macro Loss: 0.0022626034915447235, LR: 0.001\n",
      "Step 33/1000, AUM_macro Loss: 0.002149269450455904, LR: 0.001\n",
      "Step 34/1000, AUM_macro Loss: 0.001952987164258957, LR: 0.001\n",
      "Step 35/1000, AUM_macro Loss: 0.0017989237094298005, LR: 0.001\n",
      "Step 36/1000, AUM_macro Loss: 0.0016122573288157582, LR: 0.001\n",
      "Step 37/1000, AUM_macro Loss: 0.0014281116891652346, LR: 0.001\n",
      "Step 38/1000, AUM_macro Loss: 0.0012466751504689455, LR: 0.001\n",
      "Step 39/1000, AUM_macro Loss: 0.0010590096935629845, LR: 0.001\n",
      "Step 40/1000, AUM_macro Loss: 0.000983544741757214, LR: 0.001\n",
      "Step 41/1000, AUM_macro Loss: 0.0008923251298256218, LR: 0.001\n",
      "Step 42/1000, AUM_macro Loss: 0.0008117934339679778, LR: 0.001\n",
      "Step 43/1000, AUM_macro Loss: 0.0007278947159647942, LR: 0.001\n",
      "Step 44/1000, AUM_macro Loss: 0.0006615914753638208, LR: 0.001\n",
      "Step 45/1000, AUM_macro Loss: 0.0006206079269759357, LR: 0.001\n",
      "Step 46/1000, AUM_macro Loss: 0.0005625589983537793, LR: 0.001\n",
      "Step 47/1000, AUM_macro Loss: 0.0005003886763006449, LR: 0.001\n",
      "Step 48/1000, AUM_macro Loss: 0.0004645998706109822, LR: 0.001\n",
      "Step 49/1000, AUM_macro Loss: 0.00043107374222017825, LR: 0.001\n",
      "Step 50/1000, AUM_macro Loss: 0.0003978776221629232, LR: 0.001\n",
      "Step 51/1000, AUM_macro Loss: 0.0003472433309070766, LR: 0.001\n",
      "Step 52/1000, AUM_macro Loss: 0.00033249997068196535, LR: 0.001\n",
      "Step 53/1000, AUM_macro Loss: 0.0003157693427056074, LR: 0.001\n",
      "Step 54/1000, AUM_macro Loss: 0.00030225637601688504, LR: 0.001\n",
      "Step 55/1000, AUM_macro Loss: 0.00029000244103372097, LR: 0.001\n",
      "Step 56/1000, AUM_macro Loss: 0.000274333986453712, LR: 0.001\n",
      "Step 57/1000, AUM_macro Loss: 0.0002593384124338627, LR: 0.001\n",
      "Step 58/1000, AUM_macro Loss: 0.0002890664036385715, LR: 0.001\n",
      "Step 59/1000, AUM_macro Loss: 0.0002583300811238587, LR: 0.001\n",
      "Step 60/1000, AUM_macro Loss: 0.00022511454881168902, LR: 0.001\n",
      "Step 61/1000, AUM_macro Loss: 0.00021695673058275133, LR: 0.001\n",
      "Step 62/1000, AUM_macro Loss: 0.0002123141021002084, LR: 0.001\n",
      "Step 63/1000, AUM_macro Loss: 0.00021032373479101807, LR: 0.001\n",
      "Step 64/1000, AUM_macro Loss: 0.00021419602853711694, LR: 0.001\n",
      "Step 65/1000, AUM_macro Loss: 0.00019934323790948838, LR: 0.001\n",
      "Step 66/1000, AUM_macro Loss: 0.00019087764667347074, LR: 0.001\n",
      "Step 67/1000, AUM_macro Loss: 0.0001767627545632422, LR: 0.001\n",
      "Step 68/1000, AUM_macro Loss: 0.0001695856626611203, LR: 0.001\n",
      "Step 69/1000, AUM_macro Loss: 0.00016404472989961505, LR: 0.001\n",
      "Step 70/1000, AUM_macro Loss: 0.00015911852824501693, LR: 0.001\n",
      "Step 71/1000, AUM_macro Loss: 0.00014062112313695252, LR: 0.001\n",
      "Step 72/1000, AUM_macro Loss: 0.00013774060062132776, LR: 0.001\n",
      "Step 73/1000, AUM_macro Loss: 0.0001345748023595661, LR: 0.001\n",
      "Step 74/1000, AUM_macro Loss: 0.00012527950457297266, LR: 0.001\n",
      "Step 75/1000, AUM_macro Loss: 0.00011683761840686202, LR: 0.001\n",
      "Step 76/1000, AUM_macro Loss: 0.00010533861495787278, LR: 0.001\n",
      "Step 77/1000, AUM_macro Loss: 0.0001058853740687482, LR: 0.001\n",
      "Step 78/1000, AUM_macro Loss: 9.828186011873186e-05, LR: 0.001\n",
      "Step 79/1000, AUM_macro Loss: 9.455729741603136e-05, LR: 0.001\n",
      "Step 80/1000, AUM_macro Loss: 9.815824159886688e-05, LR: 0.001\n",
      "Step 81/1000, AUM_macro Loss: 8.9762601419352e-05, LR: 0.001\n",
      "Step 82/1000, AUM_macro Loss: 8.65358451846987e-05, LR: 0.001\n",
      "Step 83/1000, AUM_macro Loss: 8.582253940403461e-05, LR: 0.001\n",
      "Step 84/1000, AUM_macro Loss: 7.814697892172262e-05, LR: 0.001\n",
      "Step 85/1000, AUM_macro Loss: 7.953673048177734e-05, LR: 0.001\n",
      "Step 86/1000, AUM_macro Loss: 7.667460886295885e-05, LR: 0.001\n",
      "Step 87/1000, AUM_macro Loss: 7.092495798133314e-05, LR: 0.001\n",
      "Step 88/1000, AUM_macro Loss: 6.796963862143457e-05, LR: 0.001\n",
      "Step 89/1000, AUM_macro Loss: 6.715494237141684e-05, LR: 0.001\n",
      "Step 90/1000, AUM_macro Loss: 6.463713361881673e-05, LR: 0.001\n",
      "Step 91/1000, AUM_macro Loss: 6.39777208562009e-05, LR: 0.001\n",
      "Step 92/1000, AUM_macro Loss: 6.268351717153564e-05, LR: 0.001\n",
      "Step 93/1000, AUM_macro Loss: 5.905895886826329e-05, LR: 0.001\n",
      "Step 94/1000, AUM_macro Loss: 5.472116754390299e-05, LR: 0.001\n",
      "Step 95/1000, AUM_macro Loss: 5.1736002205871046e-05, LR: 0.001\n",
      "Step 96/1000, AUM_macro Loss: 5.051878179074265e-05, LR: 0.001\n",
      "Step 97/1000, AUM_macro Loss: 5.0042493967339396e-05, LR: 0.001\n",
      "Step 98/1000, AUM_macro Loss: 4.840768815483898e-05, LR: 0.001\n",
      "Step 99/1000, AUM_macro Loss: 4.819138121092692e-05, LR: 0.001\n",
      "Step 100/1000, AUM_macro Loss: 4.7562236431986094e-05, LR: 0.001\n",
      "Step 101/1000, AUM_macro Loss: 4.6587345423176885e-05, LR: 0.001\n",
      "Step 102/1000, AUM_macro Loss: 4.607835580827668e-05, LR: 0.001\n",
      "Step 103/1000, AUM_macro Loss: 4.4538468500832096e-05, LR: 0.001\n",
      "Step 104/1000, AUM_macro Loss: 4.377880759420805e-05, LR: 0.001\n",
      "Step 105/1000, AUM_macro Loss: 4.496586188906804e-05, LR: 0.001\n",
      "Step 106/1000, AUM_macro Loss: 4.6464807383017614e-05, LR: 0.001\n",
      "Step 107/1000, AUM_macro Loss: 4.9043013859773055e-05, LR: 0.001\n",
      "Step 108/1000, AUM_macro Loss: 4.6972847485449165e-05, LR: 0.001\n",
      "Step 109/1000, AUM_macro Loss: 4.5115772081771865e-05, LR: 0.001\n",
      "Step 110/1000, AUM_macro Loss: 4.309532232582569e-05, LR: 0.001\n",
      "Step 111/1000, AUM_macro Loss: 4.531818922259845e-05, LR: 0.001\n",
      "Step 112/1000, AUM_macro Loss: 4.5020191464573145e-05, LR: 0.001\n",
      "Step 113/1000, AUM_macro Loss: 4.286029434297234e-05, LR: 0.001\n",
      "Step 114/1000, AUM_macro Loss: 4.1996361687779427e-05, LR: 0.001\n",
      "Step 115/1000, AUM_macro Loss: 4.052717122249305e-05, LR: 0.001\n",
      "Step 116/1000, AUM_macro Loss: 3.850516804959625e-05, LR: 0.001\n",
      "Step 117/1000, AUM_macro Loss: 3.86730425816495e-05, LR: 0.001\n",
      "Step 118/1000, AUM_macro Loss: 3.855940303765237e-05, LR: 0.001\n",
      "Step 119/1000, AUM_macro Loss: 3.8726433558622375e-05, LR: 0.001\n",
      "Step 120/1000, AUM_macro Loss: 3.569645923562348e-05, LR: 0.001\n",
      "Step 121/1000, AUM_macro Loss: 3.844880120595917e-05, LR: 0.001\n",
      "Step 122/1000, AUM_macro Loss: 4.198503665975295e-05, LR: 0.001\n",
      "Step 123/1000, AUM_macro Loss: 4.014493606518954e-05, LR: 0.001\n",
      "Step 124/1000, AUM_macro Loss: 3.8331956602633e-05, LR: 0.001\n",
      "Step 125/1000, AUM_macro Loss: 3.4590131690492854e-05, LR: 0.001\n",
      "Step 126/1000, AUM_macro Loss: 3.11861076625064e-05, LR: 0.001\n",
      "Step 127/1000, AUM_macro Loss: 2.93070243060356e-05, LR: 0.001\n",
      "Step 128/1000, AUM_macro Loss: 2.8703663701890036e-05, LR: 0.001\n",
      "Step 129/1000, AUM_macro Loss: 2.5830784579738975e-05, LR: 0.001\n",
      "Step 130/1000, AUM_macro Loss: 2.603986649774015e-05, LR: 0.001\n",
      "Step 131/1000, AUM_macro Loss: 2.7383723136154003e-05, LR: 0.001\n",
      "Step 132/1000, AUM_macro Loss: 2.7307420168654062e-05, LR: 0.001\n",
      "Step 133/1000, AUM_macro Loss: 2.5183648176607676e-05, LR: 0.001\n",
      "Step 134/1000, AUM_macro Loss: 2.5582554371794686e-05, LR: 0.001\n",
      "Step 135/1000, AUM_macro Loss: 2.4320952434209175e-05, LR: 0.001\n",
      "Step 136/1000, AUM_macro Loss: 2.3113954739528708e-05, LR: 0.001\n",
      "Step 137/1000, AUM_macro Loss: 2.35478892136598e-05, LR: 0.001\n",
      "Step 138/1000, AUM_macro Loss: 2.4951516024884768e-05, LR: 0.001\n",
      "Step 139/1000, AUM_macro Loss: 2.4290900910273194e-05, LR: 0.001\n",
      "Step 140/1000, AUM_macro Loss: 2.2889627871336415e-05, LR: 0.001\n",
      "Step 141/1000, AUM_macro Loss: 2.214919550169725e-05, LR: 0.001\n",
      "Step 142/1000, AUM_macro Loss: 1.9929977497668006e-05, LR: 0.001\n",
      "Step 143/1000, AUM_macro Loss: 2.0530909750959836e-05, LR: 0.001\n",
      "Step 144/1000, AUM_macro Loss: 1.9291750504635274e-05, LR: 0.001\n",
      "Step 145/1000, AUM_macro Loss: 2.0306641090428457e-05, LR: 0.001\n",
      "Step 146/1000, AUM_macro Loss: 2.2704283765051514e-05, LR: 0.001\n",
      "Step 147/1000, AUM_macro Loss: 1.981588138733059e-05, LR: 0.001\n",
      "Step 148/1000, AUM_macro Loss: 2.3369577320409007e-05, LR: 0.001\n",
      "Step 149/1000, AUM_macro Loss: 1.9286368114990182e-05, LR: 0.001\n",
      "Step 150/1000, AUM_macro Loss: 3.1787072657607496e-05, LR: 0.001\n",
      "Step 151/1000, AUM_macro Loss: 3.5912395105697215e-05, LR: 0.001\n",
      "Step 152/1000, AUM_macro Loss: 2.1632231437251903e-05, LR: 0.001\n",
      "Step 153/1000, AUM_macro Loss: 1.8580987671157345e-05, LR: 0.001\n",
      "Step 154/1000, AUM_macro Loss: 1.8287186321686022e-05, LR: 0.001\n",
      "Step 155/1000, AUM_macro Loss: 1.7159716662717983e-05, LR: 0.001\n",
      "Step 156/1000, AUM_macro Loss: 1.803600025596097e-05, LR: 0.001\n",
      "Step 157/1000, AUM_macro Loss: 1.8390883269603364e-05, LR: 0.001\n",
      "Step 158/1000, AUM_macro Loss: 2.4832967028487474e-05, LR: 0.001\n",
      "Step 159/1000, AUM_macro Loss: 2.6927322323899716e-05, LR: 0.001\n",
      "Step 160/1000, AUM_macro Loss: 2.449632120260503e-05, LR: 0.001\n",
      "Step 161/1000, AUM_macro Loss: 1.7841559383668937e-05, LR: 0.001\n",
      "Step 162/1000, AUM_macro Loss: 2.015698373725172e-05, LR: 0.001\n",
      "Step 163/1000, AUM_macro Loss: 2.2737311155651696e-05, LR: 0.001\n",
      "Step 164/1000, AUM_macro Loss: 1.908440754050389e-05, LR: 0.001\n",
      "Step 165/1000, AUM_macro Loss: 1.971193341887556e-05, LR: 0.001\n",
      "Step 166/1000, AUM_macro Loss: 1.91059589269571e-05, LR: 0.001\n",
      "Step 167/1000, AUM_macro Loss: 1.7363952792948112e-05, LR: 0.001\n",
      "Step 168/1000, AUM_macro Loss: 1.8649636331247166e-05, LR: 0.001\n",
      "Step 169/1000, AUM_macro Loss: 1.6930493075051345e-05, LR: 0.001\n",
      "Step 170/1000, AUM_macro Loss: 1.7217736967722885e-05, LR: 0.001\n",
      "Step 171/1000, AUM_macro Loss: 1.408222033205675e-05, LR: 0.001\n",
      "Step 172/1000, AUM_macro Loss: 1.7564456356922165e-05, LR: 0.001\n",
      "Step 173/1000, AUM_macro Loss: 1.316754878644133e-05, LR: 0.001\n",
      "Step 174/1000, AUM_macro Loss: 2.2220094251679257e-05, LR: 0.001\n",
      "Step 175/1000, AUM_macro Loss: 3.2140651455847546e-05, LR: 0.001\n",
      "Step 176/1000, AUM_macro Loss: 2.53607449849369e-05, LR: 0.001\n",
      "Step 177/1000, AUM_macro Loss: 2.3055432393448427e-05, LR: 0.001\n",
      "Step 178/1000, AUM_macro Loss: 1.3283873158798087e-05, LR: 0.001\n",
      "Step 179/1000, AUM_macro Loss: 1.4587662917620037e-05, LR: 0.001\n",
      "Step 180/1000, AUM_macro Loss: 1.7802631191443652e-05, LR: 0.001\n",
      "Step 181/1000, AUM_macro Loss: 1.3665208825841546e-05, LR: 0.001\n",
      "Step 182/1000, AUM_macro Loss: 1.3718381524085999e-05, LR: 0.001\n",
      "Step 183/1000, AUM_macro Loss: 1.593140041222796e-05, LR: 0.001\n",
      "Step 184/1000, AUM_macro Loss: 1.4185323379933834e-05, LR: 0.001\n",
      "Step 185/1000, AUM_macro Loss: 1.5137848095037043e-05, LR: 0.001\n",
      "Step 186/1000, AUM_macro Loss: 1.8009735867963172e-05, LR: 0.001\n",
      "Step 187/1000, AUM_macro Loss: 1.9207080185879022e-05, LR: 0.001\n",
      "Step 188/1000, AUM_macro Loss: 1.765981323842425e-05, LR: 0.001\n",
      "Step 189/1000, AUM_macro Loss: 1.5203573639155366e-05, LR: 0.001\n",
      "Step 190/1000, AUM_macro Loss: 1.4167104382067919e-05, LR: 0.001\n",
      "Step 191/1000, AUM_macro Loss: 1.1927724699489772e-05, LR: 0.001\n",
      "Step 192/1000, AUM_macro Loss: 1.107558546209475e-05, LR: 0.001\n",
      "Step 193/1000, AUM_macro Loss: 1.1224323316127993e-05, LR: 0.001\n",
      "Step 194/1000, AUM_macro Loss: 1.2179697478131857e-05, LR: 0.001\n",
      "Step 195/1000, AUM_macro Loss: 1.0636263141350355e-05, LR: 0.001\n",
      "Step 196/1000, AUM_macro Loss: 1.0622602530929726e-05, LR: 0.001\n",
      "Step 197/1000, AUM_macro Loss: 1.1896855539816897e-05, LR: 0.001\n",
      "Step 198/1000, AUM_macro Loss: 8.734835319046397e-06, LR: 0.001\n",
      "Step 199/1000, AUM_macro Loss: 7.339568128372775e-06, LR: 0.001\n",
      "Step 200/1000, AUM_macro Loss: 6.774649136787048e-06, LR: 0.001\n",
      "Step 201/1000, AUM_macro Loss: 7.64164997235639e-06, LR: 0.001\n",
      "Step 202/1000, AUM_macro Loss: 8.658381375425961e-06, LR: 0.001\n",
      "Step 203/1000, AUM_macro Loss: 7.720110261288937e-06, LR: 0.001\n",
      "Step 204/1000, AUM_macro Loss: 9.007128028315492e-06, LR: 0.001\n",
      "Step 205/1000, AUM_macro Loss: 1.3273141121317167e-05, LR: 0.001\n",
      "Step 206/1000, AUM_macro Loss: 8.588245691498742e-06, LR: 0.001\n",
      "Step 207/1000, AUM_macro Loss: 1.0604396265989635e-05, LR: 0.001\n",
      "Step 208/1000, AUM_macro Loss: 1.153585071733687e-05, LR: 0.001\n",
      "Step 209/1000, AUM_macro Loss: 7.6216620072955266e-06, LR: 0.001\n",
      "Step 210/1000, AUM_macro Loss: 7.3632950261526275e-06, LR: 0.001\n",
      "Step 211/1000, AUM_macro Loss: 6.661273801000789e-06, LR: 0.001\n",
      "Step 212/1000, AUM_macro Loss: 1.3737154404225294e-05, LR: 0.001\n",
      "Step 213/1000, AUM_macro Loss: 9.546362889523152e-06, LR: 0.001\n",
      "Step 214/1000, AUM_macro Loss: 8.650015843159053e-06, LR: 0.001\n",
      "Step 215/1000, AUM_macro Loss: 6.697972366964677e-06, LR: 0.001\n",
      "Step 216/1000, AUM_macro Loss: 7.722665941400919e-06, LR: 0.001\n",
      "Step 217/1000, AUM_macro Loss: 7.918009941931814e-06, LR: 0.001\n",
      "Step 218/1000, AUM_macro Loss: 7.983914656506386e-06, LR: 0.001\n",
      "Step 219/1000, AUM_macro Loss: 6.794154614908621e-06, LR: 0.001\n",
      "Step 220/1000, AUM_macro Loss: 7.417020697175758e-06, LR: 0.001\n",
      "Step 221/1000, AUM_macro Loss: 7.307973191927886e-06, LR: 0.001\n",
      "Step 222/1000, AUM_macro Loss: 5.5410396271327045e-06, LR: 0.001\n",
      "Step 223/1000, AUM_macro Loss: 5.648315891448874e-06, LR: 0.001\n",
      "Step 224/1000, AUM_macro Loss: 1.1465630450402386e-05, LR: 0.001\n",
      "Step 225/1000, AUM_macro Loss: 7.00607051840052e-06, LR: 0.001\n",
      "Step 226/1000, AUM_macro Loss: 3.910650048055686e-05, LR: 0.001\n",
      "Step 227/1000, AUM_macro Loss: 1.425590835424373e-05, LR: 0.001\n",
      "Step 228/1000, AUM_macro Loss: 2.1987800209899433e-05, LR: 0.001\n",
      "Step 229/1000, AUM_macro Loss: 3.176676909788512e-05, LR: 0.001\n",
      "Step 230/1000, AUM_macro Loss: 3.1643870897823945e-05, LR: 0.001\n",
      "Step 231/1000, AUM_macro Loss: 2.8669195671682246e-05, LR: 0.001\n",
      "Step 232/1000, AUM_macro Loss: 2.762214353424497e-05, LR: 0.001\n",
      "Step 233/1000, AUM_macro Loss: 2.9529588573495857e-05, LR: 0.001\n",
      "Step 234/1000, AUM_macro Loss: 2.971862159029115e-05, LR: 0.001\n",
      "Step 235/1000, AUM_macro Loss: 2.7407222660258412e-05, LR: 0.001\n",
      "Step 236/1000, AUM_macro Loss: 2.2654325221083127e-05, LR: 0.001\n",
      "Step 237/1000, AUM_macro Loss: 1.887563121272251e-05, LR: 0.001\n",
      "Step 238/1000, AUM_macro Loss: 2.3460288502974436e-05, LR: 0.001\n",
      "Step 239/1000, AUM_macro Loss: 1.9992288798675872e-05, LR: 0.001\n",
      "Step 240/1000, AUM_macro Loss: 1.9767081539612263e-05, LR: 0.001\n",
      "Step 241/1000, AUM_macro Loss: 1.641623930481728e-05, LR: 0.001\n",
      "Step 242/1000, AUM_macro Loss: 1.647289354878012e-05, LR: 0.001\n",
      "Step 243/1000, AUM_macro Loss: 1.79769704118371e-05, LR: 0.0002\n",
      "Step 244/1000, AUM_macro Loss: 1.4437207028095145e-05, LR: 0.0002\n",
      "Step 245/1000, AUM_macro Loss: 1.4476707292487845e-05, LR: 0.0002\n",
      "Step 246/1000, AUM_macro Loss: 1.3001908882870339e-05, LR: 0.0002\n",
      "Step 247/1000, AUM_macro Loss: 1.2469953617255669e-05, LR: 0.0002\n",
      "Step 248/1000, AUM_macro Loss: 1.2147218512836844e-05, LR: 0.0002\n",
      "Step 249/1000, AUM_macro Loss: 1.1838445061584935e-05, LR: 0.0002\n",
      "Step 250/1000, AUM_macro Loss: 1.1780860404542182e-05, LR: 0.0002\n",
      "Step 251/1000, AUM_macro Loss: 1.1302734492346644e-05, LR: 0.0002\n",
      "Step 252/1000, AUM_macro Loss: 1.1660124073387124e-05, LR: 0.0002\n",
      "Step 253/1000, AUM_macro Loss: 1.125529070122866e-05, LR: 0.0002\n",
      "Step 254/1000, AUM_macro Loss: 1.113758571591461e-05, LR: 0.0002\n",
      "Step 255/1000, AUM_macro Loss: 1.1002779501723126e-05, LR: 0.0002\n",
      "Step 256/1000, AUM_macro Loss: 1.0639893844199833e-05, LR: 0.0002\n",
      "Step 257/1000, AUM_macro Loss: 9.96294056676561e-06, LR: 0.0002\n",
      "Step 258/1000, AUM_macro Loss: 9.883522579912096e-06, LR: 0.0002\n",
      "Step 259/1000, AUM_macro Loss: 9.629657142795622e-06, LR: 0.0002\n",
      "Step 260/1000, AUM_macro Loss: 9.615503586246632e-06, LR: 0.0002\n",
      "Step 261/1000, AUM_macro Loss: 8.959576916822698e-06, LR: 0.0002\n",
      "Step 262/1000, AUM_macro Loss: 8.837545465212315e-06, LR: 0.0002\n",
      "Step 263/1000, AUM_macro Loss: 8.953921678767074e-06, LR: 0.0002\n",
      "Step 264/1000, AUM_macro Loss: 8.942184649640694e-06, LR: 4e-05\n",
      "Step 265/1000, AUM_macro Loss: 8.679174243297894e-06, LR: 4e-05\n",
      "Step 266/1000, AUM_macro Loss: 8.538538168068044e-06, LR: 4e-05\n",
      "Step 267/1000, AUM_macro Loss: 8.610138138465118e-06, LR: 4e-05\n",
      "Step 268/1000, AUM_macro Loss: 8.488887033308856e-06, LR: 4e-05\n",
      "Step 269/1000, AUM_macro Loss: 8.31870420370251e-06, LR: 4e-05\n",
      "Step 270/1000, AUM_macro Loss: 8.613190402684268e-06, LR: 4e-05\n",
      "Step 271/1000, AUM_macro Loss: 8.918242201616522e-06, LR: 4e-05\n",
      "Step 272/1000, AUM_macro Loss: 8.980281563708559e-06, LR: 4e-05\n",
      "Step 273/1000, AUM_macro Loss: 8.852180144458544e-06, LR: 4e-05\n",
      "Step 274/1000, AUM_macro Loss: 8.965418601292185e-06, LR: 4e-05\n",
      "Step 275/1000, AUM_macro Loss: 8.835791959427297e-06, LR: 4e-05\n",
      "Step 276/1000, AUM_macro Loss: 8.466146027785726e-06, LR: 4e-05\n",
      "Step 277/1000, AUM_macro Loss: 8.544567208446097e-06, LR: 4e-05\n",
      "Step 278/1000, AUM_macro Loss: 8.632297976873815e-06, LR: 4e-05\n",
      "Step 279/1000, AUM_macro Loss: 8.223373697546776e-06, LR: 4e-05\n",
      "Step 280/1000, AUM_macro Loss: 8.852010068949312e-06, LR: 4e-05\n",
      "Step 281/1000, AUM_macro Loss: 9.251077244698536e-06, LR: 4e-05\n",
      "Step 282/1000, AUM_macro Loss: 9.082821634365246e-06, LR: 4e-05\n",
      "Step 283/1000, AUM_macro Loss: 8.954096301749814e-06, LR: 4e-05\n",
      "Step 284/1000, AUM_macro Loss: 8.485342732456047e-06, LR: 4e-05\n",
      "Step 285/1000, AUM_macro Loss: 8.624507245258428e-06, LR: 8.000000000000001e-06\n",
      "Step 286/1000, AUM_macro Loss: 8.781817996350583e-06, LR: 8.000000000000001e-06\n",
      "Step 287/1000, AUM_macro Loss: 8.746394087211229e-06, LR: 8.000000000000001e-06\n",
      "Step 288/1000, AUM_macro Loss: 8.70108760864241e-06, LR: 8.000000000000001e-06\n",
      "Step 289/1000, AUM_macro Loss: 8.363821507373359e-06, LR: 8.000000000000001e-06\n",
      "Step 290/1000, AUM_macro Loss: 7.806205758242868e-06, LR: 8.000000000000001e-06\n",
      "Step 291/1000, AUM_macro Loss: 8.024208000279032e-06, LR: 8.000000000000001e-06\n",
      "Step 292/1000, AUM_macro Loss: 8.255629836639855e-06, LR: 8.000000000000001e-06\n",
      "Step 293/1000, AUM_macro Loss: 7.952909982122947e-06, LR: 8.000000000000001e-06\n",
      "Step 294/1000, AUM_macro Loss: 7.928603736218065e-06, LR: 8.000000000000001e-06\n",
      "Step 295/1000, AUM_macro Loss: 8.159881872416008e-06, LR: 8.000000000000001e-06\n",
      "Step 296/1000, AUM_macro Loss: 8.127303772198502e-06, LR: 8.000000000000001e-06\n",
      "Step 297/1000, AUM_macro Loss: 8.088021786534227e-06, LR: 8.000000000000001e-06\n",
      "Step 298/1000, AUM_macro Loss: 8.048708878050093e-06, LR: 8.000000000000001e-06\n",
      "Step 299/1000, AUM_macro Loss: 8.288633580377791e-06, LR: 8.000000000000001e-06\n",
      "Step 300/1000, AUM_macro Loss: 8.544685442757327e-06, LR: 8.000000000000001e-06\n",
      "Step 301/1000, AUM_macro Loss: 8.210249689000193e-06, LR: 8.000000000000001e-06\n",
      "Step 302/1000, AUM_macro Loss: 8.173834430635907e-06, LR: 8.000000000000001e-06\n",
      "Step 303/1000, AUM_macro Loss: 8.135938514897134e-06, LR: 8.000000000000001e-06\n",
      "Step 304/1000, AUM_macro Loss: 8.094089935184456e-06, LR: 8.000000000000001e-06\n",
      "Step 305/1000, AUM_macro Loss: 8.347374205186497e-06, LR: 8.000000000000001e-06\n",
      "Step 306/1000, AUM_macro Loss: 8.62450815475313e-06, LR: 5e-06\n",
      "Step 307/1000, AUM_macro Loss: 8.288217941299081e-06, LR: 5e-06\n",
      "Step 308/1000, AUM_macro Loss: 8.26619452709565e-06, LR: 5e-06\n",
      "Step 309/1000, AUM_macro Loss: 8.245934623118956e-06, LR: 5e-06\n",
      "Step 310/1000, AUM_macro Loss: 8.229980267060455e-06, LR: 5e-06\n",
      "Step 311/1000, AUM_macro Loss: 8.210004125430714e-06, LR: 5e-06\n",
      "Step 312/1000, AUM_macro Loss: 8.49455773277441e-06, LR: 5e-06\n",
      "Step 313/1000, AUM_macro Loss: 8.170541150320787e-06, LR: 5e-06\n",
      "Step 314/1000, AUM_macro Loss: 8.151288056978956e-06, LR: 5e-06\n",
      "Step 315/1000, AUM_macro Loss: 7.851157533877995e-06, LR: 5e-06\n",
      "Step 316/1000, AUM_macro Loss: 8.113306648738217e-06, LR: 5e-06\n",
      "Step 317/1000, AUM_macro Loss: 8.090624760370702e-06, LR: 5e-06\n",
      "Step 318/1000, AUM_macro Loss: 8.069629075180274e-06, LR: 5e-06\n",
      "Step 319/1000, AUM_macro Loss: 8.050914402701892e-06, LR: 5e-06\n",
      "Step 320/1000, AUM_macro Loss: 8.037279258132912e-06, LR: 5e-06\n",
      "Step 321/1000, AUM_macro Loss: 8.022814654395916e-06, LR: 5e-06\n",
      "Step 322/1000, AUM_macro Loss: 8.298531611217186e-06, LR: 5e-06\n",
      "Step 323/1000, AUM_macro Loss: 8.280997462861706e-06, LR: 5e-06\n",
      "Step 324/1000, AUM_macro Loss: 8.25835650175577e-06, LR: 5e-06\n",
      "Step 325/1000, AUM_macro Loss: 7.944429853523616e-06, LR: 5e-06\n",
      "Step 326/1000, AUM_macro Loss: 7.652358362975065e-06, LR: 5e-06\n",
      "Step 327/1000, AUM_macro Loss: 7.632194865436759e-06, LR: 5e-06\n",
      "Step 328/1000, AUM_macro Loss: 7.885705599619541e-06, LR: 5e-06\n",
      "Step 329/1000, AUM_macro Loss: 8.159353455994278e-06, LR: 5e-06\n",
      "Step 330/1000, AUM_macro Loss: 8.14158920547925e-06, LR: 5e-06\n",
      "Step 331/1000, AUM_macro Loss: 8.122609870042652e-06, LR: 5e-06\n",
      "Step 332/1000, AUM_macro Loss: 8.103625987132546e-06, LR: 5e-06\n",
      "Step 333/1000, AUM_macro Loss: 7.796113095537294e-06, LR: 5e-06\n",
      "Step 334/1000, AUM_macro Loss: 7.775759513606317e-06, LR: 5e-06\n",
      "Step 335/1000, AUM_macro Loss: 7.490327334380709e-06, LR: 5e-06\n",
      "Step 336/1000, AUM_macro Loss: 8.026306204556022e-06, LR: 5e-06\n",
      "Step 337/1000, AUM_macro Loss: 7.4516042332106736e-06, LR: 5e-06\n",
      "Step 338/1000, AUM_macro Loss: 7.69494454289088e-06, LR: 5e-06\n",
      "Step 339/1000, AUM_macro Loss: 7.41382564228843e-06, LR: 5e-06\n",
      "Step 340/1000, AUM_macro Loss: 7.658213689865079e-06, LR: 5e-06\n",
      "Step 341/1000, AUM_macro Loss: 7.643681783520151e-06, LR: 5e-06\n",
      "Step 342/1000, AUM_macro Loss: 7.90972353570396e-06, LR: 5e-06\n",
      "Step 343/1000, AUM_macro Loss: 7.894063855928835e-06, LR: 5e-06\n",
      "Step 344/1000, AUM_macro Loss: 7.873822141846176e-06, LR: 5e-06\n",
      "Step 345/1000, AUM_macro Loss: 7.85886004450731e-06, LR: 5e-06\n",
      "Step 346/1000, AUM_macro Loss: 8.140099453157745e-06, LR: 5e-06\n",
      "Step 347/1000, AUM_macro Loss: 7.53955464460887e-06, LR: 5e-06\n",
      "Step 348/1000, AUM_macro Loss: 8.099703336483799e-06, LR: 5e-06\n",
      "Step 349/1000, AUM_macro Loss: 8.082470230874605e-06, LR: 5e-06\n",
      "Step 350/1000, AUM_macro Loss: 8.064612302405294e-06, LR: 5e-06\n",
      "Step 351/1000, AUM_macro Loss: 7.748974894639105e-06, LR: 5e-06\n",
      "Step 352/1000, AUM_macro Loss: 7.729515345999971e-06, LR: 5e-06\n",
      "Step 353/1000, AUM_macro Loss: 7.435969564539846e-06, LR: 5e-06\n",
      "Step 354/1000, AUM_macro Loss: 7.693168299738318e-06, LR: 5e-06\n",
      "Step 355/1000, AUM_macro Loss: 7.401783022942254e-06, LR: 5e-06\n",
      "Step 356/1000, AUM_macro Loss: 8.269706995633896e-06, LR: 5e-06\n",
      "Step 357/1000, AUM_macro Loss: 8.24578819447197e-06, LR: 5e-06\n",
      "Step 358/1000, AUM_macro Loss: 8.227015314332675e-06, LR: 5e-06\n",
      "Step 359/1000, AUM_macro Loss: 8.207951395888813e-06, LR: 5e-06\n",
      "Step 360/1000, AUM_macro Loss: 7.583440492453519e-06, LR: 5e-06\n",
      "Step 361/1000, AUM_macro Loss: 8.169405191438273e-06, LR: 5e-06\n",
      "Step 362/1000, AUM_macro Loss: 7.841068509151228e-06, LR: 5e-06\n",
      "Step 363/1000, AUM_macro Loss: 7.820951395842712e-06, LR: 5e-06\n",
      "Step 364/1000, AUM_macro Loss: 8.454096132481936e-06, LR: 5e-06\n",
      "Step 365/1000, AUM_macro Loss: 8.434371011389885e-06, LR: 5e-06\n",
      "Step 366/1000, AUM_macro Loss: 8.079631697910372e-06, LR: 5e-06\n",
      "Step 367/1000, AUM_macro Loss: 8.063535460678395e-06, LR: 5e-06\n",
      "Step 368/1000, AUM_macro Loss: 8.042988156375941e-06, LR: 5e-06\n",
      "Step 369/1000, AUM_macro Loss: 8.02660815679701e-06, LR: 5e-06\n",
      "Step 370/1000, AUM_macro Loss: 7.69994130678242e-06, LR: 5e-06\n",
      "Step 371/1000, AUM_macro Loss: 7.989438927324954e-06, LR: 5e-06\n",
      "Step 372/1000, AUM_macro Loss: 7.96854237705702e-06, LR: 5e-06\n",
      "Step 373/1000, AUM_macro Loss: 7.949056453071535e-06, LR: 5e-06\n",
      "Step 374/1000, AUM_macro Loss: 7.933014785521664e-06, LR: 5e-06\n",
      "Step 375/1000, AUM_macro Loss: 7.91424736235058e-06, LR: 5e-06\n",
      "Step 376/1000, AUM_macro Loss: 7.896189345046878e-06, LR: 5e-06\n",
      "Step 377/1000, AUM_macro Loss: 7.877500138420146e-06, LR: 5e-06\n",
      "Step 378/1000, AUM_macro Loss: 7.856881893530954e-06, LR: 5e-06\n",
      "Step 379/1000, AUM_macro Loss: 7.837762495910283e-06, LR: 5e-06\n",
      "Step 380/1000, AUM_macro Loss: 7.819687198207248e-06, LR: 5e-06\n",
      "Step 381/1000, AUM_macro Loss: 7.799355444149114e-06, LR: 5e-06\n",
      "Step 382/1000, AUM_macro Loss: 7.782887223584112e-06, LR: 5e-06\n",
      "Step 383/1000, AUM_macro Loss: 7.764083420624956e-06, LR: 5e-06\n",
      "Step 384/1000, AUM_macro Loss: 7.743434252915904e-06, LR: 5e-06\n",
      "Step 385/1000, AUM_macro Loss: 7.723076123511419e-06, LR: 5e-06\n",
      "Step 386/1000, AUM_macro Loss: 7.702879884163849e-06, LR: 5e-06\n",
      "Step 387/1000, AUM_macro Loss: 7.391197414108319e-06, LR: 5e-06\n",
      "Step 388/1000, AUM_macro Loss: 7.372245363512775e-06, LR: 5e-06\n",
      "Step 389/1000, AUM_macro Loss: 7.082828233251348e-06, LR: 5e-06\n",
      "Step 390/1000, AUM_macro Loss: 7.949198334245011e-06, LR: 5e-06\n",
      "Step 391/1000, AUM_macro Loss: 7.930141691758763e-06, LR: 5e-06\n",
      "Step 392/1000, AUM_macro Loss: 7.909747182566207e-06, LR: 5e-06\n",
      "Step 393/1000, AUM_macro Loss: 7.577278211101657e-06, LR: 5e-06\n",
      "Step 394/1000, AUM_macro Loss: 7.560181074950378e-06, LR: 5e-06\n",
      "Step 395/1000, AUM_macro Loss: 7.5407228905532975e-06, LR: 5e-06\n",
      "Step 396/1000, AUM_macro Loss: 7.231377367133973e-06, LR: 5e-06\n",
      "Step 397/1000, AUM_macro Loss: 7.814865057298448e-06, LR: 5e-06\n",
      "Step 398/1000, AUM_macro Loss: 8.138078555930406e-06, LR: 5e-06\n",
      "Step 399/1000, AUM_macro Loss: 8.118276127788704e-06, LR: 5e-06\n",
      "Step 400/1000, AUM_macro Loss: 7.760557309666183e-06, LR: 5e-06\n",
      "Step 401/1000, AUM_macro Loss: 8.07828655524645e-06, LR: 5e-06\n",
      "Step 402/1000, AUM_macro Loss: 8.060138497967273e-06, LR: 5e-06\n",
      "Step 403/1000, AUM_macro Loss: 8.043927664402872e-06, LR: 5e-06\n",
      "Step 404/1000, AUM_macro Loss: 7.689805897825863e-06, LR: 5e-06\n",
      "Step 405/1000, AUM_macro Loss: 7.673715117562097e-06, LR: 5e-06\n",
      "Step 406/1000, AUM_macro Loss: 7.647008715139236e-06, LR: 5e-06\n",
      "Step 407/1000, AUM_macro Loss: 7.62522904551588e-06, LR: 5e-06\n",
      "Step 408/1000, AUM_macro Loss: 7.60631019147695e-06, LR: 5e-06\n",
      "Step 409/1000, AUM_macro Loss: 7.914742127468344e-06, LR: 5e-06\n",
      "Step 410/1000, AUM_macro Loss: 7.567459306301316e-06, LR: 5e-06\n",
      "Step 411/1000, AUM_macro Loss: 7.87623503128998e-06, LR: 5e-06\n",
      "Step 412/1000, AUM_macro Loss: 7.856057891331147e-06, LR: 5e-06\n",
      "Step 413/1000, AUM_macro Loss: 8.196272574423347e-06, LR: 5e-06\n",
      "Step 414/1000, AUM_macro Loss: 8.173707101377659e-06, LR: 5e-06\n",
      "Step 415/1000, AUM_macro Loss: 8.539112059224863e-06, LR: 5e-06\n",
      "Step 416/1000, AUM_macro Loss: 8.526802048436366e-06, LR: 5e-06\n",
      "Step 417/1000, AUM_macro Loss: 8.510112820658833e-06, LR: 5e-06\n",
      "Step 418/1000, AUM_macro Loss: 8.909875759854913e-06, LR: 5e-06\n",
      "Step 419/1000, AUM_macro Loss: 8.885162969818339e-06, LR: 5e-06\n",
      "Step 420/1000, AUM_macro Loss: 8.86055022419896e-06, LR: 5e-06\n",
      "Step 421/1000, AUM_macro Loss: 8.837797395244706e-06, LR: 5e-06\n",
      "Step 422/1000, AUM_macro Loss: 8.816259651212022e-06, LR: 5e-06\n",
      "Step 423/1000, AUM_macro Loss: 8.789205821813084e-06, LR: 5e-06\n",
      "Step 424/1000, AUM_macro Loss: 8.769916348683182e-06, LR: 5e-06\n",
      "Step 425/1000, AUM_macro Loss: 8.333984624186996e-06, LR: 5e-06\n",
      "Step 426/1000, AUM_macro Loss: 8.729166438570246e-06, LR: 5e-06\n",
      "Step 427/1000, AUM_macro Loss: 8.69998439156916e-06, LR: 5e-06\n",
      "Step 428/1000, AUM_macro Loss: 7.890987944847438e-06, LR: 5e-06\n",
      "Step 429/1000, AUM_macro Loss: 8.243617230618838e-06, LR: 5e-06\n",
      "Step 430/1000, AUM_macro Loss: 8.63240802573273e-06, LR: 5e-06\n",
      "Step 431/1000, AUM_macro Loss: 8.605487892054953e-06, LR: 5e-06\n",
      "Step 432/1000, AUM_macro Loss: 8.584001989220269e-06, LR: 5e-06\n",
      "Step 433/1000, AUM_macro Loss: 8.554480700695422e-06, LR: 5e-06\n",
      "Step 434/1000, AUM_macro Loss: 8.123283805616666e-06, LR: 5e-06\n",
      "Step 435/1000, AUM_macro Loss: 8.105313099804334e-06, LR: 5e-06\n",
      "Step 436/1000, AUM_macro Loss: 8.48990475788014e-06, LR: 5e-06\n",
      "Step 437/1000, AUM_macro Loss: 8.063634595600888e-06, LR: 5e-06\n",
      "Step 438/1000, AUM_macro Loss: 8.44172973302193e-06, LR: 5e-06\n",
      "Step 439/1000, AUM_macro Loss: 8.417078788625076e-06, LR: 5e-06\n",
      "Step 440/1000, AUM_macro Loss: 8.395595614274498e-06, LR: 5e-06\n",
      "Step 441/1000, AUM_macro Loss: 7.972681487444788e-06, LR: 5e-06\n",
      "Step 442/1000, AUM_macro Loss: 7.585306320834206e-06, LR: 5e-06\n",
      "Step 443/1000, AUM_macro Loss: 7.923674274934456e-06, LR: 5e-06\n",
      "Step 444/1000, AUM_macro Loss: 8.297416570712812e-06, LR: 5e-06\n",
      "Step 445/1000, AUM_macro Loss: 8.274932042695582e-06, LR: 5e-06\n",
      "Step 446/1000, AUM_macro Loss: 8.249734491982963e-06, LR: 5e-06\n",
      "Step 447/1000, AUM_macro Loss: 7.833902600395959e-06, LR: 5e-06\n",
      "Step 448/1000, AUM_macro Loss: 8.198991054086946e-06, LR: 5e-06\n",
      "Step 449/1000, AUM_macro Loss: 8.175689799827524e-06, LR: 5e-06\n",
      "Step 450/1000, AUM_macro Loss: 7.759481377433985e-06, LR: 5e-06\n",
      "Step 451/1000, AUM_macro Loss: 7.734017344773747e-06, LR: 5e-06\n",
      "Step 452/1000, AUM_macro Loss: 7.713284503552131e-06, LR: 5e-06\n",
      "Step 453/1000, AUM_macro Loss: 7.6901842476218e-06, LR: 5e-06\n",
      "Step 454/1000, AUM_macro Loss: 7.327338607865386e-06, LR: 5e-06\n",
      "Step 455/1000, AUM_macro Loss: 8.036654435272794e-06, LR: 5e-06\n",
      "Step 456/1000, AUM_macro Loss: 8.012738362594973e-06, LR: 5e-06\n",
      "Step 457/1000, AUM_macro Loss: 7.989793630258646e-06, LR: 5e-06\n",
      "Step 458/1000, AUM_macro Loss: 7.967949386511464e-06, LR: 5e-06\n",
      "Step 459/1000, AUM_macro Loss: 7.946367077238392e-06, LR: 5e-06\n",
      "Step 460/1000, AUM_macro Loss: 7.921256838017143e-06, LR: 5e-06\n",
      "Step 461/1000, AUM_macro Loss: 7.5200555329502095e-06, LR: 5e-06\n",
      "Step 462/1000, AUM_macro Loss: 7.876283234509174e-06, LR: 5e-06\n",
      "Step 463/1000, AUM_macro Loss: 7.85005249781534e-06, LR: 5e-06\n",
      "Step 464/1000, AUM_macro Loss: 7.830816684872843e-06, LR: 5e-06\n",
      "Step 465/1000, AUM_macro Loss: 7.80928985477658e-06, LR: 5e-06\n",
      "Step 466/1000, AUM_macro Loss: 7.788835318933707e-06, LR: 5e-06\n",
      "Step 467/1000, AUM_macro Loss: 7.76473643782083e-06, LR: 5e-06\n",
      "Step 468/1000, AUM_macro Loss: 7.372197615040932e-06, LR: 5e-06\n",
      "Step 469/1000, AUM_macro Loss: 7.721044312347658e-06, LR: 5e-06\n",
      "Step 470/1000, AUM_macro Loss: 7.3347132456547115e-06, LR: 5e-06\n",
      "Step 471/1000, AUM_macro Loss: 7.317923973459983e-06, LR: 5e-06\n",
      "Step 472/1000, AUM_macro Loss: 7.664250915695447e-06, LR: 5e-06\n",
      "Step 473/1000, AUM_macro Loss: 7.281328180397395e-06, LR: 5e-06\n",
      "Step 474/1000, AUM_macro Loss: 7.629498213646002e-06, LR: 5e-06\n",
      "Step 475/1000, AUM_macro Loss: 7.609558906551683e-06, LR: 5e-06\n",
      "Step 476/1000, AUM_macro Loss: 7.2219236244563945e-06, LR: 5e-06\n",
      "Step 477/1000, AUM_macro Loss: 7.560879112133989e-06, LR: 5e-06\n",
      "Step 478/1000, AUM_macro Loss: 7.541917057096725e-06, LR: 5e-06\n",
      "Step 479/1000, AUM_macro Loss: 7.521095540141687e-06, LR: 5e-06\n",
      "Step 480/1000, AUM_macro Loss: 7.500954325223574e-06, LR: 5e-06\n",
      "Step 481/1000, AUM_macro Loss: 7.122472197806928e-06, LR: 5e-06\n",
      "Step 482/1000, AUM_macro Loss: 7.459185326297302e-06, LR: 5e-06\n",
      "Step 483/1000, AUM_macro Loss: 7.085380275384523e-06, LR: 5e-06\n",
      "Step 484/1000, AUM_macro Loss: 7.06706487108022e-06, LR: 5e-06\n",
      "Step 485/1000, AUM_macro Loss: 7.402296432701405e-06, LR: 5e-06\n",
      "Step 486/1000, AUM_macro Loss: 7.377335805358598e-06, LR: 5e-06\n",
      "Step 487/1000, AUM_macro Loss: 7.357951290032361e-06, LR: 5e-06\n",
      "Step 488/1000, AUM_macro Loss: 7.338369414355839e-06, LR: 5e-06\n",
      "Step 489/1000, AUM_macro Loss: 7.3183596214221325e-06, LR: 5e-06\n",
      "Step 490/1000, AUM_macro Loss: 7.299433491425589e-06, LR: 5e-06\n",
      "Step 491/1000, AUM_macro Loss: 7.279707006091485e-06, LR: 5e-06\n",
      "Step 492/1000, AUM_macro Loss: 6.916801794432104e-06, LR: 5e-06\n",
      "Step 493/1000, AUM_macro Loss: 6.903510893607745e-06, LR: 5e-06\n",
      "Step 494/1000, AUM_macro Loss: 7.231601102830609e-06, LR: 5e-06\n",
      "Step 495/1000, AUM_macro Loss: 6.870301604067208e-06, LR: 5e-06\n",
      "Step 496/1000, AUM_macro Loss: 6.848787052149419e-06, LR: 5e-06\n",
      "Step 497/1000, AUM_macro Loss: 7.170091521402355e-06, LR: 5e-06\n",
      "Step 498/1000, AUM_macro Loss: 7.150093551899772e-06, LR: 5e-06\n",
      "Step 499/1000, AUM_macro Loss: 7.5022198871010914e-06, LR: 5e-06\n",
      "Step 500/1000, AUM_macro Loss: 7.480871772713726e-06, LR: 5e-06\n",
      "Step 501/1000, AUM_macro Loss: 7.461223503923975e-06, LR: 5e-06\n",
      "Step 502/1000, AUM_macro Loss: 7.442459718731698e-06, LR: 5e-06\n",
      "Step 503/1000, AUM_macro Loss: 7.0482610681210645e-06, LR: 5e-06\n",
      "Step 504/1000, AUM_macro Loss: 7.400713457172969e-06, LR: 5e-06\n",
      "Step 505/1000, AUM_macro Loss: 7.377223028015578e-06, LR: 5e-06\n",
      "Step 506/1000, AUM_macro Loss: 7.357370122917928e-06, LR: 5e-06\n",
      "Step 507/1000, AUM_macro Loss: 6.968561592657352e-06, LR: 5e-06\n",
      "Step 508/1000, AUM_macro Loss: 7.3115984378091525e-06, LR: 5e-06\n",
      "Step 509/1000, AUM_macro Loss: 6.926602509338409e-06, LR: 5e-06\n",
      "Step 510/1000, AUM_macro Loss: 6.908326668053633e-06, LR: 5e-06\n",
      "Step 511/1000, AUM_macro Loss: 7.244880634971196e-06, LR: 5e-06\n",
      "Step 512/1000, AUM_macro Loss: 7.221597115858458e-06, LR: 5e-06\n",
      "Step 513/1000, AUM_macro Loss: 6.844623385404702e-06, LR: 5e-06\n",
      "Step 514/1000, AUM_macro Loss: 7.1881545409269165e-06, LR: 5e-06\n",
      "Step 515/1000, AUM_macro Loss: 6.806933470215881e-06, LR: 5e-06\n",
      "Step 516/1000, AUM_macro Loss: 6.784277502447367e-06, LR: 5e-06\n",
      "Step 517/1000, AUM_macro Loss: 6.435960585804423e-06, LR: 5e-06\n",
      "Step 518/1000, AUM_macro Loss: 6.420135832740925e-06, LR: 5e-06\n",
      "Step 519/1000, AUM_macro Loss: 6.720517376379576e-06, LR: 5e-06\n",
      "Step 520/1000, AUM_macro Loss: 6.091860996093601e-06, LR: 5e-06\n",
      "Step 521/1000, AUM_macro Loss: 6.683806532237213e-06, LR: 5e-06\n",
      "Step 522/1000, AUM_macro Loss: 7.0085238803585526e-06, LR: 5e-06\n",
      "Step 523/1000, AUM_macro Loss: 6.637814749410609e-06, LR: 5e-06\n",
      "Step 524/1000, AUM_macro Loss: 6.6201923800690565e-06, LR: 5e-06\n",
      "Step 525/1000, AUM_macro Loss: 6.947500423848396e-06, LR: 5e-06\n",
      "Step 526/1000, AUM_macro Loss: 6.925243269506609e-06, LR: 5e-06\n",
      "Step 527/1000, AUM_macro Loss: 6.9041057031427044e-06, LR: 5e-06\n",
      "Step 528/1000, AUM_macro Loss: 6.8843310145894065e-06, LR: 5e-06\n",
      "Step 529/1000, AUM_macro Loss: 6.5177205215150025e-06, LR: 5e-06\n",
      "Step 530/1000, AUM_macro Loss: 6.497682534245541e-06, LR: 5e-06\n",
      "Step 531/1000, AUM_macro Loss: 6.8170261329214554e-06, LR: 5e-06\n",
      "Step 532/1000, AUM_macro Loss: 6.795566605433123e-06, LR: 5e-06\n",
      "Step 533/1000, AUM_macro Loss: 6.435192517528776e-06, LR: 5e-06\n",
      "Step 534/1000, AUM_macro Loss: 6.1082605498086195e-06, LR: 5e-06\n",
      "Step 535/1000, AUM_macro Loss: 6.391911483660806e-06, LR: 5e-06\n",
      "Step 536/1000, AUM_macro Loss: 6.708426099066855e-06, LR: 5e-06\n",
      "Step 537/1000, AUM_macro Loss: 6.049978310329607e-06, LR: 5e-06\n",
      "Step 538/1000, AUM_macro Loss: 6.6606503423827235e-06, LR: 5e-06\n",
      "Step 539/1000, AUM_macro Loss: 6.6368465923005715e-06, LR: 5e-06\n",
      "Step 540/1000, AUM_macro Loss: 6.61194508211338e-06, LR: 5e-06\n",
      "Step 541/1000, AUM_macro Loss: 5.965227956039598e-06, LR: 5e-06\n",
      "Step 542/1000, AUM_macro Loss: 6.240778930077795e-06, LR: 5e-06\n",
      "Step 543/1000, AUM_macro Loss: 5.652951585943811e-06, LR: 5e-06\n",
      "Step 544/1000, AUM_macro Loss: 6.194826710270718e-06, LR: 5e-06\n",
      "Step 545/1000, AUM_macro Loss: 6.494471563200932e-06, LR: 5e-06\n",
      "Step 546/1000, AUM_macro Loss: 6.142835445643868e-06, LR: 5e-06\n",
      "Step 547/1000, AUM_macro Loss: 6.443327038141433e-06, LR: 5e-06\n",
      "Step 548/1000, AUM_macro Loss: 6.416436463041464e-06, LR: 5e-06\n",
      "Step 549/1000, AUM_macro Loss: 6.3908960328262765e-06, LR: 5e-06\n",
      "Step 550/1000, AUM_macro Loss: 6.36575123280636e-06, LR: 5e-06\n",
      "Step 551/1000, AUM_macro Loss: 6.342482265608851e-06, LR: 5e-06\n",
      "Step 552/1000, AUM_macro Loss: 6.317999577731825e-06, LR: 5e-06\n",
      "Step 553/1000, AUM_macro Loss: 6.293345904850867e-06, LR: 5e-06\n",
      "Step 554/1000, AUM_macro Loss: 5.9574126680672634e-06, LR: 5e-06\n",
      "Step 555/1000, AUM_macro Loss: 6.242593372007832e-06, LR: 5e-06\n",
      "Step 556/1000, AUM_macro Loss: 5.90917943554814e-06, LR: 5e-06\n",
      "Step 557/1000, AUM_macro Loss: 6.192105502123013e-06, LR: 5e-06\n",
      "Step 558/1000, AUM_macro Loss: 6.895227215863997e-06, LR: 5e-06\n",
      "Step 559/1000, AUM_macro Loss: 6.487698556156829e-06, LR: 5e-06\n",
      "Step 560/1000, AUM_macro Loss: 5.814120868308237e-06, LR: 5e-06\n",
      "Step 561/1000, AUM_macro Loss: 6.814301741542295e-06, LR: 5e-06\n",
      "Step 562/1000, AUM_macro Loss: 6.7880955612054095e-06, LR: 5e-06\n",
      "Step 563/1000, AUM_macro Loss: 6.381304046954028e-06, LR: 5e-06\n",
      "Step 564/1000, AUM_macro Loss: 6.358294285746524e-06, LR: 5e-06\n",
      "Step 565/1000, AUM_macro Loss: 6.33340187050635e-06, LR: 5e-06\n",
      "Step 566/1000, AUM_macro Loss: 6.681435479549691e-06, LR: 5e-06\n",
      "Step 567/1000, AUM_macro Loss: 6.2857802731741685e-06, LR: 5e-06\n",
      "Step 568/1000, AUM_macro Loss: 6.6293714553467e-06, LR: 5e-06\n",
      "Step 569/1000, AUM_macro Loss: 5.914197117817821e-06, LR: 5e-06\n",
      "Step 570/1000, AUM_macro Loss: 6.212888820300577e-06, LR: 5e-06\n",
      "Step 571/1000, AUM_macro Loss: 5.8611803979147226e-06, LR: 5e-06\n",
      "Step 572/1000, AUM_macro Loss: 5.840252470079577e-06, LR: 5e-06\n",
      "Step 573/1000, AUM_macro Loss: 5.817789315187838e-06, LR: 5e-06\n",
      "Step 574/1000, AUM_macro Loss: 6.477613169408869e-06, LR: 5e-06\n",
      "Step 575/1000, AUM_macro Loss: 6.454971298808232e-06, LR: 5e-06\n",
      "Step 576/1000, AUM_macro Loss: 6.428448159567779e-06, LR: 5e-06\n",
      "Step 577/1000, AUM_macro Loss: 6.407420187315438e-06, LR: 5e-06\n",
      "Step 578/1000, AUM_macro Loss: 6.3858210523903836e-06, LR: 5e-06\n",
      "Step 579/1000, AUM_macro Loss: 6.007364845572738e-06, LR: 5e-06\n",
      "Step 580/1000, AUM_macro Loss: 5.983675691823009e-06, LR: 5e-06\n",
      "Step 581/1000, AUM_macro Loss: 5.965765922155697e-06, LR: 5e-06\n",
      "Step 582/1000, AUM_macro Loss: 6.295815637713531e-06, LR: 5e-06\n",
      "Step 583/1000, AUM_macro Loss: 5.6156145546992775e-06, LR: 5e-06\n",
      "Step 584/1000, AUM_macro Loss: 6.250729256862542e-06, LR: 5e-06\n",
      "Step 585/1000, AUM_macro Loss: 5.5753812375769485e-06, LR: 5e-06\n",
      "Step 586/1000, AUM_macro Loss: 5.866044830327155e-06, LR: 5e-06\n",
      "Step 587/1000, AUM_macro Loss: 6.190267868078081e-06, LR: 5e-06\n",
      "Step 588/1000, AUM_macro Loss: 5.517546469491208e-06, LR: 5e-06\n",
      "Step 589/1000, AUM_macro Loss: 6.1545779317384586e-06, LR: 5e-06\n",
      "Step 590/1000, AUM_macro Loss: 5.788478574686451e-06, LR: 5e-06\n",
      "Step 591/1000, AUM_macro Loss: 5.4798624660179485e-06, LR: 5e-06\n",
      "Step 592/1000, AUM_macro Loss: 5.753128789365292e-06, LR: 5e-06\n",
      "Step 593/1000, AUM_macro Loss: 5.723831691284431e-06, LR: 5e-06\n",
      "Step 594/1000, AUM_macro Loss: 6.044317615305772e-06, LR: 5e-06\n",
      "Step 595/1000, AUM_macro Loss: 5.384865289670415e-06, LR: 5e-06\n",
      "Step 596/1000, AUM_macro Loss: 5.6647450037417e-06, LR: 5e-06\n",
      "Step 597/1000, AUM_macro Loss: 5.646404588333098e-06, LR: 5e-06\n",
      "Step 598/1000, AUM_macro Loss: 5.627361588267377e-06, LR: 5e-06\n",
      "Step 599/1000, AUM_macro Loss: 5.93528056924697e-06, LR: 5e-06\n",
      "Step 600/1000, AUM_macro Loss: 5.290266017254908e-06, LR: 5e-06\n",
      "Step 601/1000, AUM_macro Loss: 5.569555014517391e-06, LR: 5e-06\n",
      "Step 602/1000, AUM_macro Loss: 5.549964043893851e-06, LR: 5e-06\n",
      "Step 603/1000, AUM_macro Loss: 5.855575636815047e-06, LR: 5e-06\n",
      "Step 604/1000, AUM_macro Loss: 5.836966465722071e-06, LR: 5e-06\n",
      "Step 605/1000, AUM_macro Loss: 5.493922344612656e-06, LR: 5e-06\n",
      "Step 606/1000, AUM_macro Loss: 5.4779438869445585e-06, LR: 5e-06\n",
      "Step 607/1000, AUM_macro Loss: 5.173604222363792e-06, LR: 5e-06\n",
      "Step 608/1000, AUM_macro Loss: 5.441730536404066e-06, LR: 5e-06\n",
      "Step 609/1000, AUM_macro Loss: 5.4244969760475215e-06, LR: 5e-06\n",
      "Step 610/1000, AUM_macro Loss: 5.404207968240371e-06, LR: 5e-06\n",
      "Step 611/1000, AUM_macro Loss: 5.38924359716475e-06, LR: 5e-06\n",
      "Step 612/1000, AUM_macro Loss: 4.602050466928631e-06, LR: 5e-06\n",
      "Step 613/1000, AUM_macro Loss: 4.817361514142249e-06, LR: 5e-06\n",
      "Step 614/1000, AUM_macro Loss: 5.049384981248295e-06, LR: 5e-06\n",
      "Step 615/1000, AUM_macro Loss: 5.033047727920348e-06, LR: 5e-06\n",
      "Step 616/1000, AUM_macro Loss: 5.960015187156387e-06, LR: 5e-06\n",
      "Step 617/1000, AUM_macro Loss: 5.592969955614535e-06, LR: 5e-06\n",
      "Step 618/1000, AUM_macro Loss: 5.921524007135304e-06, LR: 5e-06\n",
      "Step 619/1000, AUM_macro Loss: 5.245410648058169e-06, LR: 5e-06\n",
      "Step 620/1000, AUM_macro Loss: 5.881219749426236e-06, LR: 5e-06\n",
      "Step 621/1000, AUM_macro Loss: 5.51871198695153e-06, LR: 5e-06\n",
      "Step 622/1000, AUM_macro Loss: 5.844899988005636e-06, LR: 5e-06\n",
      "Step 623/1000, AUM_macro Loss: 5.825718289997894e-06, LR: 5e-06\n",
      "Step 624/1000, AUM_macro Loss: 5.464481091621565e-06, LR: 5e-06\n",
      "Step 625/1000, AUM_macro Loss: 5.7874344747688156e-06, LR: 5e-06\n",
      "Step 626/1000, AUM_macro Loss: 5.4285223995975684e-06, LR: 5e-06\n",
      "Step 627/1000, AUM_macro Loss: 5.742856046708766e-06, LR: 5e-06\n",
      "Step 628/1000, AUM_macro Loss: 5.090490958536975e-06, LR: 5e-06\n",
      "Step 629/1000, AUM_macro Loss: 5.370289727579802e-06, LR: 5e-06\n",
      "Step 630/1000, AUM_macro Loss: 5.352157131710555e-06, LR: 5e-06\n",
      "Step 631/1000, AUM_macro Loss: 5.334462002792861e-06, LR: 5e-06\n",
      "Step 632/1000, AUM_macro Loss: 5.315362159308279e-06, LR: 5e-06\n",
      "Step 633/1000, AUM_macro Loss: 5.294363290886395e-06, LR: 5e-06\n",
      "Step 634/1000, AUM_macro Loss: 5.607576895272359e-06, LR: 5e-06\n",
      "Step 635/1000, AUM_macro Loss: 5.588485237240093e-06, LR: 5e-06\n",
      "Step 636/1000, AUM_macro Loss: 5.244412932370324e-06, LR: 5e-06\n",
      "Step 637/1000, AUM_macro Loss: 4.943388830724871e-06, LR: 5e-06\n",
      "Step 638/1000, AUM_macro Loss: 4.9222458073927555e-06, LR: 5e-06\n",
      "Step 639/1000, AUM_macro Loss: 5.193769538891502e-06, LR: 5e-06\n",
      "Step 640/1000, AUM_macro Loss: 5.503040483745281e-06, LR: 5e-06\n",
      "Step 641/1000, AUM_macro Loss: 5.481306743604364e-06, LR: 5e-06\n",
      "Step 642/1000, AUM_macro Loss: 5.462085937324446e-06, LR: 5e-06\n",
      "Step 643/1000, AUM_macro Loss: 5.442839210445527e-06, LR: 5e-06\n",
      "Step 644/1000, AUM_macro Loss: 5.108247023599688e-06, LR: 5e-06\n",
      "Step 645/1000, AUM_macro Loss: 5.412751761468826e-06, LR: 5e-06\n",
      "Step 646/1000, AUM_macro Loss: 5.393054379965179e-06, LR: 5e-06\n",
      "Step 647/1000, AUM_macro Loss: 5.3751873565488495e-06, LR: 5e-06\n",
      "Step 648/1000, AUM_macro Loss: 5.359484021028038e-06, LR: 5e-06\n",
      "Step 649/1000, AUM_macro Loss: 5.025638074585004e-06, LR: 5e-06\n",
      "Step 650/1000, AUM_macro Loss: 4.7291773626056965e-06, LR: 5e-06\n",
      "Step 651/1000, AUM_macro Loss: 5.297869392961729e-06, LR: 5e-06\n",
      "Step 652/1000, AUM_macro Loss: 4.970911504642572e-06, LR: 5e-06\n",
      "Step 653/1000, AUM_macro Loss: 5.262174909148598e-06, LR: 5e-06\n",
      "Step 654/1000, AUM_macro Loss: 4.6620602915936615e-06, LR: 5e-06\n",
      "Step 655/1000, AUM_macro Loss: 4.923321739624953e-06, LR: 5e-06\n",
      "Step 656/1000, AUM_macro Loss: 4.390317371871788e-06, LR: 5e-06\n",
      "Step 657/1000, AUM_macro Loss: 4.617254035110818e-06, LR: 5e-06\n",
      "Step 658/1000, AUM_macro Loss: 5.174033503863029e-06, LR: 5e-06\n",
      "Step 659/1000, AUM_macro Loss: 4.852844995184569e-06, LR: 5e-06\n",
      "Step 660/1000, AUM_macro Loss: 4.837945198232774e-06, LR: 5e-06\n",
      "Step 661/1000, AUM_macro Loss: 4.559916305879597e-06, LR: 5e-06\n",
      "Step 662/1000, AUM_macro Loss: 4.809581696463283e-06, LR: 5e-06\n",
      "Step 663/1000, AUM_macro Loss: 5.091507318866206e-06, LR: 5e-06\n",
      "Step 664/1000, AUM_macro Loss: 5.074082309874939e-06, LR: 5e-06\n",
      "Step 665/1000, AUM_macro Loss: 4.495380835578544e-06, LR: 5e-06\n",
      "Step 666/1000, AUM_macro Loss: 5.0410758376528975e-06, LR: 5e-06\n",
      "Step 667/1000, AUM_macro Loss: 5.024027814215515e-06, LR: 5e-06\n",
      "Step 668/1000, AUM_macro Loss: 4.711167093773838e-06, LR: 5e-06\n",
      "Step 669/1000, AUM_macro Loss: 4.69433189209667e-06, LR: 5e-06\n",
      "Step 670/1000, AUM_macro Loss: 4.96899701829534e-06, LR: 5e-06\n",
      "Step 671/1000, AUM_macro Loss: 4.6569216465286445e-06, LR: 5e-06\n",
      "Step 672/1000, AUM_macro Loss: 4.63941114503541e-06, LR: 5e-06\n",
      "Step 673/1000, AUM_macro Loss: 4.6259842747531366e-06, LR: 5e-06\n",
      "Step 674/1000, AUM_macro Loss: 4.8961992433760315e-06, LR: 5e-06\n",
      "Step 675/1000, AUM_macro Loss: 4.5901047087681945e-06, LR: 5e-06\n",
      "Step 676/1000, AUM_macro Loss: 4.32161868957337e-06, LR: 5e-06\n",
      "Step 677/1000, AUM_macro Loss: 4.557752617984079e-06, LR: 5e-06\n",
      "Step 678/1000, AUM_macro Loss: 4.5414121814246755e-06, LR: 5e-06\n",
      "Step 679/1000, AUM_macro Loss: 4.2757715164043475e-06, LR: 5e-06\n",
      "Step 680/1000, AUM_macro Loss: 4.510572580329608e-06, LR: 5e-06\n",
      "Step 681/1000, AUM_macro Loss: 5.092314950161381e-06, LR: 5e-06\n",
      "Step 682/1000, AUM_macro Loss: 4.757974238600582e-06, LR: 5e-06\n",
      "Step 683/1000, AUM_macro Loss: 4.7416142479050905e-06, LR: 5e-06\n",
      "Step 684/1000, AUM_macro Loss: 4.723856363852974e-06, LR: 5e-06\n",
      "Step 685/1000, AUM_macro Loss: 4.708646883955225e-06, LR: 5e-06\n",
      "Step 686/1000, AUM_macro Loss: 5.005973889637971e-06, LR: 5e-06\n",
      "Step 687/1000, AUM_macro Loss: 4.672948762163287e-06, LR: 5e-06\n",
      "Step 688/1000, AUM_macro Loss: 4.96558050144813e-06, LR: 5e-06\n",
      "Step 689/1000, AUM_macro Loss: 4.636785888578743e-06, LR: 5e-06\n",
      "Step 690/1000, AUM_macro Loss: 4.110356712772045e-06, LR: 5e-06\n",
      "Step 691/1000, AUM_macro Loss: 4.334509867476299e-06, LR: 5e-06\n",
      "Step 692/1000, AUM_macro Loss: 4.315085334383184e-06, LR: 5e-06\n",
      "Step 693/1000, AUM_macro Loss: 4.296648512536194e-06, LR: 5e-06\n",
      "Step 694/1000, AUM_macro Loss: 4.8508791223866865e-06, LR: 5e-06\n",
      "Step 695/1000, AUM_macro Loss: 4.832536887988681e-06, LR: 5e-06\n",
      "Step 696/1000, AUM_macro Loss: 5.159744432603475e-06, LR: 5e-06\n",
      "Step 697/1000, AUM_macro Loss: 5.139683253219118e-06, LR: 5e-06\n",
      "Step 698/1000, AUM_macro Loss: 4.217445166432299e-06, LR: 5e-06\n",
      "Step 699/1000, AUM_macro Loss: 5.100709131511394e-06, LR: 5e-06\n",
      "Step 700/1000, AUM_macro Loss: 5.082411462353775e-06, LR: 5e-06\n",
      "Step 701/1000, AUM_macro Loss: 5.060857347416459e-06, LR: 5e-06\n",
      "Step 702/1000, AUM_macro Loss: 5.4332685976987705e-06, LR: 5e-06\n",
      "Step 703/1000, AUM_macro Loss: 5.0268208724446595e-06, LR: 5e-06\n",
      "Step 704/1000, AUM_macro Loss: 4.673758212447865e-06, LR: 5e-06\n",
      "Step 705/1000, AUM_macro Loss: 4.988837190467166e-06, LR: 5e-06\n",
      "Step 706/1000, AUM_macro Loss: 4.6396785364777315e-06, LR: 5e-06\n",
      "Step 707/1000, AUM_macro Loss: 5.3323142310546245e-06, LR: 5e-06\n",
      "Step 708/1000, AUM_macro Loss: 5.314003828971181e-06, LR: 5e-06\n",
      "Step 709/1000, AUM_macro Loss: 4.9113195927930064e-06, LR: 5e-06\n",
      "Step 710/1000, AUM_macro Loss: 4.889713181910338e-06, LR: 5e-06\n",
      "Step 711/1000, AUM_macro Loss: 4.872689260082552e-06, LR: 5e-06\n",
      "Step 712/1000, AUM_macro Loss: 4.2436768126208335e-06, LR: 5e-06\n",
      "Step 713/1000, AUM_macro Loss: 5.20047842655913e-06, LR: 5e-06\n",
      "Step 714/1000, AUM_macro Loss: 4.210856786812656e-06, LR: 5e-06\n",
      "Step 715/1000, AUM_macro Loss: 4.47024240202154e-06, LR: 5e-06\n",
      "Step 716/1000, AUM_macro Loss: 4.769076895172475e-06, LR: 5e-06\n",
      "Step 717/1000, AUM_macro Loss: 5.114758550917031e-06, LR: 5e-06\n",
      "Step 718/1000, AUM_macro Loss: 5.08469656779198e-06, LR: 5e-06\n",
      "Step 719/1000, AUM_macro Loss: 5.062312538939295e-06, LR: 5e-06\n",
      "Step 720/1000, AUM_macro Loss: 4.099310899619013e-06, LR: 5e-06\n",
      "Step 721/1000, AUM_macro Loss: 4.076613549841568e-06, LR: 5e-06\n",
      "Step 722/1000, AUM_macro Loss: 4.330075171310455e-06, LR: 5e-06\n",
      "Step 723/1000, AUM_macro Loss: 4.621724656317383e-06, LR: 5e-06\n",
      "Step 724/1000, AUM_macro Loss: 4.293049187253928e-06, LR: 5e-06\n",
      "Step 725/1000, AUM_macro Loss: 4.5777082959830295e-06, LR: 5e-06\n",
      "Step 726/1000, AUM_macro Loss: 4.253262432030169e-06, LR: 5e-06\n",
      "Step 727/1000, AUM_macro Loss: 4.886532224190887e-06, LR: 5e-06\n",
      "Step 728/1000, AUM_macro Loss: 4.513453404797474e-06, LR: 5e-06\n",
      "Step 729/1000, AUM_macro Loss: 4.1916559894161765e-06, LR: 5e-06\n",
      "Step 730/1000, AUM_macro Loss: 4.813533905689837e-06, LR: 5e-06\n",
      "Step 731/1000, AUM_macro Loss: 3.891314463544404e-06, LR: 5e-06\n",
      "Step 732/1000, AUM_macro Loss: 4.4261432776693255e-06, LR: 5e-06\n",
      "Step 733/1000, AUM_macro Loss: 4.40477970187203e-06, LR: 5e-06\n",
      "Step 734/1000, AUM_macro Loss: 4.714594524557469e-06, LR: 5e-06\n",
      "Step 735/1000, AUM_macro Loss: 4.688914032158209e-06, LR: 5e-06\n",
      "Step 736/1000, AUM_macro Loss: 4.335173343861243e-06, LR: 5e-06\n",
      "Step 737/1000, AUM_macro Loss: 4.647815785574494e-06, LR: 5e-06\n",
      "Step 738/1000, AUM_macro Loss: 4.624824669008376e-06, LR: 5e-06\n",
      "Step 739/1000, AUM_macro Loss: 4.271893430995988e-06, LR: 5e-06\n",
      "Step 740/1000, AUM_macro Loss: 4.252020971762249e-06, LR: 5e-06\n",
      "Step 741/1000, AUM_macro Loss: 3.701608648043475e-06, LR: 5e-06\n",
      "Step 742/1000, AUM_macro Loss: 4.528734734776663e-06, LR: 5e-06\n",
      "Step 743/1000, AUM_macro Loss: 3.904659479303518e-06, LR: 5e-06\n",
      "Step 744/1000, AUM_macro Loss: 3.879756150126923e-06, LR: 5e-06\n",
      "Step 745/1000, AUM_macro Loss: 3.860869128402555e-06, LR: 5e-06\n",
      "Step 746/1000, AUM_macro Loss: 4.4270605030760635e-06, LR: 5e-06\n",
      "Step 747/1000, AUM_macro Loss: 4.406640073284507e-06, LR: 5e-06\n",
      "Step 748/1000, AUM_macro Loss: 3.7924260141153354e-06, LR: 5e-06\n",
      "Step 749/1000, AUM_macro Loss: 4.350328254076885e-06, LR: 5e-06\n",
      "Step 750/1000, AUM_macro Loss: 4.689069555752212e-06, LR: 5e-06\n",
      "Step 751/1000, AUM_macro Loss: 4.0019599509832915e-06, LR: 5e-06\n",
      "Step 752/1000, AUM_macro Loss: 4.28188241130556e-06, LR: 5e-06\n",
      "Step 753/1000, AUM_macro Loss: 3.6801154692511773e-06, LR: 5e-06\n",
      "Step 754/1000, AUM_macro Loss: 4.211906343698502e-06, LR: 5e-06\n",
      "Step 755/1000, AUM_macro Loss: 4.168634404777549e-06, LR: 5e-06\n",
      "Step 756/1000, AUM_macro Loss: 4.102515049453359e-06, LR: 5e-06\n",
      "Step 757/1000, AUM_macro Loss: 3.6310161704022903e-06, LR: 5e-06\n",
      "Step 758/1000, AUM_macro Loss: 3.776909352382063e-06, LR: 5e-06\n",
      "Step 759/1000, AUM_macro Loss: 4.083134172105929e-06, LR: 5e-06\n",
      "Step 760/1000, AUM_macro Loss: 3.248037501180079e-06, LR: 5e-06\n",
      "Step 761/1000, AUM_macro Loss: 3.721751681950991e-06, LR: 5e-06\n",
      "Step 762/1000, AUM_macro Loss: 4.008546966360882e-06, LR: 5e-06\n",
      "Step 763/1000, AUM_macro Loss: 3.989866854681168e-06, LR: 5e-06\n",
      "Step 764/1000, AUM_macro Loss: 3.962661594414385e-06, LR: 5e-06\n",
      "Step 765/1000, AUM_macro Loss: 3.6340536553325364e-06, LR: 5e-06\n",
      "Step 766/1000, AUM_macro Loss: 3.618472874222789e-06, LR: 5e-06\n",
      "Step 767/1000, AUM_macro Loss: 3.890144853357924e-06, LR: 5e-06\n",
      "Step 768/1000, AUM_macro Loss: 3.859731805277988e-06, LR: 5e-06\n",
      "Step 769/1000, AUM_macro Loss: 3.835860297840554e-06, LR: 5e-06\n",
      "Step 770/1000, AUM_macro Loss: 3.5151779229636304e-06, LR: 5e-06\n",
      "Step 771/1000, AUM_macro Loss: 3.0280409646366024e-06, LR: 5e-06\n",
      "Step 772/1000, AUM_macro Loss: 3.4662839425436687e-06, LR: 5e-06\n",
      "Step 773/1000, AUM_macro Loss: 3.4464469536032993e-06, LR: 5e-06\n",
      "Step 774/1000, AUM_macro Loss: 3.4220809084217763e-06, LR: 5e-06\n",
      "Step 775/1000, AUM_macro Loss: 3.395128260308411e-06, LR: 5e-06\n",
      "Step 776/1000, AUM_macro Loss: 3.991476205555955e-06, LR: 5e-06\n",
      "Step 777/1000, AUM_macro Loss: 3.6352284951135516e-06, LR: 5e-06\n",
      "Step 778/1000, AUM_macro Loss: 3.942031980841421e-06, LR: 5e-06\n",
      "Step 779/1000, AUM_macro Loss: 3.3187259305123007e-06, LR: 5e-06\n",
      "Step 780/1000, AUM_macro Loss: 3.893950179190142e-06, LR: 5e-06\n",
      "Step 781/1000, AUM_macro Loss: 3.86576039090869e-06, LR: 5e-06\n",
      "Step 782/1000, AUM_macro Loss: 3.2522516448807437e-06, LR: 5e-06\n",
      "Step 783/1000, AUM_macro Loss: 3.816538537648739e-06, LR: 5e-06\n",
      "Step 784/1000, AUM_macro Loss: 3.7970060020597884e-06, LR: 5e-06\n",
      "Step 785/1000, AUM_macro Loss: 3.773148137042881e-06, LR: 5e-06\n",
      "Step 786/1000, AUM_macro Loss: 3.43791884915845e-06, LR: 5e-06\n",
      "Step 787/1000, AUM_macro Loss: 3.7215313568594866e-06, LR: 5e-06\n",
      "Step 788/1000, AUM_macro Loss: 3.3881581202876987e-06, LR: 5e-06\n",
      "Step 789/1000, AUM_macro Loss: 3.6703108889923897e-06, LR: 5e-06\n",
      "Step 790/1000, AUM_macro Loss: 3.342478521517478e-06, LR: 5e-06\n",
      "Step 791/1000, AUM_macro Loss: 3.0634234917670256e-06, LR: 5e-06\n",
      "Step 792/1000, AUM_macro Loss: 3.299025820524548e-06, LR: 5e-06\n",
      "Step 793/1000, AUM_macro Loss: 3.566701934687444e-06, LR: 5e-06\n",
      "Step 794/1000, AUM_macro Loss: 3.5457881040201755e-06, LR: 5e-06\n",
      "Step 795/1000, AUM_macro Loss: 3.5242346712038852e-06, LR: 5e-06\n",
      "Step 796/1000, AUM_macro Loss: 2.961812242574524e-06, LR: 5e-06\n",
      "Step 797/1000, AUM_macro Loss: 3.1863066851656185e-06, LR: 5e-06\n",
      "Step 798/1000, AUM_macro Loss: 3.453381395956967e-06, LR: 5e-06\n",
      "Step 799/1000, AUM_macro Loss: 3.4295435398234986e-06, LR: 5e-06\n",
      "Step 800/1000, AUM_macro Loss: 2.8800398013117956e-06, LR: 5e-06\n",
      "Step 801/1000, AUM_macro Loss: 3.3838966828625416e-06, LR: 5e-06\n",
      "Step 802/1000, AUM_macro Loss: 3.3609437650738982e-06, LR: 5e-06\n",
      "Step 803/1000, AUM_macro Loss: 3.3419416922697565e-06, LR: 5e-06\n",
      "Step 804/1000, AUM_macro Loss: 3.044127652174211e-06, LR: 5e-06\n",
      "Step 805/1000, AUM_macro Loss: 3.018148845512769e-06, LR: 5e-06\n",
      "Step 806/1000, AUM_macro Loss: 3.595986072468804e-06, LR: 5e-06\n",
      "Step 807/1000, AUM_macro Loss: 3.249480641898117e-06, LR: 5e-06\n",
      "Step 808/1000, AUM_macro Loss: 2.955613808808266e-06, LR: 5e-06\n",
      "Step 809/1000, AUM_macro Loss: 3.208535872545326e-06, LR: 5e-06\n",
      "Step 810/1000, AUM_macro Loss: 3.505160975691979e-06, LR: 5e-06\n",
      "Step 811/1000, AUM_macro Loss: 3.166312126268167e-06, LR: 5e-06\n",
      "Step 812/1000, AUM_macro Loss: 2.8842580377386184e-06, LR: 5e-06\n",
      "Step 813/1000, AUM_macro Loss: 3.429479647820699e-06, LR: 5e-06\n",
      "Step 814/1000, AUM_macro Loss: 3.4081779176631244e-06, LR: 5e-06\n",
      "Step 815/1000, AUM_macro Loss: 2.816145979522844e-06, LR: 5e-06\n",
      "Step 816/1000, AUM_macro Loss: 3.0709716156707145e-06, LR: 5e-06\n",
      "Step 817/1000, AUM_macro Loss: 3.34216156261391e-06, LR: 5e-06\n",
      "Step 818/1000, AUM_macro Loss: 3.3167020774271805e-06, LR: 5e-06\n",
      "Step 819/1000, AUM_macro Loss: 3.297378498245962e-06, LR: 5e-06\n",
      "Step 820/1000, AUM_macro Loss: 2.977155190819758e-06, LR: 5e-06\n",
      "Step 821/1000, AUM_macro Loss: 3.2563893910264596e-06, LR: 5e-06\n",
      "Step 822/1000, AUM_macro Loss: 3.23166341331671e-06, LR: 5e-06\n",
      "Step 823/1000, AUM_macro Loss: 3.213299351045862e-06, LR: 5e-06\n",
      "Step 824/1000, AUM_macro Loss: 2.8961403586436063e-06, LR: 5e-06\n",
      "Step 825/1000, AUM_macro Loss: 2.6351401629653992e-06, LR: 5e-06\n",
      "Step 826/1000, AUM_macro Loss: 2.6197121769655496e-06, LR: 5e-06\n",
      "Step 827/1000, AUM_macro Loss: 2.6072223135997774e-06, LR: 5e-06\n",
      "Step 828/1000, AUM_macro Loss: 2.815378138620872e-06, LR: 5e-06\n",
      "Step 829/1000, AUM_macro Loss: 3.0724136195203755e-06, LR: 5e-06\n",
      "Step 830/1000, AUM_macro Loss: 3.062302766920766e-06, LR: 5e-06\n",
      "Step 831/1000, AUM_macro Loss: 2.749947270785924e-06, LR: 5e-06\n",
      "Step 832/1000, AUM_macro Loss: 3.029981371582835e-06, LR: 5e-06\n",
      "Step 833/1000, AUM_macro Loss: 2.741684511420317e-06, LR: 5e-06\n",
      "Step 834/1000, AUM_macro Loss: 2.9918758173153037e-06, LR: 5e-06\n",
      "Step 835/1000, AUM_macro Loss: 3.279641987319337e-06, LR: 5e-06\n",
      "Step 836/1000, AUM_macro Loss: 3.2405164347437676e-06, LR: 5e-06\n",
      "Step 837/1000, AUM_macro Loss: 2.9275647648319136e-06, LR: 5e-06\n",
      "Step 838/1000, AUM_macro Loss: 2.8954157187399687e-06, LR: 5e-06\n",
      "Step 839/1000, AUM_macro Loss: 2.8852164177806117e-06, LR: 5e-06\n",
      "Step 840/1000, AUM_macro Loss: 3.565828365026391e-06, LR: 5e-06\n",
      "Step 841/1000, AUM_macro Loss: 3.5385789942665724e-06, LR: 5e-06\n",
      "Step 842/1000, AUM_macro Loss: 3.4978097573912237e-06, LR: 5e-06\n",
      "Step 843/1000, AUM_macro Loss: 3.4893746487796307e-06, LR: 5e-06\n",
      "Step 844/1000, AUM_macro Loss: 2.7893067908735247e-06, LR: 5e-06\n",
      "Step 845/1000, AUM_macro Loss: 3.444108642725041e-06, LR: 5e-06\n",
      "Step 846/1000, AUM_macro Loss: 3.0314436116896104e-06, LR: 5e-06\n",
      "Step 847/1000, AUM_macro Loss: 3.3871970117616e-06, LR: 5e-06\n",
      "Step 848/1000, AUM_macro Loss: 2.9948350857011974e-06, LR: 5e-06\n",
      "Step 849/1000, AUM_macro Loss: 3.3494550280011026e-06, LR: 5e-06\n",
      "Step 850/1000, AUM_macro Loss: 3.3297249046881916e-06, LR: 5e-06\n",
      "Step 851/1000, AUM_macro Loss: 3.3023466130543966e-06, LR: 5e-06\n",
      "Step 852/1000, AUM_macro Loss: 2.6262646315444726e-06, LR: 5e-06\n",
      "Step 853/1000, AUM_macro Loss: 2.8859421945526265e-06, LR: 5e-06\n",
      "Step 854/1000, AUM_macro Loss: 3.2227853807853535e-06, LR: 5e-06\n",
      "Step 855/1000, AUM_macro Loss: 3.2060495414043544e-06, LR: 5e-06\n",
      "Step 856/1000, AUM_macro Loss: 3.1735341963212704e-06, LR: 5e-06\n",
      "Step 857/1000, AUM_macro Loss: 3.1406070775119588e-06, LR: 5e-06\n",
      "Step 858/1000, AUM_macro Loss: 3.1178485642158194e-06, LR: 5e-06\n",
      "Step 859/1000, AUM_macro Loss: 3.1012077670311555e-06, LR: 5e-06\n",
      "Step 860/1000, AUM_macro Loss: 3.073843117817887e-06, LR: 5e-06\n",
      "Step 861/1000, AUM_macro Loss: 3.0515170692524407e-06, LR: 5e-06\n",
      "Step 862/1000, AUM_macro Loss: 3.0265507575677475e-06, LR: 5e-06\n",
      "Step 863/1000, AUM_macro Loss: 3.010572982020676e-06, LR: 5e-06\n",
      "Step 864/1000, AUM_macro Loss: 2.6526086003286764e-06, LR: 5e-06\n",
      "Step 865/1000, AUM_macro Loss: 2.3706065803708043e-06, LR: 5e-06\n",
      "Step 866/1000, AUM_macro Loss: 2.621999328766833e-06, LR: 5e-06\n",
      "Step 867/1000, AUM_macro Loss: 2.6060297386720777e-06, LR: 5e-06\n",
      "Step 868/1000, AUM_macro Loss: 2.593527824501507e-06, LR: 5e-06\n",
      "Step 869/1000, AUM_macro Loss: 2.3156858333095443e-06, LR: 5e-06\n",
      "Step 870/1000, AUM_macro Loss: 2.292150838911766e-06, LR: 5e-06\n",
      "Step 871/1000, AUM_macro Loss: 2.5220515453838743e-06, LR: 5e-06\n",
      "Step 872/1000, AUM_macro Loss: 2.5036754323082278e-06, LR: 5e-06\n",
      "Step 873/1000, AUM_macro Loss: 2.792167379084276e-06, LR: 5e-06\n",
      "Step 874/1000, AUM_macro Loss: 2.7716880595107796e-06, LR: 5e-06\n",
      "Step 875/1000, AUM_macro Loss: 2.452043645462254e-06, LR: 5e-06\n",
      "Step 876/1000, AUM_macro Loss: 2.7505159323482076e-06, LR: 5e-06\n",
      "Step 877/1000, AUM_macro Loss: 2.416051302134292e-06, LR: 5e-06\n",
      "Step 878/1000, AUM_macro Loss: 2.398497599642724e-06, LR: 5e-06\n",
      "Step 879/1000, AUM_macro Loss: 2.380160822212929e-06, LR: 5e-06\n",
      "Step 880/1000, AUM_macro Loss: 2.1304458641679958e-06, LR: 5e-06\n",
      "Step 881/1000, AUM_macro Loss: 2.347911276956438e-06, LR: 5e-06\n",
      "Step 882/1000, AUM_macro Loss: 2.6225848159810994e-06, LR: 5e-06\n",
      "Step 883/1000, AUM_macro Loss: 2.3197483187686885e-06, LR: 5e-06\n",
      "Step 884/1000, AUM_macro Loss: 2.0731558834086172e-06, LR: 5e-06\n",
      "Step 885/1000, AUM_macro Loss: 2.2899487248650985e-06, LR: 5e-06\n",
      "Step 886/1000, AUM_macro Loss: 2.561137080192566e-06, LR: 5e-06\n",
      "Step 887/1000, AUM_macro Loss: 2.5393906071258243e-06, LR: 5e-06\n",
      "Step 888/1000, AUM_macro Loss: 2.247257498311228e-06, LR: 5e-06\n",
      "Step 889/1000, AUM_macro Loss: 2.510369085939601e-06, LR: 5e-06\n",
      "Step 890/1000, AUM_macro Loss: 2.2167446331877727e-06, LR: 5e-06\n",
      "Step 891/1000, AUM_macro Loss: 2.4800408482406056e-06, LR: 5e-06\n",
      "Step 892/1000, AUM_macro Loss: 2.1938487861916656e-06, LR: 5e-06\n",
      "Step 893/1000, AUM_macro Loss: 2.449339717713883e-06, LR: 5e-06\n",
      "Step 894/1000, AUM_macro Loss: 2.430348786219838e-06, LR: 5e-06\n",
      "Step 895/1000, AUM_macro Loss: 2.4140040295606013e-06, LR: 5e-06\n",
      "Step 896/1000, AUM_macro Loss: 2.393544264123193e-06, LR: 5e-06\n",
      "Step 897/1000, AUM_macro Loss: 2.381337708357023e-06, LR: 5e-06\n",
      "Step 898/1000, AUM_macro Loss: 1.8956612848342047e-06, LR: 5e-06\n",
      "Step 899/1000, AUM_macro Loss: 2.3515735847468022e-06, LR: 5e-06\n",
      "Step 900/1000, AUM_macro Loss: 2.3321185835811775e-06, LR: 5e-06\n",
      "Step 901/1000, AUM_macro Loss: 2.313592176506063e-06, LR: 5e-06\n",
      "Step 902/1000, AUM_macro Loss: 2.0374882296891883e-06, LR: 5e-06\n",
      "Step 903/1000, AUM_macro Loss: 2.2766480469726957e-06, LR: 5e-06\n",
      "Step 904/1000, AUM_macro Loss: 2.2602662284043618e-06, LR: 5e-06\n",
      "Step 905/1000, AUM_macro Loss: 1.995090542550315e-06, LR: 5e-06\n",
      "Step 906/1000, AUM_macro Loss: 1.979853095690487e-06, LR: 5e-06\n",
      "Step 907/1000, AUM_macro Loss: 1.9627286746981554e-06, LR: 5e-06\n",
      "Step 908/1000, AUM_macro Loss: 1.949264515133109e-06, LR: 5e-06\n",
      "Step 909/1000, AUM_macro Loss: 2.1768837541458197e-06, LR: 5e-06\n",
      "Step 910/1000, AUM_macro Loss: 2.1631553863699082e-06, LR: 5e-06\n",
      "Step 911/1000, AUM_macro Loss: 2.459235247442848e-06, LR: 5e-06\n",
      "Step 912/1000, AUM_macro Loss: 2.4486084839736577e-06, LR: 5e-06\n",
      "Step 913/1000, AUM_macro Loss: 2.138098352588713e-06, LR: 5e-06\n",
      "Step 914/1000, AUM_macro Loss: 2.1280986857163953e-06, LR: 5e-06\n",
      "Step 915/1000, AUM_macro Loss: 2.4165997274394613e-06, LR: 5e-06\n",
      "Step 916/1000, AUM_macro Loss: 2.4082453364826506e-06, LR: 5e-06\n",
      "Step 917/1000, AUM_macro Loss: 2.398584456386743e-06, LR: 5e-06\n",
      "Step 918/1000, AUM_macro Loss: 2.3862176021793857e-06, LR: 5e-06\n",
      "Step 919/1000, AUM_macro Loss: 2.373000143052195e-06, LR: 5e-06\n",
      "Step 920/1000, AUM_macro Loss: 2.067000878014369e-06, LR: 5e-06\n",
      "Step 921/1000, AUM_macro Loss: 2.350648628635099e-06, LR: 5e-06\n",
      "Step 922/1000, AUM_macro Loss: 2.3382692688755924e-06, LR: 5e-06\n",
      "Step 923/1000, AUM_macro Loss: 2.328368282178417e-06, LR: 5e-06\n",
      "Step 924/1000, AUM_macro Loss: 2.314767016287078e-06, LR: 5e-06\n",
      "Step 925/1000, AUM_macro Loss: 2.305969246663153e-06, LR: 5e-06\n",
      "Step 926/1000, AUM_macro Loss: 2.2944516331335763e-06, LR: 5e-06\n",
      "Step 927/1000, AUM_macro Loss: 2.282112518514623e-06, LR: 5e-06\n",
      "Step 928/1000, AUM_macro Loss: 2.266654064442264e-06, LR: 5e-06\n",
      "Step 929/1000, AUM_macro Loss: 2.2585641090699937e-06, LR: 5e-06\n",
      "Step 930/1000, AUM_macro Loss: 2.247440079372609e-06, LR: 5e-06\n",
      "Step 931/1000, AUM_macro Loss: 2.2353208350978093e-06, LR: 5e-06\n",
      "Step 932/1000, AUM_macro Loss: 1.943949428095948e-06, LR: 5e-06\n",
      "Step 933/1000, AUM_macro Loss: 2.20662195715704e-06, LR: 5e-06\n",
      "Step 934/1000, AUM_macro Loss: 2.196343075411278e-06, LR: 5e-06\n",
      "Step 935/1000, AUM_macro Loss: 1.911317212943686e-06, LR: 5e-06\n",
      "Step 936/1000, AUM_macro Loss: 2.1712432953790994e-06, LR: 5e-06\n",
      "Step 937/1000, AUM_macro Loss: 2.159967607440194e-06, LR: 5e-06\n",
      "Step 938/1000, AUM_macro Loss: 2.144478912669001e-06, LR: 5e-06\n",
      "Step 939/1000, AUM_macro Loss: 1.8672847090783762e-06, LR: 5e-06\n",
      "Step 940/1000, AUM_macro Loss: 2.1224052488832967e-06, LR: 5e-06\n",
      "Step 941/1000, AUM_macro Loss: 2.1102339360368205e-06, LR: 5e-06\n",
      "Step 942/1000, AUM_macro Loss: 2.09840163734043e-06, LR: 5e-06\n",
      "Step 943/1000, AUM_macro Loss: 2.0888057861156994e-06, LR: 5e-06\n",
      "Step 944/1000, AUM_macro Loss: 2.4264227249659598e-06, LR: 5e-06\n",
      "Step 945/1000, AUM_macro Loss: 2.4176970327971503e-06, LR: 5e-06\n",
      "Step 946/1000, AUM_macro Loss: 2.0610361843864666e-06, LR: 5e-06\n",
      "Step 947/1000, AUM_macro Loss: 2.3974159830686403e-06, LR: 5e-06\n",
      "Step 948/1000, AUM_macro Loss: 2.390679128438933e-06, LR: 5e-06\n",
      "Step 949/1000, AUM_macro Loss: 2.3814156975277e-06, LR: 5e-06\n",
      "Step 950/1000, AUM_macro Loss: 2.3675693228142336e-06, LR: 5e-06\n",
      "Step 951/1000, AUM_macro Loss: 2.3641362076887162e-06, LR: 5e-06\n",
      "Step 952/1000, AUM_macro Loss: 2.357529183427687e-06, LR: 5e-06\n",
      "Step 953/1000, AUM_macro Loss: 2.350670001760591e-06, LR: 5e-06\n",
      "Step 954/1000, AUM_macro Loss: 2.336853185624932e-06, LR: 5e-06\n",
      "Step 955/1000, AUM_macro Loss: 2.323026592421229e-06, LR: 5e-06\n",
      "Step 956/1000, AUM_macro Loss: 2.3154557311499957e-06, LR: 5e-06\n",
      "Step 957/1000, AUM_macro Loss: 2.3056886675476562e-06, LR: 5e-06\n",
      "Step 958/1000, AUM_macro Loss: 2.2953586267249193e-06, LR: 5e-06\n",
      "Step 959/1000, AUM_macro Loss: 2.284974016220076e-06, LR: 5e-06\n",
      "Step 960/1000, AUM_macro Loss: 2.2758154045732226e-06, LR: 5e-06\n",
      "Step 961/1000, AUM_macro Loss: 2.2666943095828174e-06, LR: 5e-06\n",
      "Step 962/1000, AUM_macro Loss: 1.931952738232212e-06, LR: 5e-06\n",
      "Step 963/1000, AUM_macro Loss: 2.24484733735153e-06, LR: 5e-06\n",
      "Step 964/1000, AUM_macro Loss: 2.2356630324793514e-06, LR: 5e-06\n",
      "Step 965/1000, AUM_macro Loss: 2.223398723799619e-06, LR: 5e-06\n",
      "Step 966/1000, AUM_macro Loss: 2.216845359725994e-06, LR: 5e-06\n",
      "Step 967/1000, AUM_macro Loss: 2.2083570456743473e-06, LR: 5e-06\n",
      "Step 968/1000, AUM_macro Loss: 2.195020215367549e-06, LR: 5e-06\n",
      "Step 969/1000, AUM_macro Loss: 2.1856153580301907e-06, LR: 5e-06\n",
      "Step 970/1000, AUM_macro Loss: 2.17726096707338e-06, LR: 5e-06\n",
      "Step 971/1000, AUM_macro Loss: 2.1657922388840234e-06, LR: 5e-06\n",
      "Step 972/1000, AUM_macro Loss: 2.153462901333114e-06, LR: 5e-06\n",
      "Step 973/1000, AUM_macro Loss: 2.145410007869941e-06, LR: 5e-06\n",
      "Step 974/1000, AUM_macro Loss: 1.833623514357896e-06, LR: 5e-06\n",
      "Step 975/1000, AUM_macro Loss: 2.131006112904288e-06, LR: 5e-06\n",
      "Step 976/1000, AUM_macro Loss: 2.12177087632881e-06, LR: 5e-06\n",
      "Step 977/1000, AUM_macro Loss: 1.8083206896335469e-06, LR: 5e-06\n",
      "Step 978/1000, AUM_macro Loss: 2.106936108248192e-06, LR: 5e-06\n",
      "Step 979/1000, AUM_macro Loss: 2.099766561514116e-06, LR: 5e-06\n",
      "Step 980/1000, AUM_macro Loss: 2.0904683424305404e-06, LR: 5e-06\n",
      "Step 981/1000, AUM_macro Loss: 2.0783695617865305e-06, LR: 5e-06\n",
      "Step 982/1000, AUM_macro Loss: 1.771394749994215e-06, LR: 5e-06\n",
      "Step 983/1000, AUM_macro Loss: 1.767360004123475e-06, LR: 5e-06\n",
      "Step 984/1000, AUM_macro Loss: 2.056388211713056e-06, LR: 5e-06\n",
      "Step 985/1000, AUM_macro Loss: 2.0497550394793507e-06, LR: 5e-06\n",
      "Step 986/1000, AUM_macro Loss: 2.0385630250530085e-06, LR: 5e-06\n",
      "Step 987/1000, AUM_macro Loss: 2.026456286330358e-06, LR: 5e-06\n",
      "Step 988/1000, AUM_macro Loss: 2.0219247289787745e-06, LR: 5e-06\n",
      "Step 989/1000, AUM_macro Loss: 2.019166686295648e-06, LR: 5e-06\n",
      "Step 990/1000, AUM_macro Loss: 2.011844117077999e-06, LR: 5e-06\n",
      "Step 991/1000, AUM_macro Loss: 2.0004724774480565e-06, LR: 5e-06\n",
      "Step 992/1000, AUM_macro Loss: 1.9844312646455364e-06, LR: 5e-06\n",
      "Step 993/1000, AUM_macro Loss: 1.97511462829425e-06, LR: 5e-06\n",
      "Step 994/1000, AUM_macro Loss: 1.9722508568520425e-06, LR: 5e-06\n",
      "Step 995/1000, AUM_macro Loss: 1.9626677385531366e-06, LR: 5e-06\n",
      "Step 996/1000, AUM_macro Loss: 1.9520184650900774e-06, LR: 5e-06\n",
      "Step 997/1000, AUM_macro Loss: 1.9397505184315378e-06, LR: 5e-06\n",
      "Step 998/1000, AUM_macro Loss: 1.933505473061814e-06, LR: 5e-06\n",
      "Step 999/1000, AUM_macro Loss: 1.924291609611828e-06, LR: 5e-06\n",
      "Step 1000/1000, AUM_macro Loss: 1.915958137033158e-06, LR: 5e-06\n",
      "Step 2/1000, Cross-entropy Loss: 10.9995698928833, LR: 0.001\n",
      "Step 3/1000, Cross-entropy Loss: 10.803977966308594, LR: 0.001\n",
      "Step 4/1000, Cross-entropy Loss: 10.609060287475586, LR: 0.001\n",
      "Step 5/1000, Cross-entropy Loss: 10.424201965332031, LR: 0.001\n",
      "Step 6/1000, Cross-entropy Loss: 10.223258018493652, LR: 0.001\n",
      "Step 7/1000, Cross-entropy Loss: 10.017773628234863, LR: 0.001\n",
      "Step 8/1000, Cross-entropy Loss: 9.823755264282227, LR: 0.001\n",
      "Step 9/1000, Cross-entropy Loss: 9.605813026428223, LR: 0.001\n",
      "Step 10/1000, Cross-entropy Loss: 9.398721694946289, LR: 0.001\n",
      "Step 11/1000, Cross-entropy Loss: 9.202635765075684, LR: 0.001\n",
      "Step 12/1000, Cross-entropy Loss: 9.01384449005127, LR: 0.001\n",
      "Step 13/1000, Cross-entropy Loss: 8.81797981262207, LR: 0.001\n",
      "Step 14/1000, Cross-entropy Loss: 8.611133575439453, LR: 0.001\n",
      "Step 15/1000, Cross-entropy Loss: 8.401854515075684, LR: 0.001\n",
      "Step 16/1000, Cross-entropy Loss: 8.19161605834961, LR: 0.001\n",
      "Step 17/1000, Cross-entropy Loss: 7.99329137802124, LR: 0.001\n",
      "Step 18/1000, Cross-entropy Loss: 7.785562992095947, LR: 0.001\n",
      "Step 19/1000, Cross-entropy Loss: 7.575058937072754, LR: 0.001\n",
      "Step 20/1000, Cross-entropy Loss: 7.3681511878967285, LR: 0.001\n",
      "Step 21/1000, Cross-entropy Loss: 7.170346736907959, LR: 0.001\n",
      "Step 22/1000, Cross-entropy Loss: 6.978731632232666, LR: 0.001\n",
      "Step 23/1000, Cross-entropy Loss: 6.793314456939697, LR: 0.001\n",
      "Step 24/1000, Cross-entropy Loss: 6.615664005279541, LR: 0.001\n",
      "Step 25/1000, Cross-entropy Loss: 6.45147705078125, LR: 0.001\n",
      "Step 26/1000, Cross-entropy Loss: 6.299149036407471, LR: 0.001\n",
      "Step 27/1000, Cross-entropy Loss: 6.153326988220215, LR: 0.001\n",
      "Step 28/1000, Cross-entropy Loss: 6.019808769226074, LR: 0.001\n",
      "Step 29/1000, Cross-entropy Loss: 5.899352550506592, LR: 0.001\n",
      "Step 30/1000, Cross-entropy Loss: 5.794150352478027, LR: 0.001\n",
      "Step 31/1000, Cross-entropy Loss: 5.708199977874756, LR: 0.001\n",
      "Step 32/1000, Cross-entropy Loss: 5.63248348236084, LR: 0.001\n",
      "Step 33/1000, Cross-entropy Loss: 5.5745954513549805, LR: 0.001\n",
      "Step 34/1000, Cross-entropy Loss: 5.5103559494018555, LR: 0.001\n",
      "Step 35/1000, Cross-entropy Loss: 5.454782009124756, LR: 0.001\n",
      "Step 36/1000, Cross-entropy Loss: 5.405510425567627, LR: 0.001\n",
      "Step 37/1000, Cross-entropy Loss: 5.370660305023193, LR: 0.001\n",
      "Step 38/1000, Cross-entropy Loss: 5.331592082977295, LR: 0.001\n",
      "Step 39/1000, Cross-entropy Loss: 5.293822288513184, LR: 0.001\n",
      "Step 40/1000, Cross-entropy Loss: 5.258941173553467, LR: 0.001\n",
      "Step 41/1000, Cross-entropy Loss: 5.229718208312988, LR: 0.001\n",
      "Step 42/1000, Cross-entropy Loss: 5.210141181945801, LR: 0.001\n",
      "Step 43/1000, Cross-entropy Loss: 5.181191444396973, LR: 0.001\n",
      "Step 44/1000, Cross-entropy Loss: 5.154892921447754, LR: 0.001\n",
      "Step 45/1000, Cross-entropy Loss: 5.131636142730713, LR: 0.001\n",
      "Step 46/1000, Cross-entropy Loss: 5.102670669555664, LR: 0.001\n",
      "Step 47/1000, Cross-entropy Loss: 5.083608150482178, LR: 0.001\n",
      "Step 48/1000, Cross-entropy Loss: 5.06465482711792, LR: 0.001\n",
      "Step 49/1000, Cross-entropy Loss: 5.0369343757629395, LR: 0.001\n",
      "Step 50/1000, Cross-entropy Loss: 5.02548885345459, LR: 0.001\n",
      "Step 51/1000, Cross-entropy Loss: 4.995419025421143, LR: 0.001\n",
      "Step 52/1000, Cross-entropy Loss: 4.973622798919678, LR: 0.001\n",
      "Step 53/1000, Cross-entropy Loss: 4.952319145202637, LR: 0.001\n",
      "Step 54/1000, Cross-entropy Loss: 4.922579765319824, LR: 0.001\n",
      "Step 55/1000, Cross-entropy Loss: 4.906454086303711, LR: 0.001\n",
      "Step 56/1000, Cross-entropy Loss: 4.897956848144531, LR: 0.001\n",
      "Step 57/1000, Cross-entropy Loss: 4.880625247955322, LR: 0.001\n",
      "Step 58/1000, Cross-entropy Loss: 4.85964822769165, LR: 0.001\n",
      "Step 59/1000, Cross-entropy Loss: 4.844430923461914, LR: 0.001\n",
      "Step 60/1000, Cross-entropy Loss: 4.8257036209106445, LR: 0.001\n",
      "Step 61/1000, Cross-entropy Loss: 4.776329517364502, LR: 0.001\n",
      "Step 62/1000, Cross-entropy Loss: 4.750767707824707, LR: 0.001\n",
      "Step 63/1000, Cross-entropy Loss: 4.717878818511963, LR: 0.001\n",
      "Step 64/1000, Cross-entropy Loss: 4.707207679748535, LR: 0.001\n",
      "Step 65/1000, Cross-entropy Loss: 4.686731815338135, LR: 0.001\n",
      "Step 66/1000, Cross-entropy Loss: 4.682426929473877, LR: 0.001\n",
      "Step 67/1000, Cross-entropy Loss: 4.656743049621582, LR: 0.001\n",
      "Step 68/1000, Cross-entropy Loss: 4.636389255523682, LR: 0.001\n",
      "Step 69/1000, Cross-entropy Loss: 4.603925704956055, LR: 0.001\n",
      "Step 70/1000, Cross-entropy Loss: 4.570136070251465, LR: 0.001\n",
      "Step 71/1000, Cross-entropy Loss: 4.550281524658203, LR: 0.001\n",
      "Step 72/1000, Cross-entropy Loss: 4.516024589538574, LR: 0.001\n",
      "Step 73/1000, Cross-entropy Loss: 4.502819061279297, LR: 0.001\n",
      "Step 74/1000, Cross-entropy Loss: 4.496745586395264, LR: 0.001\n",
      "Step 75/1000, Cross-entropy Loss: 4.474175453186035, LR: 0.001\n",
      "Step 76/1000, Cross-entropy Loss: 4.450671195983887, LR: 0.001\n",
      "Step 77/1000, Cross-entropy Loss: 4.4370527267456055, LR: 0.001\n",
      "Step 78/1000, Cross-entropy Loss: 4.396047115325928, LR: 0.001\n",
      "Step 79/1000, Cross-entropy Loss: 4.359536170959473, LR: 0.001\n",
      "Step 80/1000, Cross-entropy Loss: 4.331000804901123, LR: 0.001\n",
      "Step 81/1000, Cross-entropy Loss: 4.307403564453125, LR: 0.001\n",
      "Step 82/1000, Cross-entropy Loss: 4.300656318664551, LR: 0.001\n",
      "Step 83/1000, Cross-entropy Loss: 4.286312103271484, LR: 0.001\n",
      "Step 84/1000, Cross-entropy Loss: 4.254560947418213, LR: 0.001\n",
      "Step 85/1000, Cross-entropy Loss: 4.235180854797363, LR: 0.001\n",
      "Step 86/1000, Cross-entropy Loss: 4.208568096160889, LR: 0.001\n",
      "Step 87/1000, Cross-entropy Loss: 4.176653861999512, LR: 0.001\n",
      "Step 88/1000, Cross-entropy Loss: 4.17198371887207, LR: 0.001\n",
      "Step 89/1000, Cross-entropy Loss: 4.160408973693848, LR: 0.001\n",
      "Step 90/1000, Cross-entropy Loss: 4.142861366271973, LR: 0.001\n",
      "Step 91/1000, Cross-entropy Loss: 4.112270355224609, LR: 0.001\n",
      "Step 92/1000, Cross-entropy Loss: 4.081522464752197, LR: 0.001\n",
      "Step 93/1000, Cross-entropy Loss: 4.077172756195068, LR: 0.001\n",
      "Step 94/1000, Cross-entropy Loss: 4.057435035705566, LR: 0.001\n",
      "Step 95/1000, Cross-entropy Loss: 4.055794715881348, LR: 0.001\n",
      "Step 96/1000, Cross-entropy Loss: 4.062579154968262, LR: 0.001\n",
      "Step 97/1000, Cross-entropy Loss: 4.048159599304199, LR: 0.001\n",
      "Step 98/1000, Cross-entropy Loss: 4.021729946136475, LR: 0.001\n",
      "Step 99/1000, Cross-entropy Loss: 4.002106666564941, LR: 0.001\n",
      "Step 100/1000, Cross-entropy Loss: 3.99827241897583, LR: 0.001\n",
      "Step 101/1000, Cross-entropy Loss: 3.982837200164795, LR: 0.001\n",
      "Step 102/1000, Cross-entropy Loss: 3.958386182785034, LR: 0.001\n",
      "Step 103/1000, Cross-entropy Loss: 3.9536547660827637, LR: 0.001\n",
      "Step 104/1000, Cross-entropy Loss: 3.939312696456909, LR: 0.001\n",
      "Step 105/1000, Cross-entropy Loss: 3.913240909576416, LR: 0.001\n",
      "Step 106/1000, Cross-entropy Loss: 3.8956828117370605, LR: 0.001\n",
      "Step 107/1000, Cross-entropy Loss: 3.896183729171753, LR: 0.001\n",
      "Step 108/1000, Cross-entropy Loss: 3.872831344604492, LR: 0.001\n",
      "Step 109/1000, Cross-entropy Loss: 3.842984437942505, LR: 0.001\n",
      "Step 110/1000, Cross-entropy Loss: 3.8234410285949707, LR: 0.001\n",
      "Step 111/1000, Cross-entropy Loss: 3.8378360271453857, LR: 0.001\n",
      "Step 112/1000, Cross-entropy Loss: 3.8107407093048096, LR: 0.001\n",
      "Step 113/1000, Cross-entropy Loss: 3.797802686691284, LR: 0.001\n",
      "Step 114/1000, Cross-entropy Loss: 3.770526885986328, LR: 0.001\n",
      "Step 115/1000, Cross-entropy Loss: 3.7447268962860107, LR: 0.001\n",
      "Step 116/1000, Cross-entropy Loss: 3.7178218364715576, LR: 0.001\n",
      "Step 117/1000, Cross-entropy Loss: 3.71624755859375, LR: 0.001\n",
      "Step 118/1000, Cross-entropy Loss: 3.6917736530303955, LR: 0.001\n",
      "Step 119/1000, Cross-entropy Loss: 3.6370513439178467, LR: 0.001\n",
      "Step 120/1000, Cross-entropy Loss: 3.6090145111083984, LR: 0.001\n",
      "Step 121/1000, Cross-entropy Loss: 3.5753703117370605, LR: 0.001\n",
      "Step 122/1000, Cross-entropy Loss: 3.5736985206604004, LR: 0.001\n",
      "Step 123/1000, Cross-entropy Loss: 3.5244803428649902, LR: 0.001\n",
      "Step 124/1000, Cross-entropy Loss: 3.504014492034912, LR: 0.001\n",
      "Step 125/1000, Cross-entropy Loss: 3.478285312652588, LR: 0.001\n",
      "Step 126/1000, Cross-entropy Loss: 3.4499459266662598, LR: 0.001\n",
      "Step 127/1000, Cross-entropy Loss: 3.42039155960083, LR: 0.001\n",
      "Step 128/1000, Cross-entropy Loss: 3.4282329082489014, LR: 0.001\n",
      "Step 129/1000, Cross-entropy Loss: 3.3904545307159424, LR: 0.001\n",
      "Step 130/1000, Cross-entropy Loss: 3.346843719482422, LR: 0.001\n",
      "Step 131/1000, Cross-entropy Loss: 3.3307430744171143, LR: 0.001\n",
      "Step 132/1000, Cross-entropy Loss: 3.2673840522766113, LR: 0.001\n",
      "Step 133/1000, Cross-entropy Loss: 3.2415759563446045, LR: 0.001\n",
      "Step 134/1000, Cross-entropy Loss: 3.2083919048309326, LR: 0.001\n",
      "Step 135/1000, Cross-entropy Loss: 3.2089169025421143, LR: 0.001\n",
      "Step 136/1000, Cross-entropy Loss: 3.16068172454834, LR: 0.001\n",
      "Step 137/1000, Cross-entropy Loss: 3.1342790126800537, LR: 0.001\n",
      "Step 138/1000, Cross-entropy Loss: 3.1020915508270264, LR: 0.001\n",
      "Step 139/1000, Cross-entropy Loss: 3.057661771774292, LR: 0.001\n",
      "Step 140/1000, Cross-entropy Loss: 3.051754951477051, LR: 0.001\n",
      "Step 141/1000, Cross-entropy Loss: 2.9979422092437744, LR: 0.001\n",
      "Step 142/1000, Cross-entropy Loss: 2.9891104698181152, LR: 0.001\n",
      "Step 143/1000, Cross-entropy Loss: 3.021117925643921, LR: 0.001\n",
      "Step 144/1000, Cross-entropy Loss: 2.956803321838379, LR: 0.001\n",
      "Step 145/1000, Cross-entropy Loss: 2.93715238571167, LR: 0.001\n",
      "Step 146/1000, Cross-entropy Loss: 2.877497911453247, LR: 0.001\n",
      "Step 147/1000, Cross-entropy Loss: 2.862412929534912, LR: 0.001\n",
      "Step 148/1000, Cross-entropy Loss: 2.8021888732910156, LR: 0.001\n",
      "Step 149/1000, Cross-entropy Loss: 2.757585048675537, LR: 0.001\n",
      "Step 150/1000, Cross-entropy Loss: 2.739227771759033, LR: 0.001\n",
      "Step 151/1000, Cross-entropy Loss: 2.6814632415771484, LR: 0.001\n",
      "Step 152/1000, Cross-entropy Loss: 2.7107319831848145, LR: 0.001\n",
      "Step 153/1000, Cross-entropy Loss: 2.64253830909729, LR: 0.001\n",
      "Step 154/1000, Cross-entropy Loss: 2.6037395000457764, LR: 0.001\n",
      "Step 155/1000, Cross-entropy Loss: 2.7043230533599854, LR: 0.001\n",
      "Step 156/1000, Cross-entropy Loss: 2.613473892211914, LR: 0.001\n",
      "Step 157/1000, Cross-entropy Loss: 2.5314605236053467, LR: 0.001\n",
      "Step 158/1000, Cross-entropy Loss: 2.6197328567504883, LR: 0.001\n",
      "Step 159/1000, Cross-entropy Loss: 2.4893414974212646, LR: 0.001\n",
      "Step 160/1000, Cross-entropy Loss: 2.4836294651031494, LR: 0.001\n",
      "Step 161/1000, Cross-entropy Loss: 2.621504783630371, LR: 0.001\n",
      "Step 162/1000, Cross-entropy Loss: 2.6134085655212402, LR: 0.001\n",
      "Step 163/1000, Cross-entropy Loss: 2.4964754581451416, LR: 0.001\n",
      "Step 164/1000, Cross-entropy Loss: 2.430244207382202, LR: 0.001\n",
      "Step 165/1000, Cross-entropy Loss: 2.4798107147216797, LR: 0.001\n",
      "Step 166/1000, Cross-entropy Loss: 2.5516574382781982, LR: 0.001\n",
      "Step 167/1000, Cross-entropy Loss: 2.375066041946411, LR: 0.001\n",
      "Step 168/1000, Cross-entropy Loss: 2.4283370971679688, LR: 0.001\n",
      "Step 169/1000, Cross-entropy Loss: 2.5625154972076416, LR: 0.001\n",
      "Step 170/1000, Cross-entropy Loss: 2.3792526721954346, LR: 0.001\n",
      "Step 171/1000, Cross-entropy Loss: 2.3007419109344482, LR: 0.001\n",
      "Step 172/1000, Cross-entropy Loss: 2.2952263355255127, LR: 0.001\n",
      "Step 173/1000, Cross-entropy Loss: 2.318307638168335, LR: 0.001\n",
      "Step 174/1000, Cross-entropy Loss: 2.3186888694763184, LR: 0.001\n",
      "Step 175/1000, Cross-entropy Loss: 2.2335898876190186, LR: 0.001\n",
      "Step 176/1000, Cross-entropy Loss: 2.2030177116394043, LR: 0.001\n",
      "Step 177/1000, Cross-entropy Loss: 2.175043821334839, LR: 0.001\n",
      "Step 178/1000, Cross-entropy Loss: 2.2072994709014893, LR: 0.001\n",
      "Step 179/1000, Cross-entropy Loss: 2.1213271617889404, LR: 0.001\n",
      "Step 180/1000, Cross-entropy Loss: 2.056713581085205, LR: 0.001\n",
      "Step 181/1000, Cross-entropy Loss: 2.0328164100646973, LR: 0.001\n",
      "Step 182/1000, Cross-entropy Loss: 2.0298171043395996, LR: 0.001\n",
      "Step 183/1000, Cross-entropy Loss: 1.9867465496063232, LR: 0.001\n",
      "Step 184/1000, Cross-entropy Loss: 1.9829047918319702, LR: 0.001\n",
      "Step 185/1000, Cross-entropy Loss: 1.9277890920639038, LR: 0.001\n",
      "Step 186/1000, Cross-entropy Loss: 1.8672672510147095, LR: 0.001\n",
      "Step 187/1000, Cross-entropy Loss: 1.828775405883789, LR: 0.001\n",
      "Step 188/1000, Cross-entropy Loss: 1.8324711322784424, LR: 0.001\n",
      "Step 189/1000, Cross-entropy Loss: 1.7884881496429443, LR: 0.001\n",
      "Step 190/1000, Cross-entropy Loss: 1.7278404235839844, LR: 0.001\n",
      "Step 191/1000, Cross-entropy Loss: 1.7044594287872314, LR: 0.001\n",
      "Step 192/1000, Cross-entropy Loss: 1.660996437072754, LR: 0.001\n",
      "Step 193/1000, Cross-entropy Loss: 1.6795313358306885, LR: 0.001\n",
      "Step 194/1000, Cross-entropy Loss: 1.697998046875, LR: 0.001\n",
      "Step 195/1000, Cross-entropy Loss: 1.6274452209472656, LR: 0.001\n",
      "Step 196/1000, Cross-entropy Loss: 1.5916717052459717, LR: 0.001\n",
      "Step 197/1000, Cross-entropy Loss: 1.5472749471664429, LR: 0.001\n",
      "Step 198/1000, Cross-entropy Loss: 1.507001519203186, LR: 0.001\n",
      "Step 199/1000, Cross-entropy Loss: 1.4914121627807617, LR: 0.001\n",
      "Step 200/1000, Cross-entropy Loss: 1.4671881198883057, LR: 0.001\n",
      "Step 201/1000, Cross-entropy Loss: 1.4383028745651245, LR: 0.001\n",
      "Step 202/1000, Cross-entropy Loss: 1.4074122905731201, LR: 0.001\n",
      "Step 203/1000, Cross-entropy Loss: 1.3669975996017456, LR: 0.001\n",
      "Step 204/1000, Cross-entropy Loss: 1.355128288269043, LR: 0.001\n",
      "Step 205/1000, Cross-entropy Loss: 1.3749052286148071, LR: 0.001\n",
      "Step 206/1000, Cross-entropy Loss: 1.3153486251831055, LR: 0.001\n",
      "Step 207/1000, Cross-entropy Loss: 1.2884515523910522, LR: 0.001\n",
      "Step 208/1000, Cross-entropy Loss: 1.2443151473999023, LR: 0.001\n",
      "Step 209/1000, Cross-entropy Loss: 1.2072557210922241, LR: 0.001\n",
      "Step 210/1000, Cross-entropy Loss: 1.2028298377990723, LR: 0.001\n",
      "Step 211/1000, Cross-entropy Loss: 1.1608517169952393, LR: 0.001\n",
      "Step 212/1000, Cross-entropy Loss: 1.1452534198760986, LR: 0.001\n",
      "Step 213/1000, Cross-entropy Loss: 1.117095708847046, LR: 0.001\n",
      "Step 214/1000, Cross-entropy Loss: 1.0842876434326172, LR: 0.001\n",
      "Step 215/1000, Cross-entropy Loss: 1.028804898262024, LR: 0.001\n",
      "Step 216/1000, Cross-entropy Loss: 1.0366499423980713, LR: 0.001\n",
      "Step 217/1000, Cross-entropy Loss: 1.0541917085647583, LR: 0.001\n",
      "Step 218/1000, Cross-entropy Loss: 1.0197527408599854, LR: 0.001\n",
      "Step 219/1000, Cross-entropy Loss: 0.9776138067245483, LR: 0.001\n",
      "Step 220/1000, Cross-entropy Loss: 0.9953901171684265, LR: 0.001\n",
      "Step 221/1000, Cross-entropy Loss: 0.961807906627655, LR: 0.001\n",
      "Step 222/1000, Cross-entropy Loss: 0.9290577173233032, LR: 0.001\n",
      "Step 223/1000, Cross-entropy Loss: 0.8809598684310913, LR: 0.001\n",
      "Step 224/1000, Cross-entropy Loss: 0.9124068021774292, LR: 0.001\n",
      "Step 225/1000, Cross-entropy Loss: 0.8576738238334656, LR: 0.001\n",
      "Step 226/1000, Cross-entropy Loss: 0.8312252163887024, LR: 0.001\n",
      "Step 227/1000, Cross-entropy Loss: 0.8294178247451782, LR: 0.001\n",
      "Step 228/1000, Cross-entropy Loss: 0.8224884867668152, LR: 0.001\n",
      "Step 229/1000, Cross-entropy Loss: 0.7799461483955383, LR: 0.001\n",
      "Step 230/1000, Cross-entropy Loss: 0.7648895978927612, LR: 0.001\n",
      "Step 231/1000, Cross-entropy Loss: 0.731850266456604, LR: 0.001\n",
      "Step 232/1000, Cross-entropy Loss: 0.7150508165359497, LR: 0.001\n",
      "Step 233/1000, Cross-entropy Loss: 0.7141676545143127, LR: 0.001\n",
      "Step 234/1000, Cross-entropy Loss: 0.7012280225753784, LR: 0.001\n",
      "Step 235/1000, Cross-entropy Loss: 0.6780065298080444, LR: 0.001\n",
      "Step 236/1000, Cross-entropy Loss: 0.6504127383232117, LR: 0.001\n",
      "Step 237/1000, Cross-entropy Loss: 0.6325465440750122, LR: 0.001\n",
      "Step 238/1000, Cross-entropy Loss: 0.6197452545166016, LR: 0.001\n",
      "Step 239/1000, Cross-entropy Loss: 0.6001253724098206, LR: 0.001\n",
      "Step 240/1000, Cross-entropy Loss: 0.5648237466812134, LR: 0.001\n",
      "Step 241/1000, Cross-entropy Loss: 0.5588467717170715, LR: 0.001\n",
      "Step 242/1000, Cross-entropy Loss: 0.5779587030410767, LR: 0.001\n",
      "Step 243/1000, Cross-entropy Loss: 0.5457502007484436, LR: 0.001\n",
      "Step 244/1000, Cross-entropy Loss: 0.5692167282104492, LR: 0.001\n",
      "Step 245/1000, Cross-entropy Loss: 0.5567766427993774, LR: 0.001\n",
      "Step 246/1000, Cross-entropy Loss: 0.5310106873512268, LR: 0.001\n",
      "Step 247/1000, Cross-entropy Loss: 0.5502675175666809, LR: 0.001\n",
      "Step 248/1000, Cross-entropy Loss: 0.5388351082801819, LR: 0.001\n",
      "Step 249/1000, Cross-entropy Loss: 0.5146695375442505, LR: 0.001\n",
      "Step 250/1000, Cross-entropy Loss: 0.513241708278656, LR: 0.001\n",
      "Step 251/1000, Cross-entropy Loss: 0.4975881576538086, LR: 0.001\n",
      "Step 252/1000, Cross-entropy Loss: 0.5012083649635315, LR: 0.001\n",
      "Step 253/1000, Cross-entropy Loss: 0.4623090326786041, LR: 0.001\n",
      "Step 254/1000, Cross-entropy Loss: 0.47318920493125916, LR: 0.001\n",
      "Step 255/1000, Cross-entropy Loss: 0.47033780813217163, LR: 0.001\n",
      "Step 256/1000, Cross-entropy Loss: 0.4749329686164856, LR: 0.001\n",
      "Step 257/1000, Cross-entropy Loss: 0.4907159209251404, LR: 0.001\n",
      "Step 258/1000, Cross-entropy Loss: 0.4398782253265381, LR: 0.001\n",
      "Step 259/1000, Cross-entropy Loss: 0.4296647906303406, LR: 0.001\n",
      "Step 260/1000, Cross-entropy Loss: 0.43921375274658203, LR: 0.001\n",
      "Step 261/1000, Cross-entropy Loss: 0.42715901136398315, LR: 0.001\n",
      "Step 262/1000, Cross-entropy Loss: 0.40685051679611206, LR: 0.001\n",
      "Step 263/1000, Cross-entropy Loss: 0.4015483856201172, LR: 0.001\n",
      "Step 264/1000, Cross-entropy Loss: 0.399497926235199, LR: 0.001\n",
      "Step 265/1000, Cross-entropy Loss: 0.3652811050415039, LR: 0.001\n",
      "Step 266/1000, Cross-entropy Loss: 0.3556631803512573, LR: 0.001\n",
      "Step 267/1000, Cross-entropy Loss: 0.34559953212738037, LR: 0.001\n",
      "Step 268/1000, Cross-entropy Loss: 0.30965226888656616, LR: 0.001\n",
      "Step 269/1000, Cross-entropy Loss: 0.3183504641056061, LR: 0.001\n",
      "Step 270/1000, Cross-entropy Loss: 0.3164215683937073, LR: 0.001\n",
      "Step 271/1000, Cross-entropy Loss: 0.28677383065223694, LR: 0.001\n",
      "Step 272/1000, Cross-entropy Loss: 0.27712303400039673, LR: 0.001\n",
      "Step 273/1000, Cross-entropy Loss: 0.31661340594291687, LR: 0.001\n",
      "Step 274/1000, Cross-entropy Loss: 0.27759987115859985, LR: 0.001\n",
      "Step 275/1000, Cross-entropy Loss: 0.3028293251991272, LR: 0.001\n",
      "Step 276/1000, Cross-entropy Loss: 0.2976686358451843, LR: 0.001\n",
      "Step 277/1000, Cross-entropy Loss: 0.252004474401474, LR: 0.001\n",
      "Step 278/1000, Cross-entropy Loss: 0.2547283172607422, LR: 0.001\n",
      "Step 279/1000, Cross-entropy Loss: 0.2258332073688507, LR: 0.001\n",
      "Step 280/1000, Cross-entropy Loss: 0.2616187334060669, LR: 0.001\n",
      "Step 281/1000, Cross-entropy Loss: 0.24086466431617737, LR: 0.001\n",
      "Step 282/1000, Cross-entropy Loss: 0.245092511177063, LR: 0.001\n",
      "Step 283/1000, Cross-entropy Loss: 0.2376922369003296, LR: 0.001\n",
      "Step 284/1000, Cross-entropy Loss: 0.23637863993644714, LR: 0.001\n",
      "Step 285/1000, Cross-entropy Loss: 0.24617519974708557, LR: 0.001\n",
      "Step 286/1000, Cross-entropy Loss: 0.223436638712883, LR: 0.001\n",
      "Step 287/1000, Cross-entropy Loss: 0.21164672076702118, LR: 0.001\n",
      "Step 288/1000, Cross-entropy Loss: 0.20789146423339844, LR: 0.001\n",
      "Step 289/1000, Cross-entropy Loss: 0.20929419994354248, LR: 0.001\n",
      "Step 290/1000, Cross-entropy Loss: 0.192968487739563, LR: 0.001\n",
      "Step 291/1000, Cross-entropy Loss: 0.1782800257205963, LR: 0.001\n",
      "Step 292/1000, Cross-entropy Loss: 0.19923773407936096, LR: 0.001\n",
      "Step 293/1000, Cross-entropy Loss: 0.21924933791160583, LR: 0.001\n",
      "Step 294/1000, Cross-entropy Loss: 0.19536158442497253, LR: 0.001\n",
      "Step 295/1000, Cross-entropy Loss: 0.21511411666870117, LR: 0.001\n",
      "Step 296/1000, Cross-entropy Loss: 0.2141283005475998, LR: 0.001\n",
      "Step 297/1000, Cross-entropy Loss: 0.20180635154247284, LR: 0.001\n",
      "Step 298/1000, Cross-entropy Loss: 0.188910573720932, LR: 0.001\n",
      "Step 299/1000, Cross-entropy Loss: 0.2097810059785843, LR: 0.001\n",
      "Step 300/1000, Cross-entropy Loss: 0.19156187772750854, LR: 0.001\n",
      "Step 301/1000, Cross-entropy Loss: 0.16651302576065063, LR: 0.001\n",
      "Step 302/1000, Cross-entropy Loss: 0.14652547240257263, LR: 0.001\n",
      "Step 303/1000, Cross-entropy Loss: 0.14755494892597198, LR: 0.001\n",
      "Step 304/1000, Cross-entropy Loss: 0.16081735491752625, LR: 0.001\n",
      "Step 305/1000, Cross-entropy Loss: 0.13876448571681976, LR: 0.001\n",
      "Step 306/1000, Cross-entropy Loss: 0.17173083126544952, LR: 0.001\n",
      "Step 307/1000, Cross-entropy Loss: 0.16997137665748596, LR: 0.001\n",
      "Step 308/1000, Cross-entropy Loss: 0.17982850968837738, LR: 0.001\n",
      "Step 309/1000, Cross-entropy Loss: 0.1646220088005066, LR: 0.001\n",
      "Step 310/1000, Cross-entropy Loss: 0.15301676094532013, LR: 0.001\n",
      "Step 311/1000, Cross-entropy Loss: 0.17226465046405792, LR: 0.001\n",
      "Step 312/1000, Cross-entropy Loss: 0.18742278218269348, LR: 0.001\n",
      "Step 313/1000, Cross-entropy Loss: 0.15506069362163544, LR: 0.001\n",
      "Step 314/1000, Cross-entropy Loss: 0.16287243366241455, LR: 0.001\n",
      "Step 315/1000, Cross-entropy Loss: 0.13961192965507507, LR: 0.001\n",
      "Step 316/1000, Cross-entropy Loss: 0.13754169642925262, LR: 0.001\n",
      "Step 317/1000, Cross-entropy Loss: 0.1309695541858673, LR: 0.001\n",
      "Step 318/1000, Cross-entropy Loss: 0.11044250428676605, LR: 0.001\n",
      "Step 319/1000, Cross-entropy Loss: 0.12144593149423599, LR: 0.001\n",
      "Step 320/1000, Cross-entropy Loss: 0.12428998947143555, LR: 0.001\n",
      "Step 321/1000, Cross-entropy Loss: 0.1263977736234665, LR: 0.001\n",
      "Step 322/1000, Cross-entropy Loss: 0.11725033819675446, LR: 0.001\n",
      "Step 323/1000, Cross-entropy Loss: 0.11437257379293442, LR: 0.001\n",
      "Step 324/1000, Cross-entropy Loss: 0.1343056857585907, LR: 0.001\n",
      "Step 325/1000, Cross-entropy Loss: 0.1311502903699875, LR: 0.001\n",
      "Step 326/1000, Cross-entropy Loss: 0.1408374160528183, LR: 0.001\n",
      "Step 327/1000, Cross-entropy Loss: 0.14586645364761353, LR: 0.001\n",
      "Step 328/1000, Cross-entropy Loss: 0.1182011142373085, LR: 0.001\n",
      "Step 329/1000, Cross-entropy Loss: 0.11775054782629013, LR: 0.001\n",
      "Step 330/1000, Cross-entropy Loss: 0.1411788910627365, LR: 0.001\n",
      "Step 331/1000, Cross-entropy Loss: 0.1287398636341095, LR: 0.001\n",
      "Step 332/1000, Cross-entropy Loss: 0.12655797600746155, LR: 0.001\n",
      "Step 333/1000, Cross-entropy Loss: 0.11888567358255386, LR: 0.001\n",
      "Step 334/1000, Cross-entropy Loss: 0.11119582504034042, LR: 0.001\n",
      "Step 335/1000, Cross-entropy Loss: 0.1037963405251503, LR: 0.001\n",
      "Step 336/1000, Cross-entropy Loss: 0.0926017016172409, LR: 0.001\n",
      "Step 337/1000, Cross-entropy Loss: 0.0875379741191864, LR: 0.001\n",
      "Step 338/1000, Cross-entropy Loss: 0.08431217819452286, LR: 0.001\n",
      "Step 339/1000, Cross-entropy Loss: 0.07867754995822906, LR: 0.001\n",
      "Step 340/1000, Cross-entropy Loss: 0.07714207470417023, LR: 0.001\n",
      "Step 341/1000, Cross-entropy Loss: 0.09119237959384918, LR: 0.001\n",
      "Step 342/1000, Cross-entropy Loss: 0.07718636095523834, LR: 0.001\n",
      "Step 343/1000, Cross-entropy Loss: 0.07082492858171463, LR: 0.001\n",
      "Step 344/1000, Cross-entropy Loss: 0.06900309026241302, LR: 0.001\n",
      "Step 345/1000, Cross-entropy Loss: 0.07433711737394333, LR: 0.001\n",
      "Step 346/1000, Cross-entropy Loss: 0.09134405106306076, LR: 0.001\n",
      "Step 347/1000, Cross-entropy Loss: 0.08542182296514511, LR: 0.001\n",
      "Step 348/1000, Cross-entropy Loss: 0.07593758404254913, LR: 0.001\n",
      "Step 349/1000, Cross-entropy Loss: 0.07219292223453522, LR: 0.001\n",
      "Step 350/1000, Cross-entropy Loss: 0.07926683872938156, LR: 0.001\n",
      "Step 351/1000, Cross-entropy Loss: 0.0693020299077034, LR: 0.001\n",
      "Step 352/1000, Cross-entropy Loss: 0.07975341379642487, LR: 0.001\n",
      "Step 353/1000, Cross-entropy Loss: 0.06629530340433121, LR: 0.001\n",
      "Step 354/1000, Cross-entropy Loss: 0.0801343023777008, LR: 0.001\n",
      "Step 355/1000, Cross-entropy Loss: 0.08597022294998169, LR: 0.001\n",
      "Step 356/1000, Cross-entropy Loss: 0.07011622190475464, LR: 0.001\n",
      "Step 357/1000, Cross-entropy Loss: 0.0787448137998581, LR: 0.001\n",
      "Step 358/1000, Cross-entropy Loss: 0.07807030528783798, LR: 0.001\n",
      "Step 359/1000, Cross-entropy Loss: 0.06260988861322403, LR: 0.001\n",
      "Step 360/1000, Cross-entropy Loss: 0.054212652146816254, LR: 0.001\n",
      "Step 361/1000, Cross-entropy Loss: 0.07632168382406235, LR: 0.001\n",
      "Step 362/1000, Cross-entropy Loss: 0.06347133964300156, LR: 0.001\n",
      "Step 363/1000, Cross-entropy Loss: 0.0991564467549324, LR: 0.001\n",
      "Step 364/1000, Cross-entropy Loss: 0.06725797057151794, LR: 0.001\n",
      "Step 365/1000, Cross-entropy Loss: 0.07463071495294571, LR: 0.001\n",
      "Step 366/1000, Cross-entropy Loss: 0.06380540132522583, LR: 0.001\n",
      "Step 367/1000, Cross-entropy Loss: 0.05573425814509392, LR: 0.001\n",
      "Step 368/1000, Cross-entropy Loss: 0.07907180488109589, LR: 0.001\n",
      "Step 369/1000, Cross-entropy Loss: 0.06910047680139542, LR: 0.001\n",
      "Step 370/1000, Cross-entropy Loss: 0.0498850978910923, LR: 0.001\n",
      "Step 371/1000, Cross-entropy Loss: 0.0492677241563797, LR: 0.001\n",
      "Step 372/1000, Cross-entropy Loss: 0.045587360858917236, LR: 0.001\n",
      "Step 373/1000, Cross-entropy Loss: 0.05438612774014473, LR: 0.001\n",
      "Step 374/1000, Cross-entropy Loss: 0.04253667965531349, LR: 0.001\n",
      "Step 375/1000, Cross-entropy Loss: 0.04353504627943039, LR: 0.001\n",
      "Step 376/1000, Cross-entropy Loss: 0.0526551678776741, LR: 0.001\n",
      "Step 377/1000, Cross-entropy Loss: 0.049264006316661835, LR: 0.001\n",
      "Step 378/1000, Cross-entropy Loss: 0.05304126814007759, LR: 0.001\n",
      "Step 379/1000, Cross-entropy Loss: 0.04256826266646385, LR: 0.001\n",
      "Step 380/1000, Cross-entropy Loss: 0.038880981504917145, LR: 0.001\n",
      "Step 381/1000, Cross-entropy Loss: 0.03644861653447151, LR: 0.001\n",
      "Step 382/1000, Cross-entropy Loss: 0.03615660220384598, LR: 0.001\n",
      "Step 383/1000, Cross-entropy Loss: 0.0324551984667778, LR: 0.001\n",
      "Step 384/1000, Cross-entropy Loss: 0.027763862162828445, LR: 0.001\n",
      "Step 385/1000, Cross-entropy Loss: 0.029292132705450058, LR: 0.001\n",
      "Step 386/1000, Cross-entropy Loss: 0.030069202184677124, LR: 0.001\n",
      "Step 387/1000, Cross-entropy Loss: 0.028277594596147537, LR: 0.001\n",
      "Step 388/1000, Cross-entropy Loss: 0.02855820022523403, LR: 0.001\n",
      "Step 389/1000, Cross-entropy Loss: 0.028236526995897293, LR: 0.001\n",
      "Step 390/1000, Cross-entropy Loss: 0.026403218507766724, LR: 0.001\n",
      "Step 391/1000, Cross-entropy Loss: 0.02889677882194519, LR: 0.001\n",
      "Step 392/1000, Cross-entropy Loss: 0.02481948956847191, LR: 0.001\n",
      "Step 393/1000, Cross-entropy Loss: 0.024938981980085373, LR: 0.001\n",
      "Step 394/1000, Cross-entropy Loss: 0.02399580366909504, LR: 0.001\n",
      "Step 395/1000, Cross-entropy Loss: 0.024496445432305336, LR: 0.001\n",
      "Step 396/1000, Cross-entropy Loss: 0.02301676943898201, LR: 0.001\n",
      "Step 397/1000, Cross-entropy Loss: 0.026018794625997543, LR: 0.001\n",
      "Step 398/1000, Cross-entropy Loss: 0.025941094383597374, LR: 0.001\n",
      "Step 399/1000, Cross-entropy Loss: 0.02277727611362934, LR: 0.001\n",
      "Step 400/1000, Cross-entropy Loss: 0.022595413029193878, LR: 0.001\n",
      "Step 401/1000, Cross-entropy Loss: 0.021699413657188416, LR: 0.001\n",
      "Step 402/1000, Cross-entropy Loss: 0.020415356382727623, LR: 0.001\n",
      "Step 403/1000, Cross-entropy Loss: 0.01865217834711075, LR: 0.001\n",
      "Step 404/1000, Cross-entropy Loss: 0.01806936226785183, LR: 0.001\n",
      "Step 405/1000, Cross-entropy Loss: 0.01762428507208824, LR: 0.001\n",
      "Step 406/1000, Cross-entropy Loss: 0.01753833517432213, LR: 0.001\n",
      "Step 407/1000, Cross-entropy Loss: 0.01645900122821331, LR: 0.001\n",
      "Step 408/1000, Cross-entropy Loss: 0.01552564837038517, LR: 0.001\n",
      "Step 409/1000, Cross-entropy Loss: 0.015550725162029266, LR: 0.001\n",
      "Step 410/1000, Cross-entropy Loss: 0.01526928972452879, LR: 0.001\n",
      "Step 411/1000, Cross-entropy Loss: 0.014546612277626991, LR: 0.001\n",
      "Step 412/1000, Cross-entropy Loss: 0.014961781911551952, LR: 0.001\n",
      "Step 413/1000, Cross-entropy Loss: 0.014140124432742596, LR: 0.001\n",
      "Step 414/1000, Cross-entropy Loss: 0.013725998811423779, LR: 0.001\n",
      "Step 415/1000, Cross-entropy Loss: 0.013310429640114307, LR: 0.001\n",
      "Step 416/1000, Cross-entropy Loss: 0.012957513332366943, LR: 0.001\n",
      "Step 417/1000, Cross-entropy Loss: 0.012409191578626633, LR: 0.001\n",
      "Step 418/1000, Cross-entropy Loss: 0.012306295335292816, LR: 0.001\n",
      "Step 419/1000, Cross-entropy Loss: 0.015370418317615986, LR: 0.001\n",
      "Step 420/1000, Cross-entropy Loss: 0.01294703222811222, LR: 0.001\n",
      "Step 421/1000, Cross-entropy Loss: 0.013452140614390373, LR: 0.001\n",
      "Step 422/1000, Cross-entropy Loss: 0.014446747489273548, LR: 0.001\n",
      "Step 423/1000, Cross-entropy Loss: 0.01341969333589077, LR: 0.001\n",
      "Step 424/1000, Cross-entropy Loss: 0.013108238577842712, LR: 0.001\n",
      "Step 425/1000, Cross-entropy Loss: 0.013815348967909813, LR: 0.001\n",
      "Step 426/1000, Cross-entropy Loss: 0.012988296337425709, LR: 0.001\n",
      "Step 427/1000, Cross-entropy Loss: 0.01296275295317173, LR: 0.001\n",
      "Step 428/1000, Cross-entropy Loss: 0.012098035775125027, LR: 0.001\n",
      "Step 429/1000, Cross-entropy Loss: 0.01386116724461317, LR: 0.001\n",
      "Step 430/1000, Cross-entropy Loss: 0.015249530784785748, LR: 0.001\n",
      "Step 431/1000, Cross-entropy Loss: 0.013038858771324158, LR: 0.001\n",
      "Step 432/1000, Cross-entropy Loss: 0.012653110548853874, LR: 0.001\n",
      "Step 433/1000, Cross-entropy Loss: 0.013184644281864166, LR: 0.001\n",
      "Step 434/1000, Cross-entropy Loss: 0.01254165917634964, LR: 0.001\n",
      "Step 435/1000, Cross-entropy Loss: 0.01238099206238985, LR: 0.001\n",
      "Step 436/1000, Cross-entropy Loss: 0.012303194031119347, LR: 0.001\n",
      "Step 437/1000, Cross-entropy Loss: 0.012258081696927547, LR: 0.001\n",
      "Step 438/1000, Cross-entropy Loss: 0.012929318472743034, LR: 0.001\n",
      "Step 439/1000, Cross-entropy Loss: 0.012670869939029217, LR: 0.001\n",
      "Step 440/1000, Cross-entropy Loss: 0.012374588288366795, LR: 0.001\n",
      "Step 441/1000, Cross-entropy Loss: 0.01338307373225689, LR: 0.001\n",
      "Step 442/1000, Cross-entropy Loss: 0.013054360635578632, LR: 0.001\n",
      "Step 443/1000, Cross-entropy Loss: 0.015574753284454346, LR: 0.001\n",
      "Step 444/1000, Cross-entropy Loss: 0.013859678991138935, LR: 0.001\n",
      "Step 445/1000, Cross-entropy Loss: 0.014089453034102917, LR: 0.001\n",
      "Step 446/1000, Cross-entropy Loss: 0.013388906605541706, LR: 0.001\n",
      "Step 447/1000, Cross-entropy Loss: 0.013380302116274834, LR: 0.001\n",
      "Step 448/1000, Cross-entropy Loss: 0.012830806896090508, LR: 0.001\n",
      "Step 449/1000, Cross-entropy Loss: 0.011724629439413548, LR: 0.001\n",
      "Step 450/1000, Cross-entropy Loss: 0.012693946249783039, LR: 0.001\n",
      "Step 451/1000, Cross-entropy Loss: 0.010674710385501385, LR: 0.001\n",
      "Step 452/1000, Cross-entropy Loss: 0.011992060579359531, LR: 0.001\n",
      "Step 453/1000, Cross-entropy Loss: 0.010583958588540554, LR: 0.001\n",
      "Step 454/1000, Cross-entropy Loss: 0.010690704919397831, LR: 0.001\n",
      "Step 455/1000, Cross-entropy Loss: 0.010600227862596512, LR: 0.001\n",
      "Step 456/1000, Cross-entropy Loss: 0.011186948977410793, LR: 0.001\n",
      "Step 457/1000, Cross-entropy Loss: 0.010175572708249092, LR: 0.001\n",
      "Step 458/1000, Cross-entropy Loss: 0.009535851888358593, LR: 0.001\n",
      "Step 459/1000, Cross-entropy Loss: 0.010320836678147316, LR: 0.001\n",
      "Step 460/1000, Cross-entropy Loss: 0.00944393128156662, LR: 0.001\n",
      "Step 461/1000, Cross-entropy Loss: 0.00959697924554348, LR: 0.001\n",
      "Step 462/1000, Cross-entropy Loss: 0.009472610428929329, LR: 0.001\n",
      "Step 463/1000, Cross-entropy Loss: 0.00909335445612669, LR: 0.001\n",
      "Step 464/1000, Cross-entropy Loss: 0.009226420894265175, LR: 0.001\n",
      "Step 465/1000, Cross-entropy Loss: 0.008712285198271275, LR: 0.001\n",
      "Step 466/1000, Cross-entropy Loss: 0.0088309021666646, LR: 0.001\n",
      "Step 467/1000, Cross-entropy Loss: 0.00870695523917675, LR: 0.001\n",
      "Step 468/1000, Cross-entropy Loss: 0.008554868400096893, LR: 0.001\n",
      "Step 469/1000, Cross-entropy Loss: 0.008581099100410938, LR: 0.001\n",
      "Step 470/1000, Cross-entropy Loss: 0.00829965341836214, LR: 0.001\n",
      "Step 471/1000, Cross-entropy Loss: 0.00845514889806509, LR: 0.001\n",
      "Step 472/1000, Cross-entropy Loss: 0.00821758434176445, LR: 0.001\n",
      "Step 473/1000, Cross-entropy Loss: 0.008257421664893627, LR: 0.001\n",
      "Step 474/1000, Cross-entropy Loss: 0.008184118196368217, LR: 0.001\n",
      "Step 475/1000, Cross-entropy Loss: 0.008336258120834827, LR: 0.001\n",
      "Step 476/1000, Cross-entropy Loss: 0.008199220523238182, LR: 0.001\n",
      "Step 477/1000, Cross-entropy Loss: 0.008062691427767277, LR: 0.001\n",
      "Step 478/1000, Cross-entropy Loss: 0.008167317137122154, LR: 0.001\n",
      "Step 479/1000, Cross-entropy Loss: 0.008005563169717789, LR: 0.001\n",
      "Step 480/1000, Cross-entropy Loss: 0.008002412505447865, LR: 0.001\n",
      "Step 481/1000, Cross-entropy Loss: 0.007943915203213692, LR: 0.001\n",
      "Step 482/1000, Cross-entropy Loss: 0.008145013824105263, LR: 0.001\n",
      "Step 483/1000, Cross-entropy Loss: 0.00799819640815258, LR: 0.001\n",
      "Step 484/1000, Cross-entropy Loss: 0.007929204031825066, LR: 0.001\n",
      "Step 485/1000, Cross-entropy Loss: 0.007868546061217785, LR: 0.001\n",
      "Step 486/1000, Cross-entropy Loss: 0.007823828607797623, LR: 0.001\n",
      "Step 487/1000, Cross-entropy Loss: 0.00775094935670495, LR: 0.001\n",
      "Step 488/1000, Cross-entropy Loss: 0.009529383853077888, LR: 0.001\n",
      "Step 489/1000, Cross-entropy Loss: 0.008431862108409405, LR: 0.001\n",
      "Step 490/1000, Cross-entropy Loss: 0.008334215730428696, LR: 0.001\n",
      "Step 491/1000, Cross-entropy Loss: 0.009074711240828037, LR: 0.001\n",
      "Step 492/1000, Cross-entropy Loss: 0.008422727696597576, LR: 0.001\n",
      "Step 493/1000, Cross-entropy Loss: 0.008387262001633644, LR: 0.001\n",
      "Step 494/1000, Cross-entropy Loss: 0.008382354862987995, LR: 0.001\n",
      "Step 495/1000, Cross-entropy Loss: 0.008525596000254154, LR: 0.001\n",
      "Step 496/1000, Cross-entropy Loss: 0.008089995943009853, LR: 0.001\n",
      "Step 497/1000, Cross-entropy Loss: 0.0080243069678545, LR: 0.001\n",
      "Step 498/1000, Cross-entropy Loss: 0.008195832371711731, LR: 0.001\n",
      "Step 499/1000, Cross-entropy Loss: 0.00833895243704319, LR: 0.001\n",
      "Step 500/1000, Cross-entropy Loss: 0.008147863671183586, LR: 0.001\n",
      "Step 501/1000, Cross-entropy Loss: 0.00825081579387188, LR: 0.001\n",
      "Step 502/1000, Cross-entropy Loss: 0.009328948333859444, LR: 0.001\n",
      "Step 503/1000, Cross-entropy Loss: 0.008472112007439137, LR: 0.001\n",
      "Step 504/1000, Cross-entropy Loss: 0.008017439395189285, LR: 0.001\n",
      "Step 505/1000, Cross-entropy Loss: 0.00816249754279852, LR: 0.001\n",
      "Step 506/1000, Cross-entropy Loss: 0.008665356785058975, LR: 0.001\n",
      "Step 507/1000, Cross-entropy Loss: 0.008594604209065437, LR: 0.001\n",
      "Step 508/1000, Cross-entropy Loss: 0.008254104293882847, LR: 0.0002\n",
      "Step 509/1000, Cross-entropy Loss: 0.008411642163991928, LR: 0.0002\n",
      "Step 510/1000, Cross-entropy Loss: 0.008251961320638657, LR: 0.0002\n",
      "Step 511/1000, Cross-entropy Loss: 0.008008900098502636, LR: 0.0002\n",
      "Step 512/1000, Cross-entropy Loss: 0.007800726685672998, LR: 0.0002\n",
      "Step 513/1000, Cross-entropy Loss: 0.007638721261173487, LR: 0.0002\n",
      "Step 514/1000, Cross-entropy Loss: 0.007524046115577221, LR: 0.0002\n",
      "Step 515/1000, Cross-entropy Loss: 0.0074385590851306915, LR: 0.0002\n",
      "Step 516/1000, Cross-entropy Loss: 0.007399705238640308, LR: 0.0002\n",
      "Step 517/1000, Cross-entropy Loss: 0.007372945547103882, LR: 0.0002\n",
      "Step 518/1000, Cross-entropy Loss: 0.007330307271331549, LR: 0.0002\n",
      "Step 519/1000, Cross-entropy Loss: 0.007284160703420639, LR: 0.0002\n",
      "Step 520/1000, Cross-entropy Loss: 0.00724807009100914, LR: 0.0002\n",
      "Step 521/1000, Cross-entropy Loss: 0.007223695516586304, LR: 0.0002\n",
      "Step 522/1000, Cross-entropy Loss: 0.007198167033493519, LR: 0.0002\n",
      "Step 523/1000, Cross-entropy Loss: 0.0071693598292768, LR: 0.0002\n",
      "Step 524/1000, Cross-entropy Loss: 0.00714438920840621, LR: 0.0002\n",
      "Step 525/1000, Cross-entropy Loss: 0.00712422002106905, LR: 0.0002\n",
      "Step 526/1000, Cross-entropy Loss: 0.007102302275598049, LR: 0.0002\n",
      "Step 527/1000, Cross-entropy Loss: 0.007075199391692877, LR: 0.0002\n",
      "Step 528/1000, Cross-entropy Loss: 0.007044795900583267, LR: 0.0002\n",
      "Step 529/1000, Cross-entropy Loss: 0.0070198699831962585, LR: 0.0002\n",
      "Step 530/1000, Cross-entropy Loss: 0.006996182259172201, LR: 0.0002\n",
      "Step 531/1000, Cross-entropy Loss: 0.006977254990488291, LR: 0.0002\n",
      "Step 532/1000, Cross-entropy Loss: 0.006958925165235996, LR: 0.0002\n",
      "Step 533/1000, Cross-entropy Loss: 0.006938966456800699, LR: 0.0002\n",
      "Step 534/1000, Cross-entropy Loss: 0.006914637051522732, LR: 0.0002\n",
      "Step 535/1000, Cross-entropy Loss: 0.006895706057548523, LR: 0.0002\n",
      "Step 536/1000, Cross-entropy Loss: 0.006876507308334112, LR: 0.0002\n",
      "Step 537/1000, Cross-entropy Loss: 0.00685674324631691, LR: 0.0002\n",
      "Step 538/1000, Cross-entropy Loss: 0.006837619934231043, LR: 0.0002\n",
      "Step 539/1000, Cross-entropy Loss: 0.006821741349995136, LR: 0.0002\n",
      "Step 540/1000, Cross-entropy Loss: 0.006805545650422573, LR: 0.0002\n",
      "Step 541/1000, Cross-entropy Loss: 0.006792780943214893, LR: 0.0002\n",
      "Step 542/1000, Cross-entropy Loss: 0.006775225047022104, LR: 0.0002\n",
      "Step 543/1000, Cross-entropy Loss: 0.006759838666766882, LR: 0.0002\n",
      "Step 544/1000, Cross-entropy Loss: 0.006744311191141605, LR: 0.0002\n",
      "Step 545/1000, Cross-entropy Loss: 0.006729391403496265, LR: 0.0002\n",
      "Step 546/1000, Cross-entropy Loss: 0.006715989205986261, LR: 0.0002\n",
      "Step 547/1000, Cross-entropy Loss: 0.006702786777168512, LR: 0.0002\n",
      "Step 548/1000, Cross-entropy Loss: 0.006689182482659817, LR: 0.0002\n",
      "Step 549/1000, Cross-entropy Loss: 0.0066758752800524235, LR: 0.0002\n",
      "Step 550/1000, Cross-entropy Loss: 0.006662630941718817, LR: 0.0002\n",
      "Step 551/1000, Cross-entropy Loss: 0.006648375187069178, LR: 0.0002\n",
      "Step 552/1000, Cross-entropy Loss: 0.006634793244302273, LR: 0.0002\n",
      "Step 553/1000, Cross-entropy Loss: 0.006620532833039761, LR: 0.0002\n",
      "Step 554/1000, Cross-entropy Loss: 0.0066103688441216946, LR: 0.0002\n",
      "Step 555/1000, Cross-entropy Loss: 0.006597516126930714, LR: 0.0002\n",
      "Step 556/1000, Cross-entropy Loss: 0.006605419330298901, LR: 0.0002\n",
      "Step 557/1000, Cross-entropy Loss: 0.006574516650289297, LR: 0.0002\n",
      "Step 558/1000, Cross-entropy Loss: 0.006562050431966782, LR: 0.0002\n",
      "Step 559/1000, Cross-entropy Loss: 0.006549706216901541, LR: 0.0002\n",
      "Step 560/1000, Cross-entropy Loss: 0.006537950597703457, LR: 0.0002\n",
      "Step 561/1000, Cross-entropy Loss: 0.0065256827510893345, LR: 0.0002\n",
      "Step 562/1000, Cross-entropy Loss: 0.0065133897587656975, LR: 0.0002\n",
      "Step 563/1000, Cross-entropy Loss: 0.0065019624307751656, LR: 0.0002\n",
      "Step 564/1000, Cross-entropy Loss: 0.00648975744843483, LR: 0.0002\n",
      "Step 565/1000, Cross-entropy Loss: 0.006477832794189453, LR: 0.0002\n",
      "Step 566/1000, Cross-entropy Loss: 0.006465586367994547, LR: 0.0002\n",
      "Step 567/1000, Cross-entropy Loss: 0.0064536272548139095, LR: 0.0002\n",
      "Step 568/1000, Cross-entropy Loss: 0.006441828794777393, LR: 0.0002\n",
      "Step 569/1000, Cross-entropy Loss: 0.006429885979741812, LR: 0.0002\n",
      "Step 570/1000, Cross-entropy Loss: 0.006417685188353062, LR: 0.0002\n",
      "Step 571/1000, Cross-entropy Loss: 0.006405453197658062, LR: 0.0002\n",
      "Step 572/1000, Cross-entropy Loss: 0.00639319559559226, LR: 0.0002\n",
      "Step 573/1000, Cross-entropy Loss: 0.006381125655025244, LR: 0.0002\n",
      "Step 574/1000, Cross-entropy Loss: 0.0063692741096019745, LR: 0.0002\n",
      "Step 575/1000, Cross-entropy Loss: 0.0063574365340173244, LR: 0.0002\n",
      "Step 576/1000, Cross-entropy Loss: 0.006345444358885288, LR: 0.0002\n",
      "Step 577/1000, Cross-entropy Loss: 0.0063334577716887, LR: 0.0002\n",
      "Step 578/1000, Cross-entropy Loss: 0.0063216486014425755, LR: 0.0002\n",
      "Step 579/1000, Cross-entropy Loss: 0.0063099912367761135, LR: 0.0002\n",
      "Step 580/1000, Cross-entropy Loss: 0.006298366002738476, LR: 0.0002\n",
      "Step 581/1000, Cross-entropy Loss: 0.006286622490733862, LR: 0.0002\n",
      "Step 582/1000, Cross-entropy Loss: 0.006274793297052383, LR: 0.0002\n",
      "Step 583/1000, Cross-entropy Loss: 0.006263001821935177, LR: 0.0002\n",
      "Step 584/1000, Cross-entropy Loss: 0.006251317914575338, LR: 0.0002\n",
      "Step 585/1000, Cross-entropy Loss: 0.006239711306989193, LR: 0.0002\n",
      "Step 586/1000, Cross-entropy Loss: 0.006228073034435511, LR: 0.0002\n",
      "Step 587/1000, Cross-entropy Loss: 0.006216309033334255, LR: 0.0002\n",
      "Step 588/1000, Cross-entropy Loss: 0.0062055084854364395, LR: 0.0002\n",
      "Step 589/1000, Cross-entropy Loss: 0.006193422246724367, LR: 0.0002\n",
      "Step 590/1000, Cross-entropy Loss: 0.006181842647492886, LR: 0.0002\n",
      "Step 591/1000, Cross-entropy Loss: 0.006170164328068495, LR: 0.0002\n",
      "Step 592/1000, Cross-entropy Loss: 0.006158456206321716, LR: 0.0002\n",
      "Step 593/1000, Cross-entropy Loss: 0.006146734114736319, LR: 0.0002\n",
      "Step 594/1000, Cross-entropy Loss: 0.006135034374892712, LR: 0.0002\n",
      "Step 595/1000, Cross-entropy Loss: 0.006123429164290428, LR: 0.0002\n",
      "Step 596/1000, Cross-entropy Loss: 0.006111861206591129, LR: 0.0002\n",
      "Step 597/1000, Cross-entropy Loss: 0.006100327707827091, LR: 0.0002\n",
      "Step 598/1000, Cross-entropy Loss: 0.006088855676352978, LR: 0.0002\n",
      "Step 599/1000, Cross-entropy Loss: 0.006077382247895002, LR: 0.0002\n",
      "Step 600/1000, Cross-entropy Loss: 0.006065895315259695, LR: 0.0002\n",
      "Step 601/1000, Cross-entropy Loss: 0.00605437159538269, LR: 0.0002\n",
      "Step 602/1000, Cross-entropy Loss: 0.006042829714715481, LR: 0.0002\n",
      "Step 603/1000, Cross-entropy Loss: 0.00603130180388689, LR: 0.0002\n",
      "Step 604/1000, Cross-entropy Loss: 0.006019775755703449, LR: 0.0002\n",
      "Step 605/1000, Cross-entropy Loss: 0.006008228752762079, LR: 0.0002\n",
      "Step 606/1000, Cross-entropy Loss: 0.005996636115014553, LR: 0.0002\n",
      "Step 607/1000, Cross-entropy Loss: 0.005985083524137735, LR: 0.0002\n",
      "Step 608/1000, Cross-entropy Loss: 0.005973562598228455, LR: 0.0002\n",
      "Step 609/1000, Cross-entropy Loss: 0.0059620533138513565, LR: 0.0002\n",
      "Step 610/1000, Cross-entropy Loss: 0.005950530990958214, LR: 0.0002\n",
      "Step 611/1000, Cross-entropy Loss: 0.005939007271081209, LR: 0.0002\n",
      "Step 612/1000, Cross-entropy Loss: 0.005927469115704298, LR: 0.0002\n",
      "Step 613/1000, Cross-entropy Loss: 0.005915953777730465, LR: 0.0002\n",
      "Step 614/1000, Cross-entropy Loss: 0.005904472898691893, LR: 0.0002\n",
      "Step 615/1000, Cross-entropy Loss: 0.005892985966056585, LR: 0.0002\n",
      "Step 616/1000, Cross-entropy Loss: 0.00588148133829236, LR: 0.0002\n",
      "Step 617/1000, Cross-entropy Loss: 0.005869974847882986, LR: 0.0002\n",
      "Step 618/1000, Cross-entropy Loss: 0.00585847906768322, LR: 0.0002\n",
      "Step 619/1000, Cross-entropy Loss: 0.0058469995856285095, LR: 0.0002\n",
      "Step 620/1000, Cross-entropy Loss: 0.005835509859025478, LR: 0.0002\n",
      "Step 621/1000, Cross-entropy Loss: 0.0058240401558578014, LR: 0.0002\n",
      "Step 622/1000, Cross-entropy Loss: 0.005812530871480703, LR: 0.0002\n",
      "Step 623/1000, Cross-entropy Loss: 0.005800941959023476, LR: 0.0002\n",
      "Step 624/1000, Cross-entropy Loss: 0.005789268296211958, LR: 0.0002\n",
      "Step 625/1000, Cross-entropy Loss: 0.005777700338512659, LR: 0.0002\n",
      "Step 626/1000, Cross-entropy Loss: 0.0057662115432322025, LR: 0.0002\n",
      "Step 627/1000, Cross-entropy Loss: 0.0057547567412257195, LR: 0.0002\n",
      "Step 628/1000, Cross-entropy Loss: 0.0057433415204286575, LR: 0.0002\n",
      "Step 629/1000, Cross-entropy Loss: 0.005731952842324972, LR: 0.0002\n",
      "Step 630/1000, Cross-entropy Loss: 0.0057205925695598125, LR: 0.0002\n",
      "Step 631/1000, Cross-entropy Loss: 0.0057092392817139626, LR: 0.0002\n",
      "Step 632/1000, Cross-entropy Loss: 0.005697870161384344, LR: 0.0002\n",
      "Step 633/1000, Cross-entropy Loss: 0.005686499178409576, LR: 0.0002\n",
      "Step 634/1000, Cross-entropy Loss: 0.005675149150192738, LR: 0.0002\n",
      "Step 635/1000, Cross-entropy Loss: 0.0056637912057340145, LR: 0.0002\n",
      "Step 636/1000, Cross-entropy Loss: 0.00565243000164628, LR: 0.0002\n",
      "Step 637/1000, Cross-entropy Loss: 0.005641012452542782, LR: 0.0002\n",
      "Step 638/1000, Cross-entropy Loss: 0.005629573483020067, LR: 0.0002\n",
      "Step 639/1000, Cross-entropy Loss: 0.0056181401014328, LR: 0.0002\n",
      "Step 640/1000, Cross-entropy Loss: 0.005606671795248985, LR: 0.0002\n",
      "Step 641/1000, Cross-entropy Loss: 0.005595178343355656, LR: 0.0002\n",
      "Step 642/1000, Cross-entropy Loss: 0.005583656020462513, LR: 0.0002\n",
      "Step 643/1000, Cross-entropy Loss: 0.005572117865085602, LR: 0.0002\n",
      "Step 644/1000, Cross-entropy Loss: 0.005560600198805332, LR: 0.0002\n",
      "Step 645/1000, Cross-entropy Loss: 0.005549062974750996, LR: 0.0002\n",
      "Step 646/1000, Cross-entropy Loss: 0.005537549965083599, LR: 0.0002\n",
      "Step 647/1000, Cross-entropy Loss: 0.005526022054255009, LR: 0.0002\n",
      "Step 648/1000, Cross-entropy Loss: 0.0055144778452813625, LR: 0.0002\n",
      "Step 649/1000, Cross-entropy Loss: 0.0055029066279530525, LR: 0.0002\n",
      "Step 650/1000, Cross-entropy Loss: 0.005491326563060284, LR: 0.0002\n",
      "Step 651/1000, Cross-entropy Loss: 0.005479677580296993, LR: 0.0002\n",
      "Step 652/1000, Cross-entropy Loss: 0.005468219518661499, LR: 0.0002\n",
      "Step 653/1000, Cross-entropy Loss: 0.005456417333334684, LR: 0.0002\n",
      "Step 654/1000, Cross-entropy Loss: 0.005444697104394436, LR: 0.0002\n",
      "Step 655/1000, Cross-entropy Loss: 0.0054330541752278805, LR: 0.0002\n",
      "Step 656/1000, Cross-entropy Loss: 0.005421360023319721, LR: 0.0002\n",
      "Step 657/1000, Cross-entropy Loss: 0.005409603007137775, LR: 0.0002\n",
      "Step 658/1000, Cross-entropy Loss: 0.005397771019488573, LR: 0.0002\n",
      "Step 659/1000, Cross-entropy Loss: 0.005385865457355976, LR: 0.0002\n",
      "Step 660/1000, Cross-entropy Loss: 0.005373891908675432, LR: 0.0002\n",
      "Step 661/1000, Cross-entropy Loss: 0.005361795425415039, LR: 0.0002\n",
      "Step 662/1000, Cross-entropy Loss: 0.005349519196897745, LR: 0.0002\n",
      "Step 663/1000, Cross-entropy Loss: 0.005336820147931576, LR: 0.0002\n",
      "Step 664/1000, Cross-entropy Loss: 0.005322759039700031, LR: 0.0002\n",
      "Step 665/1000, Cross-entropy Loss: 0.005298965610563755, LR: 0.0002\n",
      "Step 666/1000, Cross-entropy Loss: 0.005152949132025242, LR: 0.0002\n",
      "Step 667/1000, Cross-entropy Loss: 0.005066442769020796, LR: 0.0002\n",
      "Step 668/1000, Cross-entropy Loss: 0.005062807351350784, LR: 0.0002\n",
      "Step 669/1000, Cross-entropy Loss: 0.00518889632076025, LR: 0.0002\n",
      "Step 670/1000, Cross-entropy Loss: 0.00659686466678977, LR: 0.0002\n",
      "Step 671/1000, Cross-entropy Loss: 0.00537418108433485, LR: 0.0002\n",
      "Step 672/1000, Cross-entropy Loss: 0.00637001171708107, LR: 0.0002\n",
      "Step 673/1000, Cross-entropy Loss: 0.005279670003801584, LR: 0.0002\n",
      "Step 674/1000, Cross-entropy Loss: 0.006095512770116329, LR: 0.0002\n",
      "Step 675/1000, Cross-entropy Loss: 0.005459870211780071, LR: 0.0002\n",
      "Step 676/1000, Cross-entropy Loss: 0.005750748328864574, LR: 0.0002\n",
      "Step 677/1000, Cross-entropy Loss: 0.0056358929723501205, LR: 0.0002\n",
      "Step 678/1000, Cross-entropy Loss: 0.005438664928078651, LR: 0.0002\n",
      "Step 679/1000, Cross-entropy Loss: 0.005666952580213547, LR: 0.0002\n",
      "Step 680/1000, Cross-entropy Loss: 0.005371300503611565, LR: 0.0002\n",
      "Step 681/1000, Cross-entropy Loss: 0.005526578985154629, LR: 0.0002\n",
      "Step 682/1000, Cross-entropy Loss: 0.005589426029473543, LR: 0.0002\n",
      "Step 683/1000, Cross-entropy Loss: 0.005587770603597164, LR: 0.0002\n",
      "Step 684/1000, Cross-entropy Loss: 0.005411012098193169, LR: 0.0002\n",
      "Step 685/1000, Cross-entropy Loss: 0.005317059811204672, LR: 0.0002\n",
      "Step 686/1000, Cross-entropy Loss: 0.00537442322820425, LR: 0.0002\n",
      "Step 687/1000, Cross-entropy Loss: 0.0052962834015488625, LR: 0.0002\n",
      "Step 688/1000, Cross-entropy Loss: 0.005241218023002148, LR: 0.0002\n",
      "Step 689/1000, Cross-entropy Loss: 0.005315832328051329, LR: 4e-05\n",
      "Step 690/1000, Cross-entropy Loss: 0.00515682902187109, LR: 4e-05\n",
      "Step 691/1000, Cross-entropy Loss: 0.005157248117029667, LR: 4e-05\n",
      "Step 692/1000, Cross-entropy Loss: 0.0051544299349188805, LR: 4e-05\n",
      "Step 693/1000, Cross-entropy Loss: 0.0051454599015414715, LR: 4e-05\n",
      "Step 694/1000, Cross-entropy Loss: 0.005133353173732758, LR: 4e-05\n",
      "Step 695/1000, Cross-entropy Loss: 0.005125958006829023, LR: 4e-05\n",
      "Step 696/1000, Cross-entropy Loss: 0.005126957315951586, LR: 4e-05\n",
      "Step 697/1000, Cross-entropy Loss: 0.005131165962666273, LR: 4e-05\n",
      "Step 698/1000, Cross-entropy Loss: 0.005130507983267307, LR: 4e-05\n",
      "Step 699/1000, Cross-entropy Loss: 0.00512224156409502, LR: 4e-05\n",
      "Step 700/1000, Cross-entropy Loss: 0.005110877566039562, LR: 4e-05\n",
      "Step 701/1000, Cross-entropy Loss: 0.005103491246700287, LR: 4e-05\n",
      "Step 702/1000, Cross-entropy Loss: 0.005102913826704025, LR: 4e-05\n",
      "Step 703/1000, Cross-entropy Loss: 0.005105456802994013, LR: 4e-05\n",
      "Step 704/1000, Cross-entropy Loss: 0.0051051052287220955, LR: 4e-05\n",
      "Step 705/1000, Cross-entropy Loss: 0.0050993990153074265, LR: 4e-05\n",
      "Step 706/1000, Cross-entropy Loss: 0.0050912038423120975, LR: 4e-05\n",
      "Step 707/1000, Cross-entropy Loss: 0.005085112992674112, LR: 4e-05\n",
      "Step 708/1000, Cross-entropy Loss: 0.005082930438220501, LR: 4e-05\n",
      "Step 709/1000, Cross-entropy Loss: 0.0050826240330934525, LR: 4e-05\n",
      "Step 710/1000, Cross-entropy Loss: 0.0050809732638299465, LR: 8.000000000000001e-06\n",
      "Step 711/1000, Cross-entropy Loss: 0.005076852161437273, LR: 8.000000000000001e-06\n",
      "Step 712/1000, Cross-entropy Loss: 0.005075697787106037, LR: 8.000000000000001e-06\n",
      "Step 713/1000, Cross-entropy Loss: 0.005074321758002043, LR: 8.000000000000001e-06\n",
      "Step 714/1000, Cross-entropy Loss: 0.005072879604995251, LR: 8.000000000000001e-06\n",
      "Step 715/1000, Cross-entropy Loss: 0.005071504972875118, LR: 8.000000000000001e-06\n",
      "Step 716/1000, Cross-entropy Loss: 0.005070300307124853, LR: 8.000000000000001e-06\n",
      "Step 717/1000, Cross-entropy Loss: 0.005069296341389418, LR: 8.000000000000001e-06\n",
      "Step 718/1000, Cross-entropy Loss: 0.005068495869636536, LR: 8.000000000000001e-06\n",
      "Step 719/1000, Cross-entropy Loss: 0.005067870020866394, LR: 8.000000000000001e-06\n",
      "Step 720/1000, Cross-entropy Loss: 0.005067325197160244, LR: 8.000000000000001e-06\n",
      "Step 721/1000, Cross-entropy Loss: 0.005066821817308664, LR: 8.000000000000001e-06\n",
      "Step 722/1000, Cross-entropy Loss: 0.005066304467618465, LR: 8.000000000000001e-06\n",
      "Step 723/1000, Cross-entropy Loss: 0.00506573636084795, LR: 8.000000000000001e-06\n",
      "Step 724/1000, Cross-entropy Loss: 0.005065112374722958, LR: 8.000000000000001e-06\n",
      "Step 725/1000, Cross-entropy Loss: 0.005064431112259626, LR: 8.000000000000001e-06\n",
      "Step 726/1000, Cross-entropy Loss: 0.005063721910119057, LR: 8.000000000000001e-06\n",
      "Step 727/1000, Cross-entropy Loss: 0.005063004791736603, LR: 8.000000000000001e-06\n",
      "Step 728/1000, Cross-entropy Loss: 0.005062288139015436, LR: 8.000000000000001e-06\n",
      "Step 729/1000, Cross-entropy Loss: 0.005061564035713673, LR: 8.000000000000001e-06\n",
      "Step 730/1000, Cross-entropy Loss: 0.005060884170234203, LR: 8.000000000000001e-06\n",
      "Step 731/1000, Cross-entropy Loss: 0.005060215480625629, LR: 8.000000000000001e-06\n",
      "Step 732/1000, Cross-entropy Loss: 0.0050595588982105255, LR: 8.000000000000001e-06\n",
      "Step 733/1000, Cross-entropy Loss: 0.005058891139924526, LR: 8.000000000000001e-06\n",
      "Step 734/1000, Cross-entropy Loss: 0.005058232694864273, LR: 8.000000000000001e-06\n",
      "Step 735/1000, Cross-entropy Loss: 0.005057556554675102, LR: 8.000000000000001e-06\n",
      "Step 736/1000, Cross-entropy Loss: 0.005056882277131081, LR: 8.000000000000001e-06\n",
      "Step 737/1000, Cross-entropy Loss: 0.005056202877312899, LR: 8.000000000000001e-06\n",
      "Step 738/1000, Cross-entropy Loss: 0.005055529065430164, LR: 8.000000000000001e-06\n",
      "Step 739/1000, Cross-entropy Loss: 0.005054866895079613, LR: 8.000000000000001e-06\n",
      "Step 740/1000, Cross-entropy Loss: 0.005054214037954807, LR: 8.000000000000001e-06\n",
      "Step 741/1000, Cross-entropy Loss: 0.005053592845797539, LR: 8.000000000000001e-06\n",
      "Step 742/1000, Cross-entropy Loss: 0.0050529614090919495, LR: 8.000000000000001e-06\n",
      "Step 743/1000, Cross-entropy Loss: 0.005052343942224979, LR: 8.000000000000001e-06\n",
      "Step 744/1000, Cross-entropy Loss: 0.005051713436841965, LR: 8.000000000000001e-06\n",
      "Step 745/1000, Cross-entropy Loss: 0.005051077343523502, LR: 8.000000000000001e-06\n",
      "Step 746/1000, Cross-entropy Loss: 0.005050443112850189, LR: 8.000000000000001e-06\n",
      "Step 747/1000, Cross-entropy Loss: 0.005049793049693108, LR: 8.000000000000001e-06\n",
      "Step 748/1000, Cross-entropy Loss: 0.0050491332076489925, LR: 8.000000000000001e-06\n",
      "Step 749/1000, Cross-entropy Loss: 0.005048476159572601, LR: 8.000000000000001e-06\n",
      "Step 750/1000, Cross-entropy Loss: 0.0050478121265769005, LR: 8.000000000000001e-06\n",
      "Step 751/1000, Cross-entropy Loss: 0.005047153681516647, LR: 8.000000000000001e-06\n",
      "Step 752/1000, Cross-entropy Loss: 0.005046514328569174, LR: 8.000000000000001e-06\n",
      "Step 753/1000, Cross-entropy Loss: 0.0050458661280572414, LR: 8.000000000000001e-06\n",
      "Step 754/1000, Cross-entropy Loss: 0.005045234225690365, LR: 8.000000000000001e-06\n",
      "Step 755/1000, Cross-entropy Loss: 0.005044601857662201, LR: 8.000000000000001e-06\n",
      "Step 756/1000, Cross-entropy Loss: 0.005043963901698589, LR: 8.000000000000001e-06\n",
      "Step 757/1000, Cross-entropy Loss: 0.0050433180294930935, LR: 8.000000000000001e-06\n",
      "Step 758/1000, Cross-entropy Loss: 0.005042684730142355, LR: 8.000000000000001e-06\n",
      "Step 759/1000, Cross-entropy Loss: 0.0050420439802110195, LR: 8.000000000000001e-06\n",
      "Step 760/1000, Cross-entropy Loss: 0.005041391588747501, LR: 8.000000000000001e-06\n",
      "Step 761/1000, Cross-entropy Loss: 0.005040747579187155, LR: 8.000000000000001e-06\n",
      "Step 762/1000, Cross-entropy Loss: 0.00504010496661067, LR: 8.000000000000001e-06\n",
      "Step 763/1000, Cross-entropy Loss: 0.005039452575147152, LR: 8.000000000000001e-06\n",
      "Step 764/1000, Cross-entropy Loss: 0.005038802046328783, LR: 8.000000000000001e-06\n",
      "Step 765/1000, Cross-entropy Loss: 0.005038160365074873, LR: 8.000000000000001e-06\n",
      "Step 766/1000, Cross-entropy Loss: 0.005037512164562941, LR: 8.000000000000001e-06\n",
      "Step 767/1000, Cross-entropy Loss: 0.005036868620663881, LR: 8.000000000000001e-06\n",
      "Step 768/1000, Cross-entropy Loss: 0.005036226473748684, LR: 8.000000000000001e-06\n",
      "Step 769/1000, Cross-entropy Loss: 0.005035593640059233, LR: 8.000000000000001e-06\n",
      "Step 770/1000, Cross-entropy Loss: 0.005034957081079483, LR: 8.000000000000001e-06\n",
      "Step 771/1000, Cross-entropy Loss: 0.005034302361309528, LR: 8.000000000000001e-06\n",
      "Step 772/1000, Cross-entropy Loss: 0.005033671855926514, LR: 8.000000000000001e-06\n",
      "Step 773/1000, Cross-entropy Loss: 0.005033027846366167, LR: 8.000000000000001e-06\n",
      "Step 774/1000, Cross-entropy Loss: 0.005032388959079981, LR: 8.000000000000001e-06\n",
      "Step 775/1000, Cross-entropy Loss: 0.005031746346503496, LR: 8.000000000000001e-06\n",
      "Step 776/1000, Cross-entropy Loss: 0.0050310976803302765, LR: 8.000000000000001e-06\n",
      "Step 777/1000, Cross-entropy Loss: 0.0050304485484957695, LR: 8.000000000000001e-06\n",
      "Step 778/1000, Cross-entropy Loss: 0.005029815714806318, LR: 8.000000000000001e-06\n",
      "Step 779/1000, Cross-entropy Loss: 0.005029161460697651, LR: 8.000000000000001e-06\n",
      "Step 780/1000, Cross-entropy Loss: 0.005028514191508293, LR: 8.000000000000001e-06\n",
      "Step 781/1000, Cross-entropy Loss: 0.005027871113270521, LR: 8.000000000000001e-06\n",
      "Step 782/1000, Cross-entropy Loss: 0.005027217790484428, LR: 8.000000000000001e-06\n",
      "Step 783/1000, Cross-entropy Loss: 0.005026576109230518, LR: 8.000000000000001e-06\n",
      "Step 784/1000, Cross-entropy Loss: 0.005025939550250769, LR: 8.000000000000001e-06\n",
      "Step 785/1000, Cross-entropy Loss: 0.0050252946093678474, LR: 8.000000000000001e-06\n",
      "Step 786/1000, Cross-entropy Loss: 0.005024644546210766, LR: 8.000000000000001e-06\n",
      "Step 787/1000, Cross-entropy Loss: 0.005024001467972994, LR: 8.000000000000001e-06\n",
      "Step 788/1000, Cross-entropy Loss: 0.005023352801799774, LR: 8.000000000000001e-06\n",
      "Step 789/1000, Cross-entropy Loss: 0.005022700410336256, LR: 8.000000000000001e-06\n",
      "Step 790/1000, Cross-entropy Loss: 0.005022057332098484, LR: 8.000000000000001e-06\n",
      "Step 791/1000, Cross-entropy Loss: 0.005021408200263977, LR: 8.000000000000001e-06\n",
      "Step 792/1000, Cross-entropy Loss: 0.005020757671445608, LR: 8.000000000000001e-06\n",
      "Step 793/1000, Cross-entropy Loss: 0.005020114593207836, LR: 8.000000000000001e-06\n",
      "Step 794/1000, Cross-entropy Loss: 0.0050194645300507545, LR: 8.000000000000001e-06\n",
      "Step 795/1000, Cross-entropy Loss: 0.0050188153982162476, LR: 8.000000000000001e-06\n",
      "Step 796/1000, Cross-entropy Loss: 0.005018168594688177, LR: 8.000000000000001e-06\n",
      "Step 797/1000, Cross-entropy Loss: 0.00501751434057951, LR: 8.000000000000001e-06\n",
      "Step 798/1000, Cross-entropy Loss: 0.0050168693996965885, LR: 8.000000000000001e-06\n",
      "Step 799/1000, Cross-entropy Loss: 0.005016217939555645, LR: 8.000000000000001e-06\n",
      "Step 800/1000, Cross-entropy Loss: 0.005015574395656586, LR: 8.000000000000001e-06\n",
      "Step 801/1000, Cross-entropy Loss: 0.005014922469854355, LR: 8.000000000000001e-06\n",
      "Step 802/1000, Cross-entropy Loss: 0.005014271475374699, LR: 8.000000000000001e-06\n",
      "Step 803/1000, Cross-entropy Loss: 0.005013620480895042, LR: 8.000000000000001e-06\n",
      "Step 804/1000, Cross-entropy Loss: 0.005012980196624994, LR: 8.000000000000001e-06\n",
      "Step 805/1000, Cross-entropy Loss: 0.00501231849193573, LR: 8.000000000000001e-06\n",
      "Step 806/1000, Cross-entropy Loss: 0.0050116549246013165, LR: 8.000000000000001e-06\n",
      "Step 807/1000, Cross-entropy Loss: 0.005011021159589291, LR: 8.000000000000001e-06\n",
      "Step 808/1000, Cross-entropy Loss: 0.005010364577174187, LR: 8.000000000000001e-06\n",
      "Step 809/1000, Cross-entropy Loss: 0.0050097061321139336, LR: 8.000000000000001e-06\n",
      "Step 810/1000, Cross-entropy Loss: 0.005009053274989128, LR: 8.000000000000001e-06\n",
      "Step 811/1000, Cross-entropy Loss: 0.005008403677493334, LR: 8.000000000000001e-06\n",
      "Step 812/1000, Cross-entropy Loss: 0.005007745698094368, LR: 8.000000000000001e-06\n",
      "Step 813/1000, Cross-entropy Loss: 0.005007095169275999, LR: 8.000000000000001e-06\n",
      "Step 814/1000, Cross-entropy Loss: 0.005006424617022276, LR: 8.000000000000001e-06\n",
      "Step 815/1000, Cross-entropy Loss: 0.005005776882171631, LR: 8.000000000000001e-06\n",
      "Step 816/1000, Cross-entropy Loss: 0.005005120765417814, LR: 8.000000000000001e-06\n",
      "Step 817/1000, Cross-entropy Loss: 0.005004464648663998, LR: 8.000000000000001e-06\n",
      "Step 818/1000, Cross-entropy Loss: 0.005003799218684435, LR: 8.000000000000001e-06\n",
      "Step 819/1000, Cross-entropy Loss: 0.005003142170608044, LR: 8.000000000000001e-06\n",
      "Step 820/1000, Cross-entropy Loss: 0.00500249769538641, LR: 8.000000000000001e-06\n",
      "Step 821/1000, Cross-entropy Loss: 0.005001843441277742, LR: 8.000000000000001e-06\n",
      "Step 822/1000, Cross-entropy Loss: 0.005001169629395008, LR: 8.000000000000001e-06\n",
      "Step 823/1000, Cross-entropy Loss: 0.005000516306608915, LR: 8.000000000000001e-06\n",
      "Step 824/1000, Cross-entropy Loss: 0.004999865777790546, LR: 8.000000000000001e-06\n",
      "Step 825/1000, Cross-entropy Loss: 0.0049991970881819725, LR: 8.000000000000001e-06\n",
      "Step 826/1000, Cross-entropy Loss: 0.004998545628041029, LR: 8.000000000000001e-06\n",
      "Step 827/1000, Cross-entropy Loss: 0.004997873678803444, LR: 8.000000000000001e-06\n",
      "Step 828/1000, Cross-entropy Loss: 0.004997200798243284, LR: 8.000000000000001e-06\n",
      "Step 829/1000, Cross-entropy Loss: 0.004996554926037788, LR: 8.000000000000001e-06\n",
      "Step 830/1000, Cross-entropy Loss: 0.004995884373784065, LR: 8.000000000000001e-06\n",
      "Step 831/1000, Cross-entropy Loss: 0.004995222669094801, LR: 8.000000000000001e-06\n",
      "Step 832/1000, Cross-entropy Loss: 0.0049945586360991, LR: 8.000000000000001e-06\n",
      "Step 833/1000, Cross-entropy Loss: 0.00499389786273241, LR: 8.000000000000001e-06\n",
      "Step 834/1000, Cross-entropy Loss: 0.004993238020688295, LR: 8.000000000000001e-06\n",
      "Step 835/1000, Cross-entropy Loss: 0.004992570262402296, LR: 8.000000000000001e-06\n",
      "Step 836/1000, Cross-entropy Loss: 0.0049919020384550095, LR: 8.000000000000001e-06\n",
      "Step 837/1000, Cross-entropy Loss: 0.004991242196410894, LR: 8.000000000000001e-06\n",
      "Step 838/1000, Cross-entropy Loss: 0.0049905674532055855, LR: 8.000000000000001e-06\n",
      "Step 839/1000, Cross-entropy Loss: 0.004989902023226023, LR: 8.000000000000001e-06\n",
      "Step 840/1000, Cross-entropy Loss: 0.004989250097423792, LR: 8.000000000000001e-06\n",
      "Step 841/1000, Cross-entropy Loss: 0.004988578613847494, LR: 8.000000000000001e-06\n",
      "Step 842/1000, Cross-entropy Loss: 0.0049879117868840694, LR: 8.000000000000001e-06\n",
      "Step 843/1000, Cross-entropy Loss: 0.004987246356904507, LR: 8.000000000000001e-06\n",
      "Step 844/1000, Cross-entropy Loss: 0.004986572079360485, LR: 8.000000000000001e-06\n",
      "Step 845/1000, Cross-entropy Loss: 0.004985899664461613, LR: 8.000000000000001e-06\n",
      "Step 846/1000, Cross-entropy Loss: 0.004985237494111061, LR: 8.000000000000001e-06\n",
      "Step 847/1000, Cross-entropy Loss: 0.004984562750905752, LR: 8.000000000000001e-06\n",
      "Step 848/1000, Cross-entropy Loss: 0.004983888007700443, LR: 8.000000000000001e-06\n",
      "Step 849/1000, Cross-entropy Loss: 0.004983227234333754, LR: 8.000000000000001e-06\n",
      "Step 850/1000, Cross-entropy Loss: 0.004982560873031616, LR: 8.000000000000001e-06\n",
      "Step 851/1000, Cross-entropy Loss: 0.004981883801519871, LR: 8.000000000000001e-06\n",
      "Step 852/1000, Cross-entropy Loss: 0.004981216974556446, LR: 8.000000000000001e-06\n",
      "Step 853/1000, Cross-entropy Loss: 0.00498055387288332, LR: 8.000000000000001e-06\n",
      "Step 854/1000, Cross-entropy Loss: 0.004979868419468403, LR: 8.000000000000001e-06\n",
      "Step 855/1000, Cross-entropy Loss: 0.004979184363037348, LR: 8.000000000000001e-06\n",
      "Step 856/1000, Cross-entropy Loss: 0.004978527314960957, LR: 8.000000000000001e-06\n",
      "Step 857/1000, Cross-entropy Loss: 0.004977855831384659, LR: 8.000000000000001e-06\n",
      "Step 858/1000, Cross-entropy Loss: 0.004977191332727671, LR: 8.000000000000001e-06\n",
      "Step 859/1000, Cross-entropy Loss: 0.004976504947990179, LR: 8.000000000000001e-06\n",
      "Step 860/1000, Cross-entropy Loss: 0.004975834395736456, LR: 8.000000000000001e-06\n",
      "Step 861/1000, Cross-entropy Loss: 0.004975163843482733, LR: 8.000000000000001e-06\n",
      "Step 862/1000, Cross-entropy Loss: 0.004974482115358114, LR: 8.000000000000001e-06\n",
      "Step 863/1000, Cross-entropy Loss: 0.004973800387233496, LR: 8.000000000000001e-06\n",
      "Step 864/1000, Cross-entropy Loss: 0.004973125644028187, LR: 8.000000000000001e-06\n",
      "Step 865/1000, Cross-entropy Loss: 0.004972441587597132, LR: 8.000000000000001e-06\n",
      "Step 866/1000, Cross-entropy Loss: 0.004971773829311132, LR: 8.000000000000001e-06\n",
      "Step 867/1000, Cross-entropy Loss: 0.004971095826476812, LR: 8.000000000000001e-06\n",
      "Step 868/1000, Cross-entropy Loss: 0.004970402456820011, LR: 8.000000000000001e-06\n",
      "Step 869/1000, Cross-entropy Loss: 0.004969739355146885, LR: 8.000000000000001e-06\n",
      "Step 870/1000, Cross-entropy Loss: 0.004969049245119095, LR: 8.000000000000001e-06\n",
      "Step 871/1000, Cross-entropy Loss: 0.004968376364558935, LR: 8.000000000000001e-06\n",
      "Step 872/1000, Cross-entropy Loss: 0.0049676913768053055, LR: 8.000000000000001e-06\n",
      "Step 873/1000, Cross-entropy Loss: 0.004967011045664549, LR: 8.000000000000001e-06\n",
      "Step 874/1000, Cross-entropy Loss: 0.004966328851878643, LR: 8.000000000000001e-06\n",
      "Step 875/1000, Cross-entropy Loss: 0.004965643864125013, LR: 8.000000000000001e-06\n",
      "Step 876/1000, Cross-entropy Loss: 0.0049649616703391075, LR: 8.000000000000001e-06\n",
      "Step 877/1000, Cross-entropy Loss: 0.004964278545230627, LR: 8.000000000000001e-06\n",
      "Step 878/1000, Cross-entropy Loss: 0.0049635907635092735, LR: 8.000000000000001e-06\n",
      "Step 879/1000, Cross-entropy Loss: 0.004962909035384655, LR: 8.000000000000001e-06\n",
      "Step 880/1000, Cross-entropy Loss: 0.004962226841598749, LR: 8.000000000000001e-06\n",
      "Step 881/1000, Cross-entropy Loss: 0.00496153999119997, LR: 8.000000000000001e-06\n",
      "Step 882/1000, Cross-entropy Loss: 0.004960849415510893, LR: 8.000000000000001e-06\n",
      "Step 883/1000, Cross-entropy Loss: 0.004960168153047562, LR: 8.000000000000001e-06\n",
      "Step 884/1000, Cross-entropy Loss: 0.00495947478339076, LR: 8.000000000000001e-06\n",
      "Step 885/1000, Cross-entropy Loss: 0.0049587879329919815, LR: 8.000000000000001e-06\n",
      "Step 886/1000, Cross-entropy Loss: 0.0049580964259803295, LR: 8.000000000000001e-06\n",
      "Step 887/1000, Cross-entropy Loss: 0.004957405850291252, LR: 8.000000000000001e-06\n",
      "Step 888/1000, Cross-entropy Loss: 0.004956717602908611, LR: 8.000000000000001e-06\n",
      "Step 889/1000, Cross-entropy Loss: 0.004956030752509832, LR: 8.000000000000001e-06\n",
      "Step 890/1000, Cross-entropy Loss: 0.004955343436449766, LR: 8.000000000000001e-06\n",
      "Step 891/1000, Cross-entropy Loss: 0.004954651463776827, LR: 8.000000000000001e-06\n",
      "Step 892/1000, Cross-entropy Loss: 0.004953951574862003, LR: 8.000000000000001e-06\n",
      "Step 893/1000, Cross-entropy Loss: 0.004953254945576191, LR: 8.000000000000001e-06\n",
      "Step 894/1000, Cross-entropy Loss: 0.004952573217451572, LR: 8.000000000000001e-06\n",
      "Step 895/1000, Cross-entropy Loss: 0.004951873794198036, LR: 8.000000000000001e-06\n",
      "Step 896/1000, Cross-entropy Loss: 0.004951189272105694, LR: 8.000000000000001e-06\n",
      "Step 897/1000, Cross-entropy Loss: 0.004950494039803743, LR: 8.000000000000001e-06\n",
      "Step 898/1000, Cross-entropy Loss: 0.004949796479195356, LR: 8.000000000000001e-06\n",
      "Step 899/1000, Cross-entropy Loss: 0.004949107766151428, LR: 8.000000000000001e-06\n",
      "Step 900/1000, Cross-entropy Loss: 0.004948410205543041, LR: 8.000000000000001e-06\n",
      "Step 901/1000, Cross-entropy Loss: 0.004947714041918516, LR: 8.000000000000001e-06\n",
      "Step 902/1000, Cross-entropy Loss: 0.0049470108933746815, LR: 8.000000000000001e-06\n",
      "Step 903/1000, Cross-entropy Loss: 0.004946312867105007, LR: 8.000000000000001e-06\n",
      "Step 904/1000, Cross-entropy Loss: 0.0049456204287707806, LR: 8.000000000000001e-06\n",
      "Step 905/1000, Cross-entropy Loss: 0.004944922402501106, LR: 8.000000000000001e-06\n",
      "Step 906/1000, Cross-entropy Loss: 0.004944224376231432, LR: 8.000000000000001e-06\n",
      "Step 907/1000, Cross-entropy Loss: 0.004943531937897205, LR: 8.000000000000001e-06\n",
      "Step 908/1000, Cross-entropy Loss: 0.004942825995385647, LR: 8.000000000000001e-06\n",
      "Step 909/1000, Cross-entropy Loss: 0.004942127037793398, LR: 8.000000000000001e-06\n",
      "Step 910/1000, Cross-entropy Loss: 0.004941411316394806, LR: 8.000000000000001e-06\n",
      "Step 911/1000, Cross-entropy Loss: 0.004940720275044441, LR: 8.000000000000001e-06\n",
      "Step 912/1000, Cross-entropy Loss: 0.0049400124698877335, LR: 8.000000000000001e-06\n",
      "Step 913/1000, Cross-entropy Loss: 0.004939318168908358, LR: 8.000000000000001e-06\n",
      "Step 914/1000, Cross-entropy Loss: 0.004938618745654821, LR: 8.000000000000001e-06\n",
      "Step 915/1000, Cross-entropy Loss: 0.0049379076808691025, LR: 8.000000000000001e-06\n",
      "Step 916/1000, Cross-entropy Loss: 0.0049372026696801186, LR: 8.000000000000001e-06\n",
      "Step 917/1000, Cross-entropy Loss: 0.004936490673571825, LR: 8.000000000000001e-06\n",
      "Step 918/1000, Cross-entropy Loss: 0.004935793578624725, LR: 8.000000000000001e-06\n",
      "Step 919/1000, Cross-entropy Loss: 0.004935078788548708, LR: 8.000000000000001e-06\n",
      "Step 920/1000, Cross-entropy Loss: 0.004934377036988735, LR: 8.000000000000001e-06\n",
      "Step 921/1000, Cross-entropy Loss: 0.004933665972203016, LR: 8.000000000000001e-06\n",
      "Step 922/1000, Cross-entropy Loss: 0.004932955838739872, LR: 8.000000000000001e-06\n",
      "Step 923/1000, Cross-entropy Loss: 0.004932251758873463, LR: 8.000000000000001e-06\n",
      "Step 924/1000, Cross-entropy Loss: 0.004931542091071606, LR: 8.000000000000001e-06\n",
      "Step 925/1000, Cross-entropy Loss: 0.004930826835334301, LR: 8.000000000000001e-06\n",
      "Step 926/1000, Cross-entropy Loss: 0.004930123221129179, LR: 8.000000000000001e-06\n",
      "Step 927/1000, Cross-entropy Loss: 0.004929396789520979, LR: 8.000000000000001e-06\n",
      "Step 928/1000, Cross-entropy Loss: 0.004928689915686846, LR: 8.000000000000001e-06\n",
      "Step 929/1000, Cross-entropy Loss: 0.004927987232804298, LR: 8.000000000000001e-06\n",
      "Step 930/1000, Cross-entropy Loss: 0.004927271511405706, LR: 8.000000000000001e-06\n",
      "Step 931/1000, Cross-entropy Loss: 0.004926560912281275, LR: 8.000000000000001e-06\n",
      "Step 932/1000, Cross-entropy Loss: 0.004925842396914959, LR: 8.000000000000001e-06\n",
      "Step 933/1000, Cross-entropy Loss: 0.004925127141177654, LR: 8.000000000000001e-06\n",
      "Step 934/1000, Cross-entropy Loss: 0.004924410954117775, LR: 8.000000000000001e-06\n",
      "Step 935/1000, Cross-entropy Loss: 0.004923696629703045, LR: 8.000000000000001e-06\n",
      "Step 936/1000, Cross-entropy Loss: 0.00492298137396574, LR: 8.000000000000001e-06\n",
      "Step 937/1000, Cross-entropy Loss: 0.004922270309180021, LR: 8.000000000000001e-06\n",
      "Step 938/1000, Cross-entropy Loss: 0.004921550862491131, LR: 8.000000000000001e-06\n",
      "Step 939/1000, Cross-entropy Loss: 0.00492082629352808, LR: 8.000000000000001e-06\n",
      "Step 940/1000, Cross-entropy Loss: 0.004920107312500477, LR: 8.000000000000001e-06\n",
      "Step 941/1000, Cross-entropy Loss: 0.004919387400150299, LR: 8.000000000000001e-06\n",
      "Step 942/1000, Cross-entropy Loss: 0.004918660502880812, LR: 8.000000000000001e-06\n",
      "Step 943/1000, Cross-entropy Loss: 0.004917942453175783, LR: 8.000000000000001e-06\n",
      "Step 944/1000, Cross-entropy Loss: 0.0049172271974384785, LR: 8.000000000000001e-06\n",
      "Step 945/1000, Cross-entropy Loss: 0.004916500765830278, LR: 8.000000000000001e-06\n",
      "Step 946/1000, Cross-entropy Loss: 0.00491577060893178, LR: 8.000000000000001e-06\n",
      "Step 947/1000, Cross-entropy Loss: 0.004915057681500912, LR: 8.000000000000001e-06\n",
      "Step 948/1000, Cross-entropy Loss: 0.0049143326468765736, LR: 8.000000000000001e-06\n",
      "Step 949/1000, Cross-entropy Loss: 0.004913597833365202, LR: 8.000000000000001e-06\n",
      "Step 950/1000, Cross-entropy Loss: 0.004912867210805416, LR: 8.000000000000001e-06\n",
      "Step 951/1000, Cross-entropy Loss: 0.004912147764116526, LR: 8.000000000000001e-06\n",
      "Step 952/1000, Cross-entropy Loss: 0.004911419469863176, LR: 8.000000000000001e-06\n",
      "Step 953/1000, Cross-entropy Loss: 0.004910691641271114, LR: 8.000000000000001e-06\n",
      "Step 954/1000, Cross-entropy Loss: 0.004909956827759743, LR: 8.000000000000001e-06\n",
      "Step 955/1000, Cross-entropy Loss: 0.004909234121441841, LR: 8.000000000000001e-06\n",
      "Step 956/1000, Cross-entropy Loss: 0.004908506292849779, LR: 8.000000000000001e-06\n",
      "Step 957/1000, Cross-entropy Loss: 0.00490777101367712, LR: 8.000000000000001e-06\n",
      "Step 958/1000, Cross-entropy Loss: 0.004907038528472185, LR: 8.000000000000001e-06\n",
      "Step 959/1000, Cross-entropy Loss: 0.004906309302896261, LR: 8.000000000000001e-06\n",
      "Step 960/1000, Cross-entropy Loss: 0.00490557961165905, LR: 8.000000000000001e-06\n",
      "Step 961/1000, Cross-entropy Loss: 0.004904844332486391, LR: 8.000000000000001e-06\n",
      "Step 962/1000, Cross-entropy Loss: 0.004904116503894329, LR: 8.000000000000001e-06\n",
      "Step 963/1000, Cross-entropy Loss: 0.004903371911495924, LR: 8.000000000000001e-06\n",
      "Step 964/1000, Cross-entropy Loss: 0.004902641288936138, LR: 8.000000000000001e-06\n",
      "Step 965/1000, Cross-entropy Loss: 0.004901902750134468, LR: 8.000000000000001e-06\n",
      "Step 966/1000, Cross-entropy Loss: 0.004901169799268246, LR: 8.000000000000001e-06\n",
      "Step 967/1000, Cross-entropy Loss: 0.0049004387110471725, LR: 8.000000000000001e-06\n",
      "Step 968/1000, Cross-entropy Loss: 0.004899693187326193, LR: 8.000000000000001e-06\n",
      "Step 969/1000, Cross-entropy Loss: 0.00489896209910512, LR: 8.000000000000001e-06\n",
      "Step 970/1000, Cross-entropy Loss: 0.004898217041045427, LR: 8.000000000000001e-06\n",
      "Step 971/1000, Cross-entropy Loss: 0.004897483624517918, LR: 8.000000000000001e-06\n",
      "Step 972/1000, Cross-entropy Loss: 0.004896737169474363, LR: 8.000000000000001e-06\n",
      "Step 973/1000, Cross-entropy Loss: 0.004896001890301704, LR: 8.000000000000001e-06\n",
      "Step 974/1000, Cross-entropy Loss: 0.004895263351500034, LR: 8.000000000000001e-06\n",
      "Step 975/1000, Cross-entropy Loss: 0.004894515965133905, LR: 8.000000000000001e-06\n",
      "Step 976/1000, Cross-entropy Loss: 0.004893783014267683, LR: 8.000000000000001e-06\n",
      "Step 977/1000, Cross-entropy Loss: 0.004893036559224129, LR: 8.000000000000001e-06\n",
      "Step 978/1000, Cross-entropy Loss: 0.004892298020422459, LR: 8.000000000000001e-06\n",
      "Step 979/1000, Cross-entropy Loss: 0.004891547374427319, LR: 8.000000000000001e-06\n",
      "Step 980/1000, Cross-entropy Loss: 0.004890806507319212, LR: 8.000000000000001e-06\n",
      "Step 981/1000, Cross-entropy Loss: 0.004890063777565956, LR: 8.000000000000001e-06\n",
      "Step 982/1000, Cross-entropy Loss: 0.004889317788183689, LR: 8.000000000000001e-06\n",
      "Step 983/1000, Cross-entropy Loss: 0.004888578783720732, LR: 8.000000000000001e-06\n",
      "Step 984/1000, Cross-entropy Loss: 0.004887822084128857, LR: 8.000000000000001e-06\n",
      "Step 985/1000, Cross-entropy Loss: 0.00488709332421422, LR: 8.000000000000001e-06\n",
      "Step 986/1000, Cross-entropy Loss: 0.004886339418590069, LR: 8.000000000000001e-06\n",
      "Step 987/1000, Cross-entropy Loss: 0.004885585978627205, LR: 8.000000000000001e-06\n",
      "Step 988/1000, Cross-entropy Loss: 0.004884852096438408, LR: 8.000000000000001e-06\n",
      "Step 989/1000, Cross-entropy Loss: 0.004884104244410992, LR: 8.000000000000001e-06\n",
      "Step 990/1000, Cross-entropy Loss: 0.004883353598415852, LR: 8.000000000000001e-06\n",
      "Step 991/1000, Cross-entropy Loss: 0.0048826043494045734, LR: 8.000000000000001e-06\n",
      "Step 992/1000, Cross-entropy Loss: 0.004881854634732008, LR: 8.000000000000001e-06\n",
      "Step 993/1000, Cross-entropy Loss: 0.0048810942098498344, LR: 8.000000000000001e-06\n",
      "Step 994/1000, Cross-entropy Loss: 0.0048803482204675674, LR: 8.000000000000001e-06\n",
      "Step 995/1000, Cross-entropy Loss: 0.004879600368440151, LR: 8.000000000000001e-06\n",
      "Step 996/1000, Cross-entropy Loss: 0.004878845065832138, LR: 8.000000000000001e-06\n",
      "Step 997/1000, Cross-entropy Loss: 0.004878106992691755, LR: 8.000000000000001e-06\n",
      "Step 998/1000, Cross-entropy Loss: 0.0048773447051644325, LR: 8.000000000000001e-06\n",
      "Step 999/1000, Cross-entropy Loss: 0.004876595921814442, LR: 8.000000000000001e-06\n",
      "Step 1000/1000, Cross-entropy Loss: 0.004875838290899992, LR: 8.000000000000001e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example config:\n",
    "batch_size = 10\n",
    "sequence_len = 128\n",
    "num_steps = 1000\n",
    "train_inputs, train_targets, _, _ = get_dataset(10, sequence_len, 0)\n",
    "config = GPTConfig(\n",
    "    vocab_size=tokenizer.n_vocab,\n",
    "    n_layer=4,   # fewer layers for a quick demo\n",
    "    n_head=4,\n",
    "    n_embd=128,\n",
    "    seq_len=sequence_len,\n",
    ")\n",
    "\n",
    "loss_dict={\n",
    "    \"AUM_micro\": Proposed_AUM_micro,\n",
    "    \"AUM_macro\": Proposed_AUM_macro,\n",
    "    \"Cross-entropy\": F.cross_entropy\n",
    "}\n",
    "\n",
    "# Define Scheduler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "\n",
    "for name,func in loss_dict.items():\n",
    "    i = 1\n",
    "    model = GPTModel(config)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.2, patience=20, min_lr=5e-6, threshold=1e-4)\n",
    "    losses = []\n",
    "    while i < num_steps:\n",
    "        for j in range(0, len(train_inputs), batch_size):\n",
    "            x = train_inputs[j:j+batch_size]\n",
    "            y = train_targets[j:j+batch_size]\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(x)\n",
    "            loss = func(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            \n",
    "\n",
    "            loss = loss.item()\n",
    "            scheduler.step(loss)\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            \n",
    "            print(f\"Step {i+1}/{num_steps}, {name} Loss: {loss}, LR: {lr}\")\n",
    "            i += 1\n",
    "    plt.clf()\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Training Loss per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig(f\"{name} training_loss.png\", dpi=300)\n",
    "    torch.save(model, f\"{name}_model_pretrain.pth\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dfd49481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   director Takeshi Ozawa . A large team of writers handled the script . The game 's opening\n"
     ]
    }
   ],
   "source": [
    "def inference(prompt, max_new_tokens,model):\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    for _ in range(max_new_tokens):\n",
    "        num_tokens = len(tokens)\n",
    "        tokens_padded = tokens + [tokenizer.eot_token] * (config.seq_len - num_tokens)\n",
    "        tokens_padded = torch.tensor(tokens_padded).unsqueeze(0).to(device)\n",
    "        logits = model(tokens_padded)\n",
    "        predicted_token = torch.argmax(logits[0, num_tokens-1, :]).item()\n",
    "        tokens.append(predicted_token)\n",
    "    return tokenizer.decode(tokens)\n",
    "    \n",
    "print(\"Original: \", tokenizer.decode(train_inputs[2].tolist())[:90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "238419f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nou-z\\AppData\\Local\\Temp\\ipykernel_123164\\3312788504.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model=torch.load(f\"{name}_model_pretrain.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted for AUM_micro:  director Takeshi Ozawa . A large team of III enemy III III III III III in III III III III III III III\n",
      "Predicted for AUM_macro:  director Takeshi Ozawa . A large team of prim prim prim prim prim prim prim prim prim prim prim prim prim prim prim\n",
      "Predicted for Cross-entropy:  director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May\n",
      "Original:   director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by M\n"
     ]
    }
   ],
   "source": [
    "model_dict={}\n",
    "loss_dict={\n",
    "    \"AUM_micro\": Proposed_AUM_micro,\n",
    "    \"AUM_macro\": Proposed_AUM_macro,\n",
    "    \"Cross-entropy\": F.cross_entropy\n",
    "}\n",
    "\n",
    "for name , _ in loss_dict.items():\n",
    "    model=torch.load(f\"{name}_model_pretrain.pth\")\n",
    "    model_dict[name]=model\n",
    "    print(f\"Predicted for {name}:\", inference(\" director Takeshi Ozawa . A large team of\", max_new_tokens=15,model=model))\n",
    "print(\"Original: \", tokenizer.decode(train_inputs[2].tolist())[:110])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dafd3de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC_micro for AUM_micro:0.9955000281333923\n",
      "AUC_micro for AUM_macro:0.5537813305854797\n",
      "AUC_micro for Cross-entropy:0.9738080501556396\n"
     ]
    }
   ],
   "source": [
    "train_inputs.view(-1, train_inputs.size(-1))\n",
    "for name, loss_fn in loss_dict.items():\n",
    "    model=model_dict[name]\n",
    "    logits=model(train_inputs)\n",
    "    print(f\"AUC_micro for {name}:{ROC_AUC_micro(logits.view(-1, logits.size(-1)),train_inputs.view(-1))}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cb854c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC_macro for AUM_micro:0.9969483613967896\n",
      "AUC_macro for AUM_macro:0.997461199760437\n",
      "AUC_macro for Cross-entropy:0.9978855848312378\n"
     ]
    }
   ],
   "source": [
    "for name, loss_fn in loss_dict.items():\n",
    "    model=model_dict[name]\n",
    "    logits=model(train_inputs)\n",
    "    print(f\"AUC_macro for {name}:{ROC_AUC_macro(logits.view(-1, logits.size(-1)),train_inputs.view(-1))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b75bfdc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   27,  7700,    29,  ..., 27677,   837,   262],\n",
       "        [ 4539, 10730,   284,  ..., 18354,  7496, 17740],\n",
       "        [ 3437, 33687,  5303,  ...,   351,   262,  2478],\n",
       "        ...,\n",
       "        [ 6093, 31447,   326,  ...,   543,   389,  9387],\n",
       "        [ 1936,  1180, 12608,  ...,   262,  4876,   837],\n",
       "        [ 1431,   416,   511,  ...,  4632,   360,  5605]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
