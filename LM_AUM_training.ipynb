{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba2824bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nou-z\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecbf6daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50257\n",
      "End of text token: 50256\n",
      "Example tokenization: [15496, 995, 0]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\") # Get the same tokenizer used for GPT-2\n",
    "\n",
    "\n",
    "print(\"Vocabulary size:\", tokenizer.n_vocab) # Vocabilary size is how many unique tokens the tokenizer can encode\n",
    "print(\"End of text token:\", tokenizer.eot_token) # End of text token is used to indicate the end of a text sequence\n",
    "print(\"Example tokenization:\", tokenizer.encode(\"Hello world!\"))\n",
    "\n",
    "# Convert entire dataset into a single string\n",
    "# This dataset is small enough to fit into memory\n",
    "# For larger datasets, you may need to use more \n",
    "# sophisticated methods to process the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39fec469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# A simple configuration container\n",
    "class GPTConfig:\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size,  # size of the vocabulary, from tokenizer, for gpt2 tokenizer it is 50257\n",
    "        n_layer,   # number of transformer blocks\n",
    "        n_head,    # number of attention heads for each transformer block\n",
    "        n_embd,  # embedding dimension for each token\n",
    "        seq_len,  # sequence length for the model - e.g. the \"context window\" \n",
    "    \n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.seq_len = seq_len\n",
    "     \n",
    "test_config = GPTConfig(\n",
    "    vocab_size=tokenizer.n_vocab,\n",
    "    n_layer=2,  \n",
    "    n_head=3,\n",
    "    n_embd=6,\n",
    "    seq_len=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0c10ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position encoding shape: torch.Size([1, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "def get_position_encoding(seq_len, d, n=10000):\n",
    "    \"\"\"\n",
    "    Computes the positional encoding matrix of shape (seq_len, d).\n",
    "    \n",
    "    Args:\n",
    "        seq_len (int): Length of the sequence.\n",
    "        d (int): Dimension of the embedding.\n",
    "        n (float): The base for the exponential term (default 10000 in many Transformer implementations).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (seq_len, d) containing the positional encodings.\n",
    "    \"\"\"\n",
    "    \n",
    "    P = torch.zeros(seq_len, d).to(device)\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, d // 2):\n",
    "            P[pos, 2 * i] = math.sin(pos / (n ** ((2 * i) / d)))\n",
    "            if i + 1 < d:\n",
    "                P[pos, 2* i + 1] = math.cos(pos / (n ** ((2 * i) / d)))\n",
    "\n",
    "    return P.unsqueeze(0)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "position_encoding = get_position_encoding(seq_len=test_config.seq_len, d=test_config.n_embd)\n",
    "print(\"Position encoding shape:\", position_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aa070fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.Wq = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Query weights - will transform input embeddings into queries\n",
    "        self.Wk = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Key weights - will transform input embeddings into keys\n",
    "        self.Wv = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Value weights - will transform input embeddings into values\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Attention input shape:\", x.shape)\n",
    "        print(\"\")\n",
    "        print(\"Query weights shape:\", self.Wq.shape)\n",
    "        print(\"Key weights shape:\", self.Wk.shape)\n",
    "        print(\"Value weights shape:\", self.Wv.shape)\n",
    "        queries = x @ self.Wq # Matrix multiplication to transform input embeddings into queries\n",
    "        keys = x @ self.Wk # Matrix multiplication to transform input embeddings into keys\n",
    "        values = x @ self.Wv # Matrix multiplication to transform input embeddings into values\n",
    "        print(\"\")\n",
    "        print(\"Queries shape:\", queries.shape)\n",
    "        print(\"Keys shape:\", keys.shape)\n",
    "        print(\"Values shape:\", values.shape)\n",
    "\n",
    "        qkt = queries @ keys.transpose(-2, -1) # Calculate QK^T\n",
    "        qkt_scaled = qkt / math.sqrt(queries.size(-1)) # Scale QK^T by the dimension of the keys\n",
    "        qkt_softmax = F.softmax(qkt_scaled, dim=-1) # Apply softmax row-wise to get attention weights\n",
    "        print(\"\")\n",
    "        print(\"QK^T shape:\", qkt.shape)\n",
    "\n",
    "        attn_output = qkt_softmax @ values # Multiply softmax(QK^T) by values\n",
    "        print(\"\")\n",
    "        print(\"Attention output shape:\", attn_output.shape)\n",
    "        return attn_output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b19357b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.Wq = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Query weights - will transform input embeddings into queries\n",
    "        self.Wk = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Key weights - will transform input embeddings into keys\n",
    "        self.Wv = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Value weights - will transform input embeddings into values\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1] # Get sequence length (number of tokens / context window length)\n",
    "        queries = x @ self.Wq # Matrix multiplication to transform input embeddings into queries\n",
    "        keys = x @ self.Wk    # Matrix multiplication to transform input embeddings into keys\n",
    "        values = x @ self.Wv  # Matrix multiplication to transform input embeddings into values\n",
    "        qkt = queries @ keys.transpose(-2, -1)  # Calculate QK^T\n",
    "        qkt_scaled = qkt / math.sqrt(queries.size(-1))  # Scale QK^T by the dimension of the keys\n",
    "\n",
    "        # MASKING\n",
    "        # THIS IS THE ONLY DIFFERENCE, USE -inf FOR UPPER TRIANGLE MASK SO THAT SOFTMAX WILL BE 0\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1)\n",
    "        causal_mask = causal_mask.masked_fill(causal_mask == 1, float('-inf'))  # Upper triangle masked with -inf \n",
    "        qkt_scaled = qkt_scaled + causal_mask # Add the mask to the scaled QK^T\n",
    "        # END MASKING\n",
    "\n",
    "        qkt_softmax = F.softmax(qkt_scaled, dim=-1) # Apply softmax row-wise to get attention weights, the -inf values will become 0 here\n",
    "        attn_output = qkt_softmax @ values # Multiply softmax(QK^T) by values\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12972c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn_heads = nn.ModuleList([\n",
    "            CausalSelfAttention(config) for _ in range(config.n_head)\n",
    "        ])  # Create n_head attention heads\n",
    "        self.projection = nn.Linear(config.n_embd * config.n_head, config.n_embd).to(device) # Linear layer to project multi-head attention outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        head_outputs = [head(x) for head in self.attn_heads] # Get the output of each attention head\n",
    "        multihead_output = torch.cat(head_outputs, dim=-1) # Concatenate the outputs\n",
    "        return self.projection(multihead_output) # Project the concatenated outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e759e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(config)\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd).to(device)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "        ).to(device)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c11fbe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.n_embd).to(device)\n",
    "        self.position_encoding = get_position_encoding(config.seq_len, config.n_embd)\n",
    "        self.blocks = nn.Sequential(*[GPTBlock(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd).to(device)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.token_embedding(x) + self.position_encoding\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36ea8969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_curve_micro(pred_tensor, label_tensor):\n",
    "    device=pred_tensor.device\n",
    "    n_class=pred_tensor.size(1)\n",
    "    one_hot_labels = F.one_hot(label_tensor, num_classes=n_class).to(device)\n",
    "    is_positive = one_hot_labels\n",
    "    is_negative =1-one_hot_labels\n",
    "    fn_diff = -is_positive.flatten()\n",
    "    fp_diff = is_negative.flatten()\n",
    "    thresh_tensor = -pred_tensor.flatten()\n",
    "    fn_denom = is_positive.sum()\n",
    "    fp_denom = is_negative.sum()\n",
    "    sorted_indices = torch.argsort(thresh_tensor)\n",
    "    sorted_fp_cum = fp_diff[sorted_indices].cumsum(0) / fp_denom\n",
    "    sorted_fn_cum = -fn_diff[sorted_indices].flip(0).cumsum(0).flip(0) / fn_denom\n",
    "\n",
    "    sorted_thresh = thresh_tensor[sorted_indices]\n",
    "    sorted_is_diff = sorted_thresh.diff() != 0\n",
    "    sorted_fp_end = torch.cat([sorted_is_diff, torch.tensor([True],device=device)])\n",
    "    sorted_fn_end = torch.cat([torch.tensor([True],device=device), sorted_is_diff])\n",
    "\n",
    "    uniq_thresh = sorted_thresh[sorted_fp_end]\n",
    "    uniq_fp_after = sorted_fp_cum[sorted_fp_end]\n",
    "    uniq_fn_before = sorted_fn_cum[sorted_fn_end]\n",
    "\n",
    "    FPR = torch.cat([torch.tensor([0.0],device=device), uniq_fp_after])\n",
    "    FNR = torch.cat([uniq_fn_before, torch.tensor([0.0],device=device)])\n",
    "\n",
    "    return {\n",
    "        \"FPR\": FPR,\n",
    "        \"FNR\": FNR,\n",
    "        \"TPR\": 1 - FNR,\n",
    "        \"min(FPR,FNR)\": torch.minimum(FPR, FNR),\n",
    "        \"min_constant\": torch.cat([torch.tensor([-1],device=device), uniq_thresh]),\n",
    "        \"max_constant\": torch.cat([uniq_thresh, torch.tensor([0],device=device)])\n",
    "    }\n",
    "def ROC_AUC_micro(pred_tensor, label_tensor):\n",
    "    roc = ROC_curve_micro(pred_tensor, label_tensor)\n",
    "    FPR_diff = roc[\"FPR\"][1:]-roc[\"FPR\"][:-1]   \n",
    "    TPR_sum = roc[\"TPR\"][1:]+roc[\"TPR\"][:-1]\n",
    "    return torch.sum(FPR_diff*TPR_sum/2.0)\n",
    "#AUM \n",
    "def Proposed_AUM_micro(pred_tensor, label_tensor):\n",
    "\n",
    "    roc = ROC_curve_micro(pred_tensor, label_tensor)\n",
    "    min_FPR_FNR = roc[\"min(FPR,FNR)\"][1:-1]\n",
    "    constant_diff = roc[\"min_constant\"][1:].diff()\n",
    "    return torch.sum(min_FPR_FNR * constant_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab91b166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_curve_macro(pred_tensor, label_tensor):\n",
    "    n_class=pred_tensor.size(1)\n",
    "    one_hot_labels = F.one_hot(label_tensor, num_classes=n_class)\n",
    "    is_positive = one_hot_labels\n",
    "    is_negative =1-one_hot_labels\n",
    "    fn_diff = -is_positive\n",
    "    fp_diff = is_negative\n",
    "    thresh_tensor = -pred_tensor\n",
    "    fn_denom = is_positive.sum(dim=0).clamp(min=1)\n",
    "    fp_denom = is_negative.sum(dim=0).clamp(min=1)\n",
    "    sorted_indices = torch.argsort(thresh_tensor,dim=0)\n",
    "    sorted_fp_cum = torch.div(torch.gather(fp_diff, dim=0, index=sorted_indices).cumsum(0), fp_denom)\n",
    "    sorted_fn_cum = -torch.div(torch.gather(fn_diff, dim=0, index=sorted_indices).flip(0).cumsum(0).flip(0) , fn_denom)\n",
    "    sorted_thresh = torch.gather(thresh_tensor, dim=0, index=sorted_indices)\n",
    "    #Problem starts here \n",
    "    zeros_vec=torch.zeros(1,n_class,device=device)\n",
    "    FPR = torch.cat([zeros_vec, sorted_fp_cum])\n",
    "    FNR = torch.cat([sorted_fn_cum, zeros_vec])\n",
    "    return {\n",
    "        \"FPR_all_classes\": FPR,\n",
    "        \"FNR_all_classes\": FNR,\n",
    "        \"TPR_all_classes\": 1 - FNR,\n",
    "        \"min(FPR,FNR)\": torch.minimum(FPR, FNR),\n",
    "        \"min_constant\": torch.cat([-torch.ones(1,n_class,device=device), sorted_thresh]),\n",
    "        \"max_constant\": torch.cat([sorted_thresh, zeros_vec])\n",
    "    }\n",
    "\n",
    "def ROC_AUC_macro(pred_tensor, label_tensor):\n",
    "    roc = ROC_curve_macro(pred_tensor, label_tensor)\n",
    "    FPR_diff = roc[\"FPR_all_classes\"][1:,:]-roc[\"FPR_all_classes\"][:-1,]\n",
    "    TPR_sum = roc[\"TPR_all_classes\"][1:,:]+roc[\"TPR_all_classes\"][:-1,:]\n",
    "    sum_FPR_TPR= torch.sum(FPR_diff*TPR_sum/2.0,dim=0)\n",
    "    count_non_defined=(sum_FPR_TPR == 0).sum()\n",
    "    if count_non_defined==pred_tensor.size(1):\n",
    "        return 0\n",
    "    return  sum_FPR_TPR.sum()/(pred_tensor.size(1)-count_non_defined)\n",
    "def Proposed_AUM_macro(pred_tensor, label_tensor):\n",
    "\n",
    "    roc = ROC_curve_macro(pred_tensor, label_tensor)\n",
    "    min_FPR_FNR = roc[\"min(FPR,FNR)\"][1:-1,:]\n",
    "    constant_diff = roc[\"min_constant\"][1:,:].diff(dim=0)\n",
    "    sum_min= torch.sum(min_FPR_FNR * constant_diff,dim=0)\n",
    "    count_non_defined=(sum_min== 0).sum()\n",
    "    if count_non_defined==pred_tensor.size(1):\n",
    "        return torch.tensor(0,device=pred_tensor.device)\n",
    "    return  sum_min.sum()/(pred_tensor.size(1)-count_non_defined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99a3c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "sequence_len = 256\n",
    "num_steps = 300\n",
    "config = GPTConfig(\n",
    "    vocab_size=tokenizer.n_vocab,\n",
    "    n_layer=4,   # fewer layers for a quick demo\n",
    "    n_head=4,\n",
    "    n_embd=128,\n",
    "    seq_len=sequence_len,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca7878ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example config:\n",
    "batch_size = 10\n",
    "sequence_len = 256\n",
    "num_steps = 1000\n",
    "config = GPTConfig(\n",
    "    vocab_size=tokenizer.n_vocab,\n",
    "    n_layer=4,   # fewer layers for a quick demo\n",
    "    n_head=4,\n",
    "    n_embd=128,\n",
    "    seq_len=sequence_len,\n",
    ")\n",
    "\n",
    "loss_dict={\n",
    "    \"AUM_micro\": Proposed_AUM_micro,\n",
    "    \"AUM_macro\": Proposed_AUM_macro,\n",
    "    \"Cross-entropy\": F.cross_entropy\n",
    "}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfd49481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(prompt, max_new_tokens,model):\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    for _ in range(max_new_tokens):\n",
    "        num_tokens = len(tokens)\n",
    "        tokens_padded = tokens + [tokenizer.eot_token] * (config.seq_len - num_tokens)\n",
    "        tokens_padded = torch.tensor(tokens_padded).unsqueeze(0).to(device)\n",
    "        logits = model(tokens_padded)\n",
    "        predicted_token = torch.argmax(logits[0, num_tokens-1, :]).item()\n",
    "        tokens.append(predicted_token)\n",
    "    return tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "238419f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nou-z\\AppData\\Local\\Temp\\ipykernel_9676\\1075855561.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model=torch.load(f\"{name}_model.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted for AUM_macro: He loved to play with his slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped slipped\n",
      "Predicted for Cross-entropy: He loved to play with his. He was so happy, he was so happy to the park. He was so happy to the end, but he was so he was so he was so he was so he was so he was so he was so he was so he was so he was so he was so he was so he was so he was so he was so he was so he was so he was so he was so he was so he was so he was so he was so he was so he was so he was\n",
      "Predicted for AUM_micro: He loved to play with his shorthand accents Bonds worlds drinkersovery worldsophobicsend Bonds Dir scrolls scrolls scrolls scrolls scrolls scrolls scrolls scrolls scrolls Bonds Bonds Bonds Bonds Bonds Bonds Bonds Bonds Bonds Bonds Bonds Bonds Bonds Bonds Bonds Bonds Bonds Bonds Bonds Bonds Bonds Bonds Bondsakin Dir scrolls worlds worlds worlds worlds Bonds worlds Bonds Dir scrolls scrolls scrolls scrolls scrolls scrolls scrolls scrolls scrolls scrolls scrolls worlds Bonds Dir scrolls lists scrolls lists outreach worlds worlds scrolls lists scrollsophobicophobicophobicophobicophobicophobicophobicophobicophobicophobicophobicophobic NAACP outreach worldsakinIowaakinby worldsiner worlds\n"
     ]
    }
   ],
   "source": [
    "model_dict={}\n",
    "loss_dict={\n",
    "    \"AUM_macro\": Proposed_AUM_macro,\n",
    "    \"Cross-entropy\": F.cross_entropy,\n",
    "    \"AUM_micro\": Proposed_AUM_micro\n",
    "}\n",
    "\n",
    "for name , _ in loss_dict.items():\n",
    "    model=torch.load(f\"{name}_model.pt\")\n",
    "    model_dict[name]=model\n",
    "    print(f\"Predicted for {name}:\", inference(\"He loved to play with his\", max_new_tokens=100,model=model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddf2daaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 21990\n",
      "})\n",
      "{'text': 'Once upon a time, there was a little boy named Tom. He loved to play with his red ball. One sunny day, Tom went outside to play with his ball in the land near his home.\\n\\nTom kicked the ball high in the sky. The ball went far, far away. Tom was sad because he could not find his ball. He walked and walked, looking for it. The land was big and sometimes dangerous. Tom knew he had to be careful.\\n\\nAt last, Tom found his ball near a big tree. He was very happy. Tom knew he should not kick the ball too hard next time. He went back home, holding his ball tightly. Tom played safely in his yard, away from the dangerous land.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"Tinystories_valid\")\n",
    "print(dataset)\n",
    "print(dataset[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c955fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0f947c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 4 examples [00:00, 202.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenize_and_chunk(dataset, tokenizer, chunk_size=256, train_rows=1000):\n",
    "    \"\"\"\n",
    "    Tokenizes and chunks the dataset into fixed-length 512-token segments.\n",
    "    The 'target' sequence is shifted left by 1 token.\n",
    "    Stops after generating `train_rows + test_rows` tokenized chunks.\n",
    "    \"\"\"\n",
    "    buffer = []  # Rolling buffer for tokens\n",
    "    row_count = 0\n",
    "\n",
    "    for example in dataset:\n",
    "        tokens = tokenizer(example[\"text\"], truncation=False, padding=False)['input_ids']\n",
    "        buffer.extend(tokens)\n",
    "\n",
    "        # Yield full chunks until we reach train_rows + test_rows\n",
    "        while len(buffer) >= chunk_size + 1:  # +1 to ensure we can shift target\n",
    "            if row_count >= train_rows:\n",
    "                return  # Stop yielding once enough rows are reached\n",
    "\n",
    "            # Create input-target pairs\n",
    "            input_chunk = buffer[:chunk_size]         # First 512 tokens\n",
    "            target_chunk = buffer[1:chunk_size + 1]  # Shifted by 1 token\n",
    "        \n",
    "\n",
    "            yield {\n",
    "                \"input\": input_chunk, \n",
    "                \"target\": target_chunk\n",
    "            }\n",
    "            \n",
    "            buffer = buffer[chunk_size:]  # Remove used tokens\n",
    "            row_count += 1\n",
    "tokenized_ds = datasets.Dataset.from_generator(lambda: tokenize_and_chunk(dataset, hf_tokenizer,train_rows=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "605e9b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(tokenized_ds['input']).flatten()\n",
    "input_tensor=torch.tensor(tokenized_ds['input'],device=device)\n",
    "label_tensor=torch.tensor(tokenized_ds['target'],device=device).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f700be77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nou-z\\AppData\\Local\\Temp\\ipykernel_9676\\1596273322.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model=torch.load(f\"{name}_model.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE for AUM_macro model: 10.82929515838623\n",
      "AUC_macro for AUM_macro:0.9973055720329285 \n",
      "AUC_micro for AUM_macro: 0.4979124963283539\n",
      "CE for Cross-entropy model: 3.6480677127838135\n",
      "AUC_macro for Cross-entropy:0.9996455907821655 \n",
      "AUC_micro for Cross-entropy: 0.9979820251464844\n",
      "CE for AUM_micro model: 10.807943344116211\n",
      "AUC_macro for AUM_micro:0.9973668456077576 \n",
      "AUC_micro for AUM_micro: 0.8082376718521118\n"
     ]
    }
   ],
   "source": [
    "for name , loss_fn in loss_dict.items():\n",
    "    model=torch.load(f\"{name}_model.pt\")\n",
    "    pred_tensor=model(input_tensor)\n",
    "    print(f\"CE for {name} model: {F.cross_entropy(pred_tensor.view(-1, pred_tensor.size(-1)),label_tensor.view(-1))}\")\n",
    "    print(f\"AUC_macro for {name}:{ROC_AUC_macro(pred_tensor.view(-1, pred_tensor.size(-1)),label_tensor.view(-1))} \")\n",
    "    print(f\"AUC_micro for {name}: {ROC_AUC_micro(pred_tensor.view(-1, pred_tensor.size(-1)),label_tensor.view(-1))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35dc97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nou-z\\AppData\\Local\\Temp\\ipykernel_9676\\1961044060.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model=torch.load(f\"{name}_model.pt\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m pred_tensor\u001b[38;5;241m=\u001b[39mmodel(input_tensor)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Scikit AUC_macro for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroc_auc_score(\u001b[43mlabel_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,pred_tensor\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;250m \u001b[39mpred_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(),average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m,multi_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "for name , loss_fn in loss_dict.items():\n",
    "    model=torch.load(f\"{name}_model.pt\")\n",
    "    pred_tensor=model(input_tensor)\n",
    "    print(f\" Scikit AUC_macro for {name}:{roc_auc_score(label_tensor.view(-1).cpu().numpy(),pred_tensor.view(-1, pred_tensor.size(-1)).detach().cpu().numpy(),average='macro',multi_class='ovr')} \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
