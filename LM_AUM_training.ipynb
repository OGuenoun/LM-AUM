{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba2824bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nou-z\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecbf6daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50257\n",
      "End of text token: 50256\n",
      "Example tokenization: [15496, 995, 0]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\") # Get the same tokenizer used for GPT-2\n",
    "\n",
    "\n",
    "print(\"Vocabulary size:\", tokenizer.n_vocab) # Vocabilary size is how many unique tokens the tokenizer can encode\n",
    "print(\"End of text token:\", tokenizer.eot_token) # End of text token is used to indicate the end of a text sequence\n",
    "print(\"Example tokenization:\", tokenizer.encode(\"Hello world!\"))\n",
    "\n",
    "# Convert entire dataset into a single string\n",
    "# This dataset is small enough to fit into memory\n",
    "# For larger datasets, you may need to use more \n",
    "# sophisticated methods to process the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39fec469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# A simple configuration container\n",
    "class GPTConfig:\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size,  # size of the vocabulary, from tokenizer, for gpt2 tokenizer it is 50257\n",
    "        n_layer,   # number of transformer blocks\n",
    "        n_head,    # number of attention heads for each transformer block\n",
    "        n_embd,  # embedding dimension for each token\n",
    "        seq_len,  # sequence length for the model - e.g. the \"context window\" \n",
    "    \n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.seq_len = seq_len\n",
    "     \n",
    "test_config = GPTConfig(\n",
    "    vocab_size=tokenizer.n_vocab,\n",
    "    n_layer=2,  \n",
    "    n_head=3,\n",
    "    n_embd=6,\n",
    "    seq_len=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0c10ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position encoding shape: torch.Size([1, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "def get_position_encoding(seq_len, d, n=10000):\n",
    "    \"\"\"\n",
    "    Computes the positional encoding matrix of shape (seq_len, d).\n",
    "    \n",
    "    Args:\n",
    "        seq_len (int): Length of the sequence.\n",
    "        d (int): Dimension of the embedding.\n",
    "        n (float): The base for the exponential term (default 10000 in many Transformer implementations).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (seq_len, d) containing the positional encodings.\n",
    "    \"\"\"\n",
    "    \n",
    "    P = torch.zeros(seq_len, d).to(device)\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, d // 2):\n",
    "            P[pos, 2 * i] = math.sin(pos / (n ** ((2 * i) / d)))\n",
    "            if i + 1 < d:\n",
    "                P[pos, 2* i + 1] = math.cos(pos / (n ** ((2 * i) / d)))\n",
    "\n",
    "    return P.unsqueeze(0)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "position_encoding = get_position_encoding(seq_len=test_config.seq_len, d=test_config.n_embd)\n",
    "print(\"Position encoding shape:\", position_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aa070fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.Wq = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Query weights - will transform input embeddings into queries\n",
    "        self.Wk = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Key weights - will transform input embeddings into keys\n",
    "        self.Wv = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Value weights - will transform input embeddings into values\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Attention input shape:\", x.shape)\n",
    "        print(\"\")\n",
    "        print(\"Query weights shape:\", self.Wq.shape)\n",
    "        print(\"Key weights shape:\", self.Wk.shape)\n",
    "        print(\"Value weights shape:\", self.Wv.shape)\n",
    "        queries = x @ self.Wq # Matrix multiplication to transform input embeddings into queries\n",
    "        keys = x @ self.Wk # Matrix multiplication to transform input embeddings into keys\n",
    "        values = x @ self.Wv # Matrix multiplication to transform input embeddings into values\n",
    "        print(\"\")\n",
    "        print(\"Queries shape:\", queries.shape)\n",
    "        print(\"Keys shape:\", keys.shape)\n",
    "        print(\"Values shape:\", values.shape)\n",
    "\n",
    "        qkt = queries @ keys.transpose(-2, -1) # Calculate QK^T\n",
    "        qkt_scaled = qkt / math.sqrt(queries.size(-1)) # Scale QK^T by the dimension of the keys\n",
    "        qkt_softmax = F.softmax(qkt_scaled, dim=-1) # Apply softmax row-wise to get attention weights\n",
    "        print(\"\")\n",
    "        print(\"QK^T shape:\", qkt.shape)\n",
    "\n",
    "        attn_output = qkt_softmax @ values # Multiply softmax(QK^T) by values\n",
    "        print(\"\")\n",
    "        print(\"Attention output shape:\", attn_output.shape)\n",
    "        return attn_output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b19357b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.Wq = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Query weights - will transform input embeddings into queries\n",
    "        self.Wk = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Key weights - will transform input embeddings into keys\n",
    "        self.Wv = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Value weights - will transform input embeddings into values\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1] # Get sequence length (number of tokens / context window length)\n",
    "        queries = x @ self.Wq # Matrix multiplication to transform input embeddings into queries\n",
    "        keys = x @ self.Wk    # Matrix multiplication to transform input embeddings into keys\n",
    "        values = x @ self.Wv  # Matrix multiplication to transform input embeddings into values\n",
    "        qkt = queries @ keys.transpose(-2, -1)  # Calculate QK^T\n",
    "        qkt_scaled = qkt / math.sqrt(queries.size(-1))  # Scale QK^T by the dimension of the keys\n",
    "\n",
    "        # MASKING\n",
    "        # THIS IS THE ONLY DIFFERENCE, USE -inf FOR UPPER TRIANGLE MASK SO THAT SOFTMAX WILL BE 0\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1)\n",
    "        causal_mask = causal_mask.masked_fill(causal_mask == 1, float('-inf'))  # Upper triangle masked with -inf \n",
    "        qkt_scaled = qkt_scaled + causal_mask # Add the mask to the scaled QK^T\n",
    "        # END MASKING\n",
    "\n",
    "        qkt_softmax = F.softmax(qkt_scaled, dim=-1) # Apply softmax row-wise to get attention weights, the -inf values will become 0 here\n",
    "        attn_output = qkt_softmax @ values # Multiply softmax(QK^T) by values\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12972c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn_heads = nn.ModuleList([\n",
    "            CausalSelfAttention(config) for _ in range(config.n_head)\n",
    "        ])  # Create n_head attention heads\n",
    "        self.projection = nn.Linear(config.n_embd * config.n_head, config.n_embd).to(device) # Linear layer to project multi-head attention outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        head_outputs = [head(x) for head in self.attn_heads] # Get the output of each attention head\n",
    "        multihead_output = torch.cat(head_outputs, dim=-1) # Concatenate the outputs\n",
    "        return self.projection(multihead_output) # Project the concatenated outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e759e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(config)\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd).to(device)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "        ).to(device)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c11fbe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.n_embd).to(device)\n",
    "        self.position_encoding = get_position_encoding(config.seq_len, config.n_embd)\n",
    "        self.blocks = nn.Sequential(*[GPTBlock(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd).to(device)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.token_embedding(x) + self.position_encoding\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits= self.head(x) \n",
    "        return torch.softmax(logits, dim=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36ea8969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_curve_micro(pred_tensor, label_tensor):\n",
    "    device=pred_tensor.device\n",
    "    n_class=pred_tensor.size(1)\n",
    "    one_hot_labels = F.one_hot(label_tensor, num_classes=n_class).to(device)\n",
    "    is_positive = one_hot_labels\n",
    "    is_negative =1-one_hot_labels\n",
    "    fn_diff = -is_positive.flatten()\n",
    "    fp_diff = is_negative.flatten()\n",
    "    thresh_tensor = -pred_tensor.flatten()\n",
    "    fn_denom = is_positive.sum()\n",
    "    fp_denom = is_negative.sum()\n",
    "    sorted_indices = torch.argsort(thresh_tensor)\n",
    "    sorted_fp_cum = fp_diff[sorted_indices].cumsum(0) / fp_denom\n",
    "    sorted_fn_cum = -fn_diff[sorted_indices].flip(0).cumsum(0).flip(0) / fn_denom\n",
    "\n",
    "    sorted_thresh = thresh_tensor[sorted_indices]\n",
    "    sorted_is_diff = sorted_thresh.diff() != 0\n",
    "    sorted_fp_end = torch.cat([sorted_is_diff, torch.tensor([True],device=device)])\n",
    "    sorted_fn_end = torch.cat([torch.tensor([True],device=device), sorted_is_diff])\n",
    "\n",
    "    uniq_thresh = sorted_thresh[sorted_fp_end]\n",
    "    uniq_fp_after = sorted_fp_cum[sorted_fp_end]\n",
    "    uniq_fn_before = sorted_fn_cum[sorted_fn_end]\n",
    "\n",
    "    FPR = torch.cat([torch.tensor([0.0],device=device), uniq_fp_after])\n",
    "    FNR = torch.cat([uniq_fn_before, torch.tensor([0.0],device=device)])\n",
    "\n",
    "    return {\n",
    "        \"FPR\": FPR,\n",
    "        \"FNR\": FNR,\n",
    "        \"TPR\": 1 - FNR,\n",
    "        \"min(FPR,FNR)\": torch.minimum(FPR, FNR),\n",
    "        \"min_constant\": torch.cat([torch.tensor([-1],device=device), uniq_thresh]),\n",
    "        \"max_constant\": torch.cat([uniq_thresh, torch.tensor([0],device=device)])\n",
    "    }\n",
    "def ROC_AUC_micro(pred_tensor, label_tensor):\n",
    "    roc = ROC_curve_micro(pred_tensor, label_tensor)\n",
    "    FPR_diff = roc[\"FPR\"][1:]-roc[\"FPR\"][:-1]   \n",
    "    TPR_sum = roc[\"TPR\"][1:]+roc[\"TPR\"][:-1]\n",
    "    return torch.sum(FPR_diff*TPR_sum/2.0)\n",
    "#AUM \n",
    "def Proposed_AUM_micro(pred_tensor, label_tensor):\n",
    "\n",
    "    roc = ROC_curve_micro(pred_tensor, label_tensor)\n",
    "    min_FPR_FNR = roc[\"min(FPR,FNR)\"][1:-1]\n",
    "    constant_diff = roc[\"min_constant\"][1:].diff()\n",
    "    return torch.sum(min_FPR_FNR * constant_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab91b166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_curve_macro(pred_tensor, label_tensor):\n",
    "    n_class=pred_tensor.size(1)\n",
    "    one_hot_labels = F.one_hot(label_tensor, num_classes=n_class)\n",
    "    is_positive = one_hot_labels\n",
    "    is_negative =1-one_hot_labels\n",
    "    fn_diff = -is_positive\n",
    "    fp_diff = is_negative\n",
    "    thresh_tensor = -pred_tensor\n",
    "    fn_denom = is_positive.sum(dim=0).clamp(min=1)\n",
    "    fp_denom = is_negative.sum(dim=0).clamp(min=1)\n",
    "    sorted_indices = torch.argsort(thresh_tensor,dim=0)\n",
    "    sorted_fp_cum = torch.div(torch.gather(fp_diff, dim=0, index=sorted_indices).cumsum(0), fp_denom)\n",
    "    sorted_fn_cum = -torch.div(torch.gather(fn_diff, dim=0, index=sorted_indices).flip(0).cumsum(0).flip(0) , fn_denom)\n",
    "    sorted_thresh = torch.gather(thresh_tensor, dim=0, index=sorted_indices)\n",
    "    #Problem starts here \n",
    "    zeros_vec=torch.zeros(1,n_class,device=device)\n",
    "    FPR = torch.cat([zeros_vec, sorted_fp_cum])\n",
    "    FNR = torch.cat([sorted_fn_cum, zeros_vec])\n",
    "    return {\n",
    "        \"FPR_all_classes\": FPR,\n",
    "        \"FNR_all_classes\": FNR,\n",
    "        \"TPR_all_classes\": 1 - FNR,\n",
    "        \"min(FPR,FNR)\": torch.minimum(FPR, FNR),\n",
    "        \"min_constant\": torch.cat([-torch.ones(1,n_class,device=device), sorted_thresh]),\n",
    "        \"max_constant\": torch.cat([sorted_thresh, zeros_vec])\n",
    "    }\n",
    "\n",
    "def ROC_AUC_macro(pred_tensor, label_tensor):\n",
    "    roc = ROC_curve_macro(pred_tensor, label_tensor)\n",
    "    unique_labs = torch.unique(label_tensor, return_counts=False)\n",
    "    mask = torch.zeros(pred_tensor.size(1), dtype=torch.bool)\n",
    "    mask[unique_labs] = True\n",
    "    FPR_diff = roc[\"FPR_all_classes\"][1:,:]-roc[\"FPR_all_classes\"][:-1,]\n",
    "    TPR_sum = roc[\"TPR_all_classes\"][1:,:]+roc[\"TPR_all_classes\"][:-1,:]\n",
    "    sum_FPR_TPR= torch.sum(FPR_diff*TPR_sum/2.0,dim=0)\n",
    "    return  sum_FPR_TPR[mask].mean()\n",
    "def Proposed_AUM_macro(pred_tensor, label_tensor):\n",
    "    roc = ROC_curve_macro(pred_tensor, label_tensor)\n",
    "    unique_labs = torch.unique(label_tensor, return_counts=False)\n",
    "    mask = torch.zeros(pred_tensor.size(1), dtype=torch.bool)\n",
    "    mask[unique_labs] = True\n",
    "    min_FPR_FNR = roc[\"min(FPR,FNR)\"][1:-1,:]\n",
    "    constant_diff = roc[\"min_constant\"][1:,:].diff(dim=0)\n",
    "    sum_min= torch.sum(min_FPR_FNR * constant_diff,dim=0)\n",
    "    return  sum_min[mask].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99a3c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "sequence_len = 256\n",
    "num_steps = 300\n",
    "config = GPTConfig(\n",
    "    vocab_size=tokenizer.n_vocab,\n",
    "    n_layer=4,   # fewer layers for a quick demo\n",
    "    n_head=4,\n",
    "    n_embd=128,\n",
    "    seq_len=sequence_len,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca7878ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example config:\n",
    "batch_size = 10\n",
    "sequence_len = 256\n",
    "num_steps = 1000\n",
    "config = GPTConfig(\n",
    "    vocab_size=tokenizer.n_vocab,\n",
    "    n_layer=4,   # fewer layers for a quick demo\n",
    "    n_head=4,\n",
    "    n_embd=128,\n",
    "    seq_len=sequence_len,\n",
    ")\n",
    "\n",
    "loss_dict={\n",
    "    \"AUM_micro\": Proposed_AUM_micro,\n",
    "    \"AUM_macro\": Proposed_AUM_macro,\n",
    "    \"Cross-entropy\": F.cross_entropy\n",
    "}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfd49481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(prompt, max_new_tokens,model):\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    for _ in range(max_new_tokens):\n",
    "        num_tokens = len(tokens)\n",
    "        tokens_padded = tokens + [tokenizer.eot_token] * (config.seq_len - num_tokens)\n",
    "        tokens_padded = torch.tensor(tokens_padded).unsqueeze(0).to(device)\n",
    "        logits = model(tokens_padded)\n",
    "        predicted_token = torch.argmax(logits[0, num_tokens-1, :]).item()\n",
    "        tokens.append(predicted_token)\n",
    "    return tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "238419f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nou-z\\AppData\\Local\\Temp\\ipykernel_42508\\1563250963.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model=torch.load(f\"{name}_model.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted for AUM_macro: Once upon a time,visorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisorvisor\n",
      "Predicted for Cross-entropy: Once upon a time, there was a little girl named Lily. She loved to play with her mom. One day, Lily's mommy's mommy. Lily's mommy's mommy's mommy's mommy's mommy told her mommy. Lily's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy's mommy\n",
      "Predicted for AUM_micro: Once upon a time, Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake Earthquake\n"
     ]
    }
   ],
   "source": [
    "model_dict={}\n",
    "loss_dict={\n",
    "    \"AUM_macro\": Proposed_AUM_macro,\n",
    "    \"Cross-entropy\": F.cross_entropy,\n",
    "    \"AUM_micro\": Proposed_AUM_micro\n",
    "}\n",
    "\n",
    "for name , _ in loss_dict.items():\n",
    "    model=torch.load(f\"{name}_model.pt\")\n",
    "    model_dict[name]=model\n",
    "    print(f\"Predicted for {name}:\", inference(\"Once upon a time,\", max_new_tokens=100,model=model).replace(\"\\n\", \"\\\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddf2daaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 21990\n",
      "})\n",
      "{'text': 'Once upon a time, there was a little boy named Tom. He loved to play with his red ball. One sunny day, Tom went outside to play with his ball in the land near his home.\\n\\nTom kicked the ball high in the sky. The ball went far, far away. Tom was sad because he could not find his ball. He walked and walked, looking for it. The land was big and sometimes dangerous. Tom knew he had to be careful.\\n\\nAt last, Tom found his ball near a big tree. He was very happy. Tom knew he should not kick the ball too hard next time. He went back home, holding his ball tightly. Tom played safely in his yard, away from the dangerous land.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"Tinystories_valid\")\n",
    "print(dataset)\n",
    "print(dataset[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c955fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0f947c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 5 examples [00:00, 131.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenize_and_chunk(dataset, tokenizer, chunk_size=256, train_rows=1000):\n",
    "    \"\"\"\n",
    "    Tokenizes and chunks the dataset into fixed-length 512-token segments.\n",
    "    The 'target' sequence is shifted left by 1 token.\n",
    "    Stops after generating `train_rows + test_rows` tokenized chunks.\n",
    "    \"\"\"\n",
    "    buffer = []  # Rolling buffer for tokens\n",
    "    row_count = 0\n",
    "\n",
    "    for example in dataset:\n",
    "        tokens = tokenizer(example[\"text\"], truncation=False, padding=False)['input_ids']\n",
    "        buffer.extend(tokens)\n",
    "\n",
    "        # Yield full chunks until we reach train_rows + test_rows\n",
    "        while len(buffer) >= chunk_size + 1:  # +1 to ensure we can shift target\n",
    "            if row_count >= train_rows:\n",
    "                return  # Stop yielding once enough rows are reached\n",
    "\n",
    "            # Create input-target pairs\n",
    "            input_chunk = buffer[:chunk_size]         # First 512 tokens\n",
    "            target_chunk = buffer[1:chunk_size + 1]  # Shifted by 1 token\n",
    "        \n",
    "\n",
    "            yield {\n",
    "                \"input\": input_chunk, \n",
    "                \"target\": target_chunk\n",
    "            }\n",
    "            \n",
    "            buffer = buffer[chunk_size:]  # Remove used tokens\n",
    "            row_count += 1\n",
    "tokenized_ds = datasets.Dataset.from_generator(lambda: tokenize_and_chunk(dataset, hf_tokenizer,train_rows=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "605e9b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(tokenized_ds['input']).flatten()\n",
    "input_tensor=torch.tensor(tokenized_ds['input'],device=device)\n",
    "label_tensor=torch.tensor(tokenized_ds['target'],device=device).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f700be77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nou-z\\AppData\\Local\\Temp\\ipykernel_42508\\2186435337.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model=torch.load(f\"{name}_model.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class for AUM_macro:  tensor([13131, 13131, 13131,  ..., 13131, 13131, 13131], device='cuda:0')\n",
      "True class:  tensor([   13, 15899,  2497,  ...,   290,  6807,    11], device='cuda:0')\n",
      "AUC_macro for AUM_macro:0.5108242034912109 \n",
      "AUC_micro for AUM_macro: 0.02108372375369072\n",
      "Predicted class for Cross-entropy:  tensor([  13, 1119,  290,  ..., 1497, 2497, 1497], device='cuda:0')\n",
      "True class:  tensor([   13, 15899,  2497,  ...,   290,  6807,    11], device='cuda:0')\n",
      "AUC_macro for Cross-entropy:0.9502571225166321 \n",
      "AUC_micro for Cross-entropy: 0.9988473057746887\n",
      "Predicted class for AUM_micro:  tensor([45591, 45591, 45591,  ..., 45591, 45591, 45591], device='cuda:0')\n",
      "True class:  tensor([   13, 15899,  2497,  ...,   290,  6807,    11], device='cuda:0')\n",
      "AUC_macro for AUM_micro:0.5089581608772278 \n",
      "AUC_micro for AUM_micro: 0.7694441080093384\n"
     ]
    }
   ],
   "source": [
    "for name , loss_fn in loss_dict.items():\n",
    "    model=torch.load(f\"{name}_model.pt\")\n",
    "    pred_tensor=model(input_tensor)\n",
    "    val,ind=pred_tensor.view(-1, pred_tensor.size(-1)).max(dim=1)\n",
    "    print(f\"Predicted class for {name}: \",ind)\n",
    "    print(\"True class: \",label_tensor.view(-1))\n",
    "    print(f\"AUC_macro for {name}:{ROC_AUC_macro(pred_tensor.view(-1, pred_tensor.size(-1)),label_tensor.view(-1))} \")\n",
    "    print(f\"AUC_micro for {name}: {ROC_AUC_micro(pred_tensor.view(-1, pred_tensor.size(-1)),label_tensor.view(-1))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c35dc97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nou-z\\AppData\\Local\\Temp\\ipykernel_42508\\3617698701.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model=torch.load(f\"{name}_model.pt\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes in y_true not equal to the number of columns in 'y_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m pred_tensor\u001b[38;5;241m=\u001b[39mmodel(input_tensor)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Scikit AUC_macro for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpred_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mpred_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43movr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nou-z\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\nou-z\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:634\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m multi_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_class must be in (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movo\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_multiclass_roc_auc_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    638\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_true)\n",
      "File \u001b[1;32mc:\\Users\\nou-z\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:751\u001b[0m, in \u001b[0;36m_multiclass_roc_auc_score\u001b[1;34m(y_true, y_score, labels, multi_class, average, sample_weight)\u001b[0m\n\u001b[0;32m    749\u001b[0m     classes \u001b[38;5;241m=\u001b[39m _unique(y_true)\n\u001b[0;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(classes) \u001b[38;5;241m!=\u001b[39m y_score\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 751\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    752\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes in y_true not equal to the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    753\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_score\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    754\u001b[0m         )\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124movo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Number of classes in y_true not equal to the number of columns in 'y_score'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "for name , loss_fn in loss_dict.items():\n",
    "    model=torch.load(f\"{name}_model.pt\")\n",
    "    pred_tensor=model(input_tensor)\n",
    "    print(f\" Scikit AUC_macro for {name}:{roc_auc_score(label_tensor.view(-1).cpu().numpy(),pred_tensor.view(-1, pred_tensor.size(-1)).detach().cpu().numpy(),average='macro',multi_class='ovr')} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe67954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
