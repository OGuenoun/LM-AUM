{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba2824bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e65c5af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"EleutherAI/wikitext_document_level\", \"wikitext-2-raw-v1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecbf6daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50257\n",
      "End of text token: 50256\n",
      "Example tokenization: [15496, 995, 0]\n",
      "Input Shape torch.Size([2, 4])\n",
      "Output Shape torch.Size([2, 4])\n",
      "Input Example:\n",
      "tensor([[   27,  7700,    29,   220],\n",
      "        [  569, 18354,  7496, 17740]], device='cuda:0')\n",
      "Output Example:\n",
      "tensor([[ 7700,    29,   220,   796],\n",
      "        [18354,  7496, 17740,  6711]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\") # Get the same tokenizer used for GPT-2\n",
    "\n",
    "\n",
    "print(\"Vocabulary size:\", tokenizer.n_vocab) # Vocabilary size is how many unique tokens the tokenizer can encode\n",
    "print(\"End of text token:\", tokenizer.eot_token) # End of text token is used to indicate the end of a text sequence\n",
    "print(\"Example tokenization:\", tokenizer.encode(\"Hello world!\"))\n",
    "\n",
    "# Convert entire dataset into a single string\n",
    "# This dataset is small enough to fit into memory\n",
    "# For larger datasets, you may need to use more \n",
    "# sophisticated methods to process the data.\n",
    "\n",
    "all_text = \"\"\n",
    "all_data = dataset[\"page\"]\n",
    "for example in all_data:\n",
    "    all_text += \"<page> \"+ example + \" </page>\"\n",
    "\n",
    "# Tokenize the entire text at once\n",
    "tokenized_text = tokenizer.encode(all_text)\n",
    "\n",
    "\n",
    "# We will create a function that generates a dataset of examples\n",
    "# for the language model. The function will take in the number of\n",
    "# examples to generate, the block size, and the test split.\n",
    "# It will return the training and test datasets.\n",
    "def get_dataset(num_examples, context_window_length, test_split=0.1):\n",
    "    input_blocks = [] # List to store input sequences\n",
    "    target_blocks = [] # List to store target sequences\n",
    "\n",
    "    # Use a sliding window to create input/target sequences\n",
    "    for i in range(0, len(tokenized_text), context_window_length + 1):\n",
    "        block = tokenized_text[i:i+context_window_length+ 1]\n",
    "        \n",
    "        # Skip blocks that are too short\n",
    "        if len(block) < context_window_length + 1:\n",
    "            continue\n",
    "\n",
    "        input_seq = block[:-1]  \n",
    "        target_seq = block[1:]  \n",
    "\n",
    "        input_blocks.append(input_seq)\n",
    "        target_blocks.append(target_seq)\n",
    "        \n",
    "        # Stop if we have enough examples\n",
    "        if len(input_blocks) >= num_examples:\n",
    "            break\n",
    "\n",
    "    # Convert to tensors for pytorch and move to gpu\n",
    "    inputs = torch.tensor(input_blocks, dtype=torch.long).to(device)\n",
    "    targets = torch.tensor(target_blocks, dtype=torch.long).to(device)\n",
    "\n",
    "    # Calculate train/test split point\n",
    "    split_idx = int(num_examples * (1 - test_split))\n",
    "\n",
    "    # Split into train/test\n",
    "    train_inputs = inputs[:split_idx]\n",
    "    train_targets = targets[:split_idx]\n",
    "    test_inputs = inputs[split_idx:]\n",
    "    test_targets = targets[split_idx:]\n",
    "    return train_inputs, train_targets, test_inputs, test_targets\n",
    "\n",
    "# Get a small dataset\n",
    "i, o, _, _ = get_dataset(2, 4, 0)\n",
    "print(\"Input Shape\", i.shape)\n",
    "print(\"Output Shape\", o.shape)\n",
    "print(\"Input Example:\")\n",
    "print(i)\n",
    "print(\"Output Example:\")\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39fec469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# A simple configuration container\n",
    "class GPTConfig:\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size,  # size of the vocabulary, from tokenizer, for gpt2 tokenizer it is 50257\n",
    "        n_layer,   # number of transformer blocks\n",
    "        n_head,    # number of attention heads for each transformer block\n",
    "        n_embd,  # embedding dimension for each token\n",
    "        seq_len,  # sequence length for the model - e.g. the \"context window\" \n",
    "    \n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.seq_len = seq_len\n",
    "     \n",
    "test_config = GPTConfig(\n",
    "    vocab_size=tokenizer.n_vocab,\n",
    "    n_layer=2,  \n",
    "    n_head=3,\n",
    "    n_embd=6,\n",
    "    seq_len=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0c10ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position encoding shape: torch.Size([1, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "def get_position_encoding(seq_len, d, n=10000):\n",
    "    \"\"\"\n",
    "    Computes the positional encoding matrix of shape (seq_len, d).\n",
    "    \n",
    "    Args:\n",
    "        seq_len (int): Length of the sequence.\n",
    "        d (int): Dimension of the embedding.\n",
    "        n (float): The base for the exponential term (default 10000 in many Transformer implementations).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (seq_len, d) containing the positional encodings.\n",
    "    \"\"\"\n",
    "    \n",
    "    P = torch.zeros(seq_len, d).to(device)\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, d // 2):\n",
    "            P[pos, 2 * i] = math.sin(pos / (n ** ((2 * i) / d)))\n",
    "            if i + 1 < d:\n",
    "                P[pos, 2* i + 1] = math.cos(pos / (n ** ((2 * i) / d)))\n",
    "\n",
    "    return P.unsqueeze(0)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "position_encoding = get_position_encoding(seq_len=test_config.seq_len, d=test_config.n_embd)\n",
    "print(\"Position encoding shape:\", position_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7aa070fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.Wq = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Query weights - will transform input embeddings into queries\n",
    "        self.Wk = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Key weights - will transform input embeddings into keys\n",
    "        self.Wv = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Value weights - will transform input embeddings into values\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Attention input shape:\", x.shape)\n",
    "        print(\"\")\n",
    "        print(\"Query weights shape:\", self.Wq.shape)\n",
    "        print(\"Key weights shape:\", self.Wk.shape)\n",
    "        print(\"Value weights shape:\", self.Wv.shape)\n",
    "        queries = x @ self.Wq # Matrix multiplication to transform input embeddings into queries\n",
    "        keys = x @ self.Wk # Matrix multiplication to transform input embeddings into keys\n",
    "        values = x @ self.Wv # Matrix multiplication to transform input embeddings into values\n",
    "        print(\"\")\n",
    "        print(\"Queries shape:\", queries.shape)\n",
    "        print(\"Keys shape:\", keys.shape)\n",
    "        print(\"Values shape:\", values.shape)\n",
    "\n",
    "        qkt = queries @ keys.transpose(-2, -1) # Calculate QK^T\n",
    "        qkt_scaled = qkt / math.sqrt(queries.size(-1)) # Scale QK^T by the dimension of the keys\n",
    "        qkt_softmax = F.softmax(qkt_scaled, dim=-1) # Apply softmax row-wise to get attention weights\n",
    "        print(\"\")\n",
    "        print(\"QK^T shape:\", qkt.shape)\n",
    "\n",
    "        attn_output = qkt_softmax @ values # Multiply softmax(QK^T) by values\n",
    "        print(\"\")\n",
    "        print(\"Attention output shape:\", attn_output.shape)\n",
    "        return attn_output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b19357b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.Wq = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Query weights - will transform input embeddings into queries\n",
    "        self.Wk = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Key weights - will transform input embeddings into keys\n",
    "        self.Wv = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Value weights - will transform input embeddings into values\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1] # Get sequence length (number of tokens / context window length)\n",
    "        queries = x @ self.Wq # Matrix multiplication to transform input embeddings into queries\n",
    "        keys = x @ self.Wk    # Matrix multiplication to transform input embeddings into keys\n",
    "        values = x @ self.Wv  # Matrix multiplication to transform input embeddings into values\n",
    "        qkt = queries @ keys.transpose(-2, -1)  # Calculate QK^T\n",
    "        qkt_scaled = qkt / math.sqrt(queries.size(-1))  # Scale QK^T by the dimension of the keys\n",
    "\n",
    "        # MASKING\n",
    "        # THIS IS THE ONLY DIFFERENCE, USE -inf FOR UPPER TRIANGLE MASK SO THAT SOFTMAX WILL BE 0\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1)\n",
    "        causal_mask = causal_mask.masked_fill(causal_mask == 1, float('-inf'))  # Upper triangle masked with -inf \n",
    "        qkt_scaled = qkt_scaled + causal_mask # Add the mask to the scaled QK^T\n",
    "        # END MASKING\n",
    "\n",
    "        qkt_softmax = F.softmax(qkt_scaled, dim=-1) # Apply softmax row-wise to get attention weights, the -inf values will become 0 here\n",
    "        attn_output = qkt_softmax @ values # Multiply softmax(QK^T) by values\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12972c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn_heads = nn.ModuleList([\n",
    "            CausalSelfAttention(config) for _ in range(config.n_head)\n",
    "        ])  # Create n_head attention heads\n",
    "        self.projection = nn.Linear(config.n_embd * config.n_head, config.n_embd).to(device) # Linear layer to project multi-head attention outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        head_outputs = [head(x) for head in self.attn_heads] # Get the output of each attention head\n",
    "        multihead_output = torch.cat(head_outputs, dim=-1) # Concatenate the outputs\n",
    "        return self.projection(multihead_output) # Project the concatenated outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e759e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(config)\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd).to(device)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "        ).to(device)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c11fbe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.n_embd).to(device)\n",
    "        self.position_encoding = get_position_encoding(config.seq_len, config.n_embd)\n",
    "        self.blocks = nn.Sequential(*[GPTBlock(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd).to(device)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.token_embedding(x) + self.position_encoding\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36ea8969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_curve_micro(pred_tensor, label_tensor):\n",
    "    device=pred_tensor.device\n",
    "    n_class=pred_tensor.size(1)\n",
    "    one_hot_labels = F.one_hot(label_tensor, num_classes=n_class).to(device)\n",
    "    is_positive = one_hot_labels\n",
    "    is_negative =1-one_hot_labels\n",
    "    fn_diff = -is_positive.flatten()\n",
    "    fp_diff = is_negative.flatten()\n",
    "    thresh_tensor = -pred_tensor.flatten()\n",
    "    fn_denom = is_positive.sum()\n",
    "    fp_denom = is_negative.sum()\n",
    "    sorted_indices = torch.argsort(thresh_tensor)\n",
    "    sorted_fp_cum = fp_diff[sorted_indices].cumsum(0) / fp_denom\n",
    "    sorted_fn_cum = -fn_diff[sorted_indices].flip(0).cumsum(0).flip(0) / fn_denom\n",
    "\n",
    "    sorted_thresh = thresh_tensor[sorted_indices]\n",
    "    sorted_is_diff = sorted_thresh.diff() != 0\n",
    "    sorted_fp_end = torch.cat([sorted_is_diff, torch.tensor([True],device=device)])\n",
    "    sorted_fn_end = torch.cat([torch.tensor([True],device=device), sorted_is_diff])\n",
    "\n",
    "    uniq_thresh = sorted_thresh[sorted_fp_end]\n",
    "    uniq_fp_after = sorted_fp_cum[sorted_fp_end]\n",
    "    uniq_fn_before = sorted_fn_cum[sorted_fn_end]\n",
    "\n",
    "    FPR = torch.cat([torch.tensor([0.0],device=device), uniq_fp_after])\n",
    "    FNR = torch.cat([uniq_fn_before, torch.tensor([0.0],device=device)])\n",
    "\n",
    "    return {\n",
    "        \"FPR\": FPR,\n",
    "        \"FNR\": FNR,\n",
    "        \"TPR\": 1 - FNR,\n",
    "        \"min(FPR,FNR)\": torch.minimum(FPR, FNR),\n",
    "        \"min_constant\": torch.cat([torch.tensor([-1],device=device), uniq_thresh]),\n",
    "        \"max_constant\": torch.cat([uniq_thresh, torch.tensor([0],device=device)])\n",
    "    }\n",
    "def ROC_AUC_micro(pred_tensor, label_tensor):\n",
    "    roc = ROC_curve_micro(pred_tensor, label_tensor)\n",
    "    FPR_diff = roc[\"FPR\"][1:]-roc[\"FPR\"][:-1]   \n",
    "    TPR_sum = roc[\"TPR\"][1:]+roc[\"TPR\"][:-1]\n",
    "    return torch.sum(FPR_diff*TPR_sum/2.0)\n",
    "#AUM \n",
    "def Proposed_AUM_micro(pred_tensor, label_tensor):\n",
    "\n",
    "    roc = ROC_curve_micro(pred_tensor, label_tensor)\n",
    "    min_FPR_FNR = roc[\"min(FPR,FNR)\"][1:-1]\n",
    "    constant_diff = roc[\"min_constant\"][1:].diff()\n",
    "    return torch.sum(min_FPR_FNR * constant_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab91b166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_curve_macro(pred_tensor, label_tensor):\n",
    "    n_class=pred_tensor.size(1)\n",
    "    one_hot_labels = F.one_hot(label_tensor, num_classes=n_class)\n",
    "    is_positive = one_hot_labels\n",
    "    is_negative =1-one_hot_labels\n",
    "    fn_diff = -is_positive\n",
    "    fp_diff = is_negative\n",
    "    thresh_tensor = -pred_tensor\n",
    "    fn_denom = is_positive.sum(dim=0).clamp(min=1)\n",
    "    fp_denom = is_negative.sum(dim=0).clamp(min=1)\n",
    "    sorted_indices = torch.argsort(thresh_tensor,dim=0)\n",
    "    sorted_fp_cum = torch.div(torch.gather(fp_diff, dim=0, index=sorted_indices).cumsum(0), fp_denom)\n",
    "    sorted_fn_cum = -torch.div(torch.gather(fn_diff, dim=0, index=sorted_indices).flip(0).cumsum(0).flip(0) , fn_denom)\n",
    "    sorted_thresh = torch.gather(thresh_tensor, dim=0, index=sorted_indices)\n",
    "    #Problem starts here \n",
    "    zeros_vec=torch.zeros(1,n_class)\n",
    "    FPR = torch.cat([zeros_vec, sorted_fp_cum])\n",
    "    FNR = torch.cat([sorted_fn_cum, zeros_vec])\n",
    "    return {\n",
    "        \"FPR_all_classes\": FPR,\n",
    "        \"FNR_all_classes\": FNR,\n",
    "        \"TPR_all_classes\": 1 - FNR,\n",
    "        \"min(FPR,FNR)\": torch.minimum(FPR, FNR),\n",
    "        \"min_constant\": torch.cat([-torch.ones(1,n_class), sorted_thresh]),\n",
    "        \"max_constant\": torch.cat([sorted_thresh, zeros_vec])\n",
    "    }\n",
    "\n",
    "def ROC_AUC_macro(pred_tensor, label_tensor):\n",
    "    roc = ROC_curve_macro(pred_tensor, label_tensor)\n",
    "    FPR_diff = roc[\"FPR_all_classes\"][1:,:]-roc[\"FPR_all_classes\"][:-1,]\n",
    "    TPR_sum = roc[\"TPR_all_classes\"][1:,:]+roc[\"TPR_all_classes\"][:-1,:]\n",
    "    sum_FPR_TPR= torch.sum(FPR_diff*TPR_sum/2.0,dim=0)\n",
    "    count_non_defined=(sum_FPR_TPR == 0).sum()\n",
    "    if count_non_defined==pred_tensor.size(1):\n",
    "        return 0\n",
    "    return  sum_FPR_TPR.sum()/(pred_tensor.size(1)-count_non_defined)\n",
    "def Proposed_AUM_macro(pred_tensor, label_tensor):\n",
    "\n",
    "    roc = ROC_curve_macro(pred_tensor, label_tensor)\n",
    "    min_FPR_FNR = roc[\"min(FPR,FNR)\"][1:-1,:]\n",
    "    constant_diff = roc[\"min_constant\"][1:,:].diff(dim=0)\n",
    "    sum_min= torch.sum(min_FPR_FNR * constant_diff,dim=0)\n",
    "    count_non_defined=(sum_min== 0).sum()\n",
    "    if count_non_defined==pred_tensor.size(1):\n",
    "        return 0\n",
    "    return  sum_min.sum()/(pred_tensor.size(1)-count_non_defined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99a3c4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   runs parallel to the first game and follows the \" Nameless \" , a penal military unit serv\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "sequence_len = 128\n",
    "num_steps = 300\n",
    "train_inputs, train_targets, _, _ = get_dataset(10, sequence_len, 0)\n",
    "config = GPTConfig(\n",
    "    vocab_size=tokenizer.n_vocab,\n",
    "    n_layer=4,   # fewer layers for a quick demo\n",
    "    n_head=4,\n",
    "    n_embd=128,\n",
    "    seq_len=sequence_len,\n",
    ")\n",
    "\n",
    "    \n",
    "print(\"Original: \", tokenizer.decode(train_inputs[1].tolist())[:90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca7878ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example config:\n",
    "batch_size = 10\n",
    "sequence_len = 128\n",
    "num_steps = 1000\n",
    "train_inputs, train_targets, _, _ = get_dataset(10, sequence_len, 0)\n",
    "config = GPTConfig(\n",
    "    vocab_size=tokenizer.n_vocab,\n",
    "    n_layer=4,   # fewer layers for a quick demo\n",
    "    n_head=4,\n",
    "    n_embd=128,\n",
    "    seq_len=sequence_len,\n",
    ")\n",
    "\n",
    "loss_dict={\n",
    "    \"AUM_micro\": Proposed_AUM_micro,\n",
    "    \"AUM_macro\": Proposed_AUM_macro,\n",
    "    \"Cross-entropy\": F.cross_entropy\n",
    "}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfd49481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   director Takeshi Ozawa . A large team of writers handled the script . The game 's opening\n"
     ]
    }
   ],
   "source": [
    "def inference(prompt, max_new_tokens,model):\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    for _ in range(max_new_tokens):\n",
    "        num_tokens = len(tokens)\n",
    "        tokens_padded = tokens + [tokenizer.eot_token] * (config.seq_len - num_tokens)\n",
    "        tokens_padded = torch.tensor(tokens_padded).unsqueeze(0).to(device)\n",
    "        logits = model(tokens_padded)\n",
    "        predicted_token = torch.argmax(logits[0, num_tokens-1, :]).item()\n",
    "        tokens.append(predicted_token)\n",
    "    return tokenizer.decode(tokens)\n",
    "    \n",
    "print(\"Original: \", tokenizer.decode(train_inputs[2].tolist())[:90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "238419f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nou-z\\AppData\\Local\\Temp\\ipykernel_46660\\1328981107.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model=torch.load(f\"Pretrain_{name}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted for AUM_macro: Tom and Jane are friends. One day, Jane goes to Tom's house. Tom has a big pot of soup. oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct oct\n",
      "Predicted for CE: Tom and Jane are friends. One day, Jane goes to Tom's house. Tom has a big pot of soup.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nou-z\\AppData\\Local\\Temp\\ipykernel_46660\\1328981107.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_micro=torch.load(f\"AUM_micro_model_checkpoint_99999.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted for AUM_micro: Tom and Jane are friends. One day, Jane goes to Tom's house. Tom has a big pot of soup.ett Santos trophies Gillespieyards Vale Pandora Sabbyardsoan Gamma Sabb Sabb Sabb SabbPhil fascists proteins� Notes Sabb Frankfurt evidenced 21yards 243 gadgetsket Vale Bargyards Vale proteins wrestyards proteins withholdingottesville FAC ply inject proteins withholdingceed gadgetsANCE causing Adventwrapurities inject Sabbrecord allegingyardsoanrulewrap gadgets Kin Beasts fossils Gillespieoanblock Bellev Kin ServerANCEPhys Sabbrecord termrecord termrecord termrecord termrecordacticums evidenced withholding RM Letter Templar muscle muscle Eagles AdventJulballs Sabb ousted Summers Peoplesificeoan Sabb\n"
     ]
    }
   ],
   "source": [
    "model_dict={}\n",
    "loss_dict={\n",
    "    \"AUM_macro\": Proposed_AUM_macro,\n",
    "    \"CE\": F.cross_entropy\n",
    "}\n",
    "\n",
    "for name , _ in loss_dict.items():\n",
    "    model=torch.load(f\"Pretrain_{name}.pt\")\n",
    "    model_dict[name]=model\n",
    "    print(f\"Predicted for {name}:\", inference(\"Tom and Jane are friends. One day, Jane goes to Tom's house. Tom has a big pot of soup.\", max_new_tokens=100,model=model))\n",
    "model_micro=torch.load(f\"AUM_micro_model_checkpoint_99999.pt\")\n",
    "print(f\"Predicted for AUM_micro:\", inference(\"Tom and Jane are friends. One day, Jane goes to Tom's house. Tom has a big pot of soup.\", max_new_tokens=100,model=model_micro))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf2daaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
